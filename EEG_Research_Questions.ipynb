{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaclynchiu7/EEG-Analysis/blob/main/EEG_Research_Questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHw9iTvizQPr",
        "outputId": "0b33c3aa-70b5-442c-bb21-e4005306ed09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
        "# assignment folder, e.g. 'cs260/assignments/assignment4/'\n",
        "FOLDERNAME = 'ecengr 247'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# Now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten,Dropout\n",
        "from keras.layers import Conv2D,BatchNormalization,MaxPooling2D,Reshape\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "id": "mfVeams0zs-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "tIoIBYGpzxAt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd7304f3-d8ef-41b5-dc82-fbcabd53046a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: Subject 1 Accuracy\n",
        "\n",
        "Optimize the classification accuracy for subject 1. Does it help to train across all subjects?"
      ],
      "metadata": {
        "id": "rl3PH1cZzxuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting Data for Subject 1"
      ],
      "metadata": {
        "id": "-f6OyJ3J0TIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.load(\"drive/MyDrive/ecengr 247/X_test.npy\")\n",
        "y_test = np.load(\"drive/MyDrive/ecengr 247/y_test.npy\")\n",
        "person_train_valid = np.load(\"drive/MyDrive/ecengr 247/person_train_valid.npy\")\n",
        "X_train_valid = np.load(\"drive/MyDrive/ecengr 247/X_train_valid.npy\")\n",
        "y_train_valid = np.load(\"drive/MyDrive/ecengr 247/y_train_valid.npy\")\n",
        "person_test = np.load(\"drive/MyDrive/ecengr 247/person_test.npy\")"
      ],
      "metadata": {
        "id": "1ctBJfVtz065"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remap test labels \n",
        "# 0: left hand, 1: right hand, 2: feet, 3: tongue \n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "y_train_valid = label_encoder.fit_transform(y_train_valid)\n",
        "y_test = label_encoder.fit_transform(y_test)"
      ],
      "metadata": {
        "id": "5ZifGEEt9cDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split validation and training data before subset so data don't overlap \n",
        "ind_valid = np.random.choice(2115, 375, replace=False)\n",
        "ind_train = np.array(list(set(range(2115)).difference(set(ind_valid))))\n",
        "\n",
        "# Creating the training and validation sets using the generated indices\n",
        "(X_train, X_valid) = X_train_valid[ind_train], X_train_valid[ind_valid] \n",
        "(y_train, y_valid) = y_train_valid[ind_train], y_train_valid[ind_valid]\n",
        "\n",
        "# Creating training and validation sets for subject info\n",
        "person_train = person_train_valid[ind_train]\n",
        "person_valid = person_train_valid[ind_valid]\n"
      ],
      "metadata": {
        "id": "z_AAHsfL6bC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_1_train_idx = np.where(person_train == 0)[0] # find indx of training data for subject 1\n",
        "sub_1_valid_idx = np.where(person_valid == 0)[0] \n",
        "sub_1_test_idx = np.where(person_test == 0)[0]\n",
        "\n",
        "X_train_sub1 = X_train[sub_1_train_idx, :, :] # subset based on indx\n",
        "y_train_sub1 = y_train[sub_1_train_idx]\n",
        "\n",
        "X_valid_sub1 = X_valid[sub_1_valid_idx, :, :] # subset based on indx\n",
        "y_valid_sub1 = y_valid[sub_1_valid_idx]\n",
        "\n",
        "X_test_sub1 = X_test[sub_1_test_idx, :, :]\n",
        "y_test_sub1 = y_test[sub_1_test_idx]\n",
        "\n",
        "print('Subject 1 Data:')\n",
        "print ('Training data shape: {}'.format(X_train_sub1.shape))\n",
        "print ('Valid data shape: {}'.format(X_valid_sub1.shape))\n",
        "print ('Test data shape: {}'.format(X_test_sub1.shape))\n",
        "print ('Training target shape: {}'.format(y_train_sub1.shape))\n",
        "print ('Valid target shape: {}'.format(y_valid_sub1.shape))\n",
        "print ('Test target shape: {}'.format(y_test_sub1.shape))\n",
        "\n",
        "\n",
        "print('\\nAll Subjects Data:')\n",
        "print ('Training data shape: {}'.format(X_train.shape))\n",
        "print ('Valid data shape: {}'.format(X_valid.shape))\n",
        "print ('Test data shape: {}'.format(X_test.shape))\n",
        "print ('Training target shape: {}'.format(y_train.shape))\n",
        "print ('Valid target shape: {}'.format(y_valid.shape))\n",
        "print ('Test target shape: {}'.format(y_test.shape))"
      ],
      "metadata": {
        "id": "42sK0upEz2Vt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f6da79b-a1af-448d-9b49-05f88e1064d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject 1 Data:\n",
            "Training data shape: (189, 22, 1000)\n",
            "Valid data shape: (48, 22, 1000)\n",
            "Test data shape: (50, 22, 1000)\n",
            "Training target shape: (189,)\n",
            "Valid target shape: (48,)\n",
            "Test target shape: (50,)\n",
            "\n",
            "All Subjects Data:\n",
            "Training data shape: (1740, 22, 1000)\n",
            "Valid data shape: (375, 22, 1000)\n",
            "Test data shape: (443, 22, 1000)\n",
            "Training target shape: (1740,)\n",
            "Valid target shape: (375,)\n",
            "Test target shape: (443,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Subject 1 Data"
      ],
      "metadata": {
        "id": "2sorHCei0cGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_prep(X,y,sub_sample,average,noise):\n",
        "    \n",
        "    total_X = None\n",
        "    total_y = None\n",
        "    \n",
        "    # Trimming the data (sample,22,1000) -> (sample,22,500)\n",
        "    X = X[:,:,0:500]\n",
        "    print('Shape of X after trimming:',X.shape)\n",
        "    \n",
        "    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)\n",
        "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
        "    \n",
        "    \n",
        "    total_X = X_max\n",
        "    total_y = y\n",
        "    print('Shape of X after maxpooling:',total_X.shape)\n",
        "    \n",
        "    # Averaging + noise \n",
        "    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average),axis=3)\n",
        "    X_average = X_average + np.random.normal(0.0, 0.5, X_average.shape)\n",
        "    \n",
        "    total_X = np.vstack((total_X, X_average))\n",
        "    total_y = np.hstack((total_y, y))\n",
        "    print('Shape of X after averaging+noise and concatenating:',total_X.shape)\n",
        "    \n",
        "    # Subsampling\n",
        "    \n",
        "    for i in range(sub_sample):\n",
        "        \n",
        "        X_subsample = X[:, :, i::sub_sample] + \\\n",
        "                            (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n",
        "            \n",
        "        total_X = np.vstack((total_X, X_subsample))\n",
        "        total_y = np.hstack((total_y, y))\n",
        "        \n",
        "    \n",
        "    print('Shape of X after subsampling and concatenating:',total_X.shape)\n",
        "    return total_X,total_y"
      ],
      "metadata": {
        "id": "WMUGsO2c3COc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Preprocessing subject 1 data \n",
        "x_train_sub1,y_train_sub1 = data_prep(X_train_sub1,y_train_sub1,2,2,True)\n",
        "x_valid_sub1,y_valid_sub1 = data_prep(X_valid_sub1,y_valid_sub1,2,2,True)\n",
        "X_test_prep_sub1,y_test_prep_sub1 = data_prep(X_test_sub1,y_test_sub1,2,2,True)\n",
        "\n",
        "print('Subject 1 Data after Preprocessing')\n",
        "print('Shape of testing set:',X_test_prep_sub1.shape)\n",
        "print('Shape of testing labels:',y_test_prep_sub1.shape)\n",
        "print('Shape of training set:',x_train_sub1.shape)\n",
        "print('Shape of validation set:',x_valid_sub1.shape)\n",
        "print('Shape of training labels:',y_train_sub1.shape)\n",
        "print('Shape of validation labels:',y_valid_sub1.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u2H-Od-wP_9",
        "outputId": "5f50be27-8f17-41ae-bda5-7b0cbee7108d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X after trimming: (189, 22, 500)\n",
            "Shape of X after maxpooling: (189, 22, 250)\n",
            "Shape of X after averaging+noise and concatenating: (378, 22, 250)\n",
            "Shape of X after subsampling and concatenating: (756, 22, 250)\n",
            "Shape of X after trimming: (48, 22, 500)\n",
            "Shape of X after maxpooling: (48, 22, 250)\n",
            "Shape of X after averaging+noise and concatenating: (96, 22, 250)\n",
            "Shape of X after subsampling and concatenating: (192, 22, 250)\n",
            "Shape of X after trimming: (50, 22, 500)\n",
            "Shape of X after maxpooling: (50, 22, 250)\n",
            "Shape of X after averaging+noise and concatenating: (100, 22, 250)\n",
            "Shape of X after subsampling and concatenating: (200, 22, 250)\n",
            "Subject 1 Data after Preprocessing\n",
            "Shape of testing set: (200, 22, 250)\n",
            "Shape of testing labels: (200,)\n",
            "Shape of training set: (756, 22, 250)\n",
            "Shape of validation set: (192, 22, 250)\n",
            "Shape of training labels: (756,)\n",
            "Shape of validation labels: (192,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Preprocessing all subject data \n",
        "x_train,y_train = data_prep(X_train,y_train,2,2,True)\n",
        "x_valid,y_valid = data_prep(X_valid,y_valid,2,2,True)\n",
        "X_test_prep,y_test_prep = data_prep(X_test,y_test,2,2,True)\n",
        "\n",
        "\n",
        "print('Shape of testing set:',X_test_prep.shape)\n",
        "print('Shape of testing labels:',y_test_prep.shape)\n",
        "print('Shape of training set:',x_train.shape)\n",
        "print('Shape of validation set:',x_valid.shape)\n",
        "print('Shape of training labels:',y_train.shape)\n",
        "print('Shape of validation labels:',y_valid.shape)"
      ],
      "metadata": {
        "id": "O68MXWAD3Jwt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22feee35-804f-4706-96ac-2d2aafc52b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X after trimming: (1740, 22, 500)\n",
            "Shape of X after maxpooling: (1740, 22, 250)\n",
            "Shape of X after averaging+noise and concatenating: (3480, 22, 250)\n",
            "Shape of X after subsampling and concatenating: (6960, 22, 250)\n",
            "Shape of X after trimming: (375, 22, 500)\n",
            "Shape of X after maxpooling: (375, 22, 250)\n",
            "Shape of X after averaging+noise and concatenating: (750, 22, 250)\n",
            "Shape of X after subsampling and concatenating: (1500, 22, 250)\n",
            "Shape of X after trimming: (443, 22, 500)\n",
            "Shape of X after maxpooling: (443, 22, 250)\n",
            "Shape of X after averaging+noise and concatenating: (886, 22, 250)\n",
            "Shape of X after subsampling and concatenating: (1772, 22, 250)\n",
            "Shape of testing set: (1772, 22, 250)\n",
            "Shape of testing labels: (1772,)\n",
            "Shape of training set: (6960, 22, 250)\n",
            "Shape of validation set: (1500, 22, 250)\n",
            "Shape of training labels: (6960,)\n",
            "Shape of validation labels: (1500,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VAE Augmentation "
      ],
      "metadata": {
        "id": "Sf3CHeXYxX08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "VXCQlsZix8Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder \n",
        "\n",
        "latent_dim = 8\n",
        "encoder_inputs = keras.Input(shape=(22, 250, 1), name='input_layer')\n",
        " \n",
        "# Block-1\n",
        "x = layers.Conv2D(32, kernel_size=3, strides= 1, padding='same', name='conv_1')(encoder_inputs)\n",
        "x = layers.BatchNormalization(name='bn_1')(x)\n",
        "x = layers.LeakyReLU(name='lrelu_1')(x)\n",
        "  \n",
        "# Block-2\n",
        "x = layers.Conv2D(64, kernel_size=3, strides= 2, padding='same', name='conv_2')(x)\n",
        "x = layers.BatchNormalization(name='bn_2')(x)\n",
        "x = layers.LeakyReLU(name='lrelu_2')(x)\n",
        "  \n",
        "# Block-3\n",
        "x = layers.Conv2D(64, 3, 1, padding='same', name='conv_3')(x)\n",
        "x = layers.BatchNormalization(name='bn_3')(x)\n",
        "x = layers.LeakyReLU(name='lrelu_3')(x)\n",
        "\n",
        "# Block-4\n",
        "x = layers.Conv2D(64, 3, 1, padding='same', name='conv_4')(x)\n",
        "x = layers.BatchNormalization(name='bn_4')(x)\n",
        "x = layers.LeakyReLU(name='lrelu_4')(x)\n",
        "\n",
        "# Final Block\n",
        "flatten = layers.Flatten()(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")"
      ],
      "metadata": {
        "id": "uEEStWqix8c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder \n",
        "\n",
        "latent_inputs = keras.Input(shape=(latent_dim,), name='input_layer')\n",
        "x = layers.Dense(11*125*64, name='dense_1')(latent_inputs)\n",
        "x = layers.Reshape((11, 125, 64), name='Reshape_Layer')(x)\n",
        "\n",
        "# Block-1\n",
        "x = layers.Conv2DTranspose(64, 3, strides= 1, padding='same',name='conv_transpose_1')(x)\n",
        "x = layers.BatchNormalization(name='bn_1')(x)\n",
        "x = layers.LeakyReLU(name='lrelu_1')(x)\n",
        "\n",
        "# Block-2\n",
        "x = layers.Conv2DTranspose(64, 3, strides= 2, padding='same', name='conv_transpose_2')(x)\n",
        "x = layers.BatchNormalization(name='bn_2')(x)\n",
        "x = layers.LeakyReLU(name='lrelu_2')(x)\n",
        "  \n",
        "# Block-3\n",
        "x = layers.Conv2DTranspose(32, 3, 1, padding='same', name='conv_transpose_3')(x)\n",
        "x = layers.BatchNormalization(name='bn_3')(x)\n",
        "x = layers.LeakyReLU(name='lrelu_3')(x)\n",
        "  \n",
        "# Block-4\n",
        "decoder_outputs = layers.Conv2DTranspose(1, 3, 1,padding='same', activation='elu', name='conv_transpose_4')(x)\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")"
      ],
      "metadata": {
        "id": "MKm9av0QyDhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(keras.Model): # VAE class \n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.mse_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"mse_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.mse_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            mse_loss = tf.keras.backend.mean(tf.keras.backend.square(data-reconstruction), axis=[1, 2, 3])\n",
        "            mse_loss *= 1000 #  scaling\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = mse_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.mse_loss_tracker.update_state(mse_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"mse_loss\": self.mse_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }"
      ],
      "metadata": {
        "id": "c2g76qpEyH2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def VAE_augmentation(X,y):\n",
        "    \n",
        "    total_X = X\n",
        "    total_y = y\n",
        "\n",
        "    # Class 0 VAE generation \n",
        "    class_0_idx = np.where(y == 0)[0]\n",
        "    X_train_class_0 = X[class_0_idx, :, :]\n",
        "    y_train_class_0= y[class_0_idx]\n",
        "    x_class0 = np.expand_dims(X_train_class_0, -1).astype(\"float32\")/255.0\n",
        "    vae_class0 = VAE(encoder, decoder)\n",
        "    vae_class0.compile(optimizer=keras.optimizers.Adam(learning_rate = 0.0005))\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      vae_class0.fit(x_class0, epochs = 10)\n",
        "    \n",
        "    z_mean, _, _ = vae_class0.encoder.predict(x_class0)\n",
        "    x_train_vae_class0 = vae_class0.decoder.predict(z_mean)\n",
        "    y_train_vae_class0 = np.zeros(len(y_train_class_0))\n",
        "\n",
        "    # Class 1 VAE generation \n",
        "    class_1_idx = np.where(y == 1)[0]\n",
        "    X_train_class_1 = X[class_1_idx, :, :]\n",
        "    y_train_class_1= y[class_1_idx]\n",
        "    x_class1 = np.expand_dims(X_train_class_1, -1).astype(\"float32\")/255.0\n",
        "    vae_class1 = VAE(encoder, decoder)\n",
        "    vae_class1.compile(optimizer=keras.optimizers.Adam(learning_rate = 0.0005))\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      vae_class1.fit(x_class1, epochs = 10)\n",
        "    \n",
        "    z_mean, _, _ = vae_class1.encoder.predict(x_class1)\n",
        "    x_train_vae_class1 = vae_class1.decoder.predict(z_mean)\n",
        "    y_train_vae_class1 = np.zeros(len(y_train_class_1))\n",
        "\n",
        "    # Class 2 VAE generation \n",
        "    class_2_idx = np.where(y == 2)[0]\n",
        "\n",
        "    X_train_class_2 = X[class_2_idx, :, :]\n",
        "    y_train_class_2= y[class_2_idx]\n",
        "    x_class2 = np.expand_dims(X_train_class_2, -1).astype(\"float32\")/255.0\n",
        "    vae_class2 = VAE(encoder, decoder)\n",
        "    vae_class2.compile(optimizer=keras.optimizers.Adam(learning_rate = 0.0005))\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      vae_class2.fit(x_class2, epochs = 10)\n",
        "    \n",
        "    z_mean, _, _ = vae_class2.encoder.predict(x_class2)\n",
        "    x_train_vae_class2 = vae_class2.decoder.predict(z_mean)\n",
        "    y_train_vae_class2 = np.zeros(len(y_train_class_2))\n",
        "\n",
        "    # Class 3 VAE generation \n",
        "    class_3_idx = np.where(y == 3)[0]\n",
        "\n",
        "    X_train_class_3 = X[class_3_idx, :, :]\n",
        "    y_train_class_3= y[class_3_idx]\n",
        "    x_class3 = np.expand_dims(X_train_class_3, -1).astype(\"float32\")/255.0\n",
        "    vae_class3 = VAE(encoder, decoder)\n",
        "    vae_class3.compile(optimizer=keras.optimizers.Adam(learning_rate = 0.0005))\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      vae_class3.fit(x_class3, epochs = 10)\n",
        "    \n",
        "    z_mean, _, _ = vae_class3.encoder.predict(x_class3)\n",
        "    x_train_vae_class3 = vae_class3.decoder.predict(z_mean)\n",
        "    y_train_vae_class3 = np.zeros(len(y_train_class_3))\n",
        "\n",
        "    # Concatenating values\n",
        "    # stacking X data\n",
        "    vae_X = np.vstack((x_train_vae_class0, x_train_vae_class1))\n",
        "    vae_X = np.vstack((vae_X , x_train_vae_class2))\n",
        "    vae_X = np.vstack((vae_X , x_train_vae_class3))\n",
        "\n",
        "    # stacking Y data\n",
        "    vae_y = np.hstack((y_train_vae_class0, y_train_vae_class1))\n",
        "    vae_y = np.hstack((vae_y, y_train_vae_class2))\n",
        "    vae_y = np.hstack((vae_y, y_train_vae_class3))\n",
        "\n",
        "    # stacking vae and original data\n",
        "    vae_X, vae_y = shuffle(vae_X, vae_y)\n",
        "    vae_X = vae_X.reshape(vae_X.shape[0], vae_X.shape[1], vae_X.shape[2]) # reshaping to original train data \n",
        "    total_X = np.vstack((X, vae_X))\n",
        "    total_y = np.hstack((y, vae_y))\n",
        "    \n",
        "    return total_X,total_y"
      ],
      "metadata": {
        "id": "hQ_WnQX3yKGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vae subject 1 data \n",
        "x_train_sub1, y_train_sub1 = VAE_augmentation(x_train_sub1,y_train_sub1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAGopQn-x34g",
        "outputId": "3145ac01-0f26-4e27-bf1b-473662e255ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "6/6 [==============================] - 6s 12ms/step - loss: 1.5632 - mse_loss: 1.5716 - kl_loss: 9.0955e-04\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 1.4877 - mse_loss: 1.5185 - kl_loss: 1.8463e-05\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 1.4913 - mse_loss: 1.4669 - kl_loss: 3.5355e-05\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 1.4654 - mse_loss: 1.4456 - kl_loss: 5.2027e-05\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 1.4618 - mse_loss: 1.4392 - kl_loss: 6.4344e-05\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 1.4193 - mse_loss: 1.4422 - kl_loss: 4.6665e-05\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 1.5066 - mse_loss: 1.4308 - kl_loss: 3.6274e-05\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 1.3833 - mse_loss: 1.4348 - kl_loss: 2.8511e-05\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 1.3831 - mse_loss: 1.4294 - kl_loss: 2.1989e-05\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 1.3663 - mse_loss: 1.4182 - kl_loss: 1.8020e-05\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "6/6 [==============================] - 0s 2ms/step\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 7s 13ms/step - loss: 1.3715 - mse_loss: 1.3692 - kl_loss: 1.6799e-05\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 1.3560 - mse_loss: 1.3576 - kl_loss: 2.5441e-05\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 1.3376 - mse_loss: 1.3434 - kl_loss: 2.3638e-05\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 1.3327 - mse_loss: 1.3324 - kl_loss: 2.0375e-05\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 1.3276 - mse_loss: 1.3321 - kl_loss: 1.5343e-05\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 1.3041 - mse_loss: 1.3361 - kl_loss: 1.2830e-05\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 1.3342 - mse_loss: 1.3274 - kl_loss: 1.1201e-05\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 1.3170 - mse_loss: 1.3275 - kl_loss: 9.9242e-06\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 1.3261 - mse_loss: 1.3269 - kl_loss: 9.2238e-06\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 1.3282 - mse_loss: 1.3267 - kl_loss: 7.5549e-06\n",
            "6/6 [==============================] - 0s 2ms/step\n",
            "6/6 [==============================] - 0s 2ms/step\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 6s 13ms/step - loss: 1.6613 - mse_loss: 1.6354 - kl_loss: 1.4722e-05\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 1.6368 - mse_loss: 1.5997 - kl_loss: 4.6492e-05\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 1.5461 - mse_loss: 1.5626 - kl_loss: 7.1218e-05\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 1.5413 - mse_loss: 1.5470 - kl_loss: 6.6320e-05\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 1.5649 - mse_loss: 1.5468 - kl_loss: 4.4445e-05\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 1.5893 - mse_loss: 1.5337 - kl_loss: 3.2122e-05\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 1.5022 - mse_loss: 1.5346 - kl_loss: 2.3430e-05\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 1.5265 - mse_loss: 1.5275 - kl_loss: 2.0072e-05\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 1.5345 - mse_loss: 1.5248 - kl_loss: 1.4524e-05\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 1.5284 - mse_loss: 1.5332 - kl_loss: 1.3048e-05\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 6s 12ms/step - loss: 1.5876 - mse_loss: 1.5488 - kl_loss: 1.7514e-05\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 1.5191 - mse_loss: 1.5265 - kl_loss: 2.9316e-05\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 1.5267 - mse_loss: 1.5167 - kl_loss: 2.9261e-05\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 1.5422 - mse_loss: 1.5062 - kl_loss: 2.6201e-05\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 1.5062 - mse_loss: 1.5130 - kl_loss: 2.1900e-05\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 1.4882 - mse_loss: 1.4907 - kl_loss: 1.5383e-05\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 1.4933 - mse_loss: 1.5014 - kl_loss: 1.1459e-05\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 1.5015 - mse_loss: 1.4928 - kl_loss: 1.2780e-05\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 1.4840 - mse_loss: 1.4903 - kl_loss: 1.1951e-05\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 1.4869 - mse_loss: 1.4913 - kl_loss: 1.1226e-05\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "6/6 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_valid_sub1, y_valid_sub1 = VAE_augmentation(x_valid_sub1,y_valid_sub1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKgfyGcayWFX",
        "outputId": "d00502c3-8d6b-4311-debd-0dd39217533f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 18ms/step - loss: 1.5649 - mse_loss: 1.5771 - kl_loss: 1.3664e-05\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.5006 - mse_loss: 1.5335 - kl_loss: 2.0087e-05\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.5103 - mse_loss: 1.5010 - kl_loss: 2.5079e-05\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.5048 - mse_loss: 1.4981 - kl_loss: 3.0652e-05\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.4966 - mse_loss: 1.4595 - kl_loss: 3.6463e-05\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.4350 - mse_loss: 1.4508 - kl_loss: 3.9309e-05\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.4555 - mse_loss: 1.4497 - kl_loss: 4.0069e-05\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.4199 - mse_loss: 1.4474 - kl_loss: 3.9726e-05\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.4262 - mse_loss: 1.4402 - kl_loss: 3.8952e-05\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.4561 - mse_loss: 1.4258 - kl_loss: 3.8683e-05\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 16ms/step - loss: 1.4066 - mse_loss: 1.3761 - kl_loss: 3.8311e-05\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.3019 - mse_loss: 1.2969 - kl_loss: 3.8087e-05\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.2725 - mse_loss: 1.2725 - kl_loss: 3.5271e-05\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2637 - mse_loss: 1.2746 - kl_loss: 3.1397e-05\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2494 - mse_loss: 1.2585 - kl_loss: 2.7940e-05\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2502 - mse_loss: 1.2465 - kl_loss: 2.4825e-05\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2436 - mse_loss: 1.2445 - kl_loss: 2.2888e-05\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2347 - mse_loss: 1.2332 - kl_loss: 2.2918e-05\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.2433 - mse_loss: 1.2432 - kl_loss: 2.5213e-05\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2378 - mse_loss: 1.2338 - kl_loss: 3.0383e-05\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 7s 16ms/step - loss: 1.3748 - mse_loss: 1.3610 - kl_loss: 3.2201e-05\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.3264 - mse_loss: 1.3262 - kl_loss: 2.1279e-05\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.2976 - mse_loss: 1.3036 - kl_loss: 1.4678e-05\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1.2917 - mse_loss: 1.3040 - kl_loss: 1.3188e-05\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1.2661 - mse_loss: 1.2833 - kl_loss: 1.8209e-05\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.2817 - mse_loss: 1.2701 - kl_loss: 2.8282e-05\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2585 - mse_loss: 1.2621 - kl_loss: 4.0859e-05\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2673 - mse_loss: 1.2641 - kl_loss: 5.3406e-05\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2365 - mse_loss: 1.2459 - kl_loss: 6.3851e-05\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.2417 - mse_loss: 1.2381 - kl_loss: 7.1704e-05\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 15ms/step - loss: 1.6276 - mse_loss: 1.6452 - kl_loss: 7.0378e-05\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.6423 - mse_loss: 1.6237 - kl_loss: 5.1200e-05\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.6275 - mse_loss: 1.6071 - kl_loss: 3.7640e-05\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.6060 - mse_loss: 1.5920 - kl_loss: 2.8148e-05\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.5767 - mse_loss: 1.5812 - kl_loss: 2.2352e-05\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.5831 - mse_loss: 1.5887 - kl_loss: 2.0236e-05\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.5623 - mse_loss: 1.5875 - kl_loss: 2.1622e-05\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.5838 - mse_loss: 1.5847 - kl_loss: 2.5064e-05\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.5681 - mse_loss: 1.5735 - kl_loss: 2.9773e-05\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.5619 - mse_loss: 1.5513 - kl_loss: 3.4079e-05\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_prep_sub1, y_test_prep_sub1 = VAE_augmentation(X_test_prep_sub1,y_test_prep_sub1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWfwQxLmyWJ7",
        "outputId": "27d3d717-8102-48f5-bf22-0bb8f050aed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 26ms/step - loss: 1.3394 - mse_loss: 1.3379 - kl_loss: 3.6314e-05\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.3267 - mse_loss: 1.3228 - kl_loss: 3.0443e-05\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.3087 - mse_loss: 1.3059 - kl_loss: 2.5675e-05\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.2747 - mse_loss: 1.2929 - kl_loss: 2.1458e-05\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.2940 - mse_loss: 1.2935 - kl_loss: 1.8388e-05\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2875 - mse_loss: 1.2872 - kl_loss: 1.7226e-05\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2794 - mse_loss: 1.2795 - kl_loss: 1.7256e-05\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2756 - mse_loss: 1.2706 - kl_loss: 1.8120e-05\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.2549 - mse_loss: 1.2638 - kl_loss: 1.7315e-05\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2539 - mse_loss: 1.2532 - kl_loss: 1.5885e-05\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 16ms/step - loss: 1.3517 - mse_loss: 1.3552 - kl_loss: 1.6406e-05\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.3212 - mse_loss: 1.3417 - kl_loss: 2.4155e-05\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.3336 - mse_loss: 1.3236 - kl_loss: 2.5332e-05\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.3189 - mse_loss: 1.3122 - kl_loss: 2.3484e-05\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.3036 - mse_loss: 1.2996 - kl_loss: 2.1368e-05\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2823 - mse_loss: 1.2926 - kl_loss: 2.0102e-05\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2767 - mse_loss: 1.2791 - kl_loss: 2.0355e-05\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2707 - mse_loss: 1.2747 - kl_loss: 2.1636e-05\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2719 - mse_loss: 1.2696 - kl_loss: 2.3246e-05\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2489 - mse_loss: 1.2671 - kl_loss: 2.4959e-05\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 7s 17ms/step - loss: 1.4821 - mse_loss: 1.4589 - kl_loss: 2.4721e-05\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.4303 - mse_loss: 1.4356 - kl_loss: 2.1189e-05\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.4058 - mse_loss: 1.4025 - kl_loss: 2.0027e-05\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.4082 - mse_loss: 1.4022 - kl_loss: 2.0593e-05\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.3929 - mse_loss: 1.3869 - kl_loss: 2.1979e-05\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.3845 - mse_loss: 1.3796 - kl_loss: 2.4527e-05\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.3737 - mse_loss: 1.3660 - kl_loss: 2.7820e-05\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.3587 - mse_loss: 1.3631 - kl_loss: 3.2052e-05\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.3575 - mse_loss: 1.3549 - kl_loss: 3.7119e-05\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.3505 - mse_loss: 1.3612 - kl_loss: 4.1366e-05\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 16ms/step - loss: 1.4392 - mse_loss: 1.4411 - kl_loss: 4.1202e-05\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1.4313 - mse_loss: 1.4335 - kl_loss: 3.7342e-05\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.4132 - mse_loss: 1.4128 - kl_loss: 3.6344e-05\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1.3780 - mse_loss: 1.3859 - kl_loss: 4.0650e-05\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.3507 - mse_loss: 1.3762 - kl_loss: 4.8295e-05\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.3866 - mse_loss: 1.3689 - kl_loss: 5.9247e-05\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.3444 - mse_loss: 1.3539 - kl_loss: 7.1436e-05\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1.3555 - mse_loss: 1.3306 - kl_loss: 8.1986e-05\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.3499 - mse_loss: 1.3419 - kl_loss: 8.8975e-05\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.3469 - mse_loss: 1.3357 - kl_loss: 9.6217e-05\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vae all subject data \n",
        "x_train, y_train = VAE_augmentation(x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzKeJ14zyWMg",
        "outputId": "8164a6b5-d7b9-4eb0-d1e7-f8cb370b3a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "55/55 [==============================] - 7s 12ms/step - loss: 1.6377 - mse_loss: 1.6389 - kl_loss: 2.5187e-05\n",
            "Epoch 2/10\n",
            "55/55 [==============================] - 1s 12ms/step - loss: 1.6123 - mse_loss: 1.6173 - kl_loss: 4.4682e-06\n",
            "Epoch 3/10\n",
            "55/55 [==============================] - 1s 11ms/step - loss: 1.6362 - mse_loss: 1.6164 - kl_loss: 3.4457e-06\n",
            "Epoch 4/10\n",
            "55/55 [==============================] - 1s 12ms/step - loss: 1.6132 - mse_loss: 1.6154 - kl_loss: 2.6882e-06\n",
            "Epoch 5/10\n",
            "55/55 [==============================] - 1s 11ms/step - loss: 1.6112 - mse_loss: 1.6156 - kl_loss: 3.1032e-06\n",
            "Epoch 6/10\n",
            "55/55 [==============================] - 1s 12ms/step - loss: 1.5983 - mse_loss: 1.6148 - kl_loss: 2.3468e-06\n",
            "Epoch 7/10\n",
            "55/55 [==============================] - 1s 12ms/step - loss: 1.6010 - mse_loss: 1.6138 - kl_loss: 2.5099e-06\n",
            "Epoch 8/10\n",
            "55/55 [==============================] - 1s 12ms/step - loss: 1.6057 - mse_loss: 1.6153 - kl_loss: 2.2731e-06\n",
            "Epoch 9/10\n",
            "55/55 [==============================] - 1s 11ms/step - loss: 1.6108 - mse_loss: 1.6127 - kl_loss: 1.8846e-06\n",
            "Epoch 10/10\n",
            "55/55 [==============================] - 1s 11ms/step - loss: 1.6020 - mse_loss: 1.6134 - kl_loss: 1.7220e-06\n",
            "55/55 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 0s 2ms/step\n",
            "Epoch 1/10\n",
            "55/55 [==============================] - 6s 11ms/step - loss: 1.5937 - mse_loss: 1.5784 - kl_loss: 3.7437e-06\n",
            "Epoch 2/10\n",
            "55/55 [==============================] - 1s 11ms/step - loss: 1.5824 - mse_loss: 1.5729 - kl_loss: 2.5272e-06\n",
            "Epoch 3/10\n",
            "55/55 [==============================] - 1s 11ms/step - loss: 1.5615 - mse_loss: 1.5724 - kl_loss: 2.4297e-06\n",
            "Epoch 4/10\n",
            "55/55 [==============================] - 1s 11ms/step - loss: 1.5824 - mse_loss: 1.5712 - kl_loss: 2.7071e-06\n",
            "Epoch 5/10\n",
            "55/55 [==============================] - 1s 11ms/step - loss: 1.5554 - mse_loss: 1.5704 - kl_loss: 1.6949e-06\n",
            "Epoch 6/10\n",
            "55/55 [==============================] - 1s 11ms/step - loss: 1.5477 - mse_loss: 1.5704 - kl_loss: 1.5448e-06\n",
            "Epoch 7/10\n",
            "55/55 [==============================] - 1s 11ms/step - loss: 1.5686 - mse_loss: 1.5705 - kl_loss: 1.8889e-06\n",
            "Epoch 8/10\n",
            "55/55 [==============================] - 1s 11ms/step - loss: 1.5617 - mse_loss: 1.5701 - kl_loss: 1.3969e-06\n",
            "Epoch 9/10\n",
            "55/55 [==============================] - 1s 11ms/step - loss: 1.5796 - mse_loss: 1.5715 - kl_loss: 2.7028e-06\n",
            "Epoch 10/10\n",
            "55/55 [==============================] - 1s 10ms/step - loss: 1.5594 - mse_loss: 1.5706 - kl_loss: 1.4744e-06\n",
            "55/55 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 0s 2ms/step\n",
            "Epoch 1/10\n",
            "55/55 [==============================] - 6s 11ms/step - loss: 1.7193 - mse_loss: 1.7072 - kl_loss: 5.1585e-06\n",
            "Epoch 2/10\n",
            "55/55 [==============================] - 1s 10ms/step - loss: 1.7234 - mse_loss: 1.6976 - kl_loss: 3.2669e-06\n",
            "Epoch 3/10\n",
            "55/55 [==============================] - 1s 10ms/step - loss: 1.7086 - mse_loss: 1.6975 - kl_loss: 2.9055e-06\n",
            "Epoch 4/10\n",
            "55/55 [==============================] - 1s 10ms/step - loss: 1.6990 - mse_loss: 1.6969 - kl_loss: 2.3717e-06\n",
            "Epoch 5/10\n",
            "55/55 [==============================] - 1s 10ms/step - loss: 1.6586 - mse_loss: 1.6977 - kl_loss: 2.0060e-06\n",
            "Epoch 6/10\n",
            "55/55 [==============================] - 1s 10ms/step - loss: 1.7015 - mse_loss: 1.6967 - kl_loss: 1.8987e-06\n",
            "Epoch 7/10\n",
            "55/55 [==============================] - 1s 10ms/step - loss: 1.6876 - mse_loss: 1.6969 - kl_loss: 2.0081e-06\n",
            "Epoch 8/10\n",
            "55/55 [==============================] - 1s 10ms/step - loss: 1.7111 - mse_loss: 1.6969 - kl_loss: 1.7177e-06\n",
            "Epoch 9/10\n",
            "55/55 [==============================] - 1s 10ms/step - loss: 1.6956 - mse_loss: 1.6977 - kl_loss: 1.8505e-06\n",
            "Epoch 10/10\n",
            "55/55 [==============================] - 1s 10ms/step - loss: 1.6783 - mse_loss: 1.6964 - kl_loss: 1.2463e-06\n",
            "55/55 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 0s 2ms/step\n",
            "Epoch 1/10\n",
            "55/55 [==============================] - 7s 11ms/step - loss: 1.7948 - mse_loss: 1.7550 - kl_loss: 2.8068e-06\n",
            "Epoch 2/10\n",
            "55/55 [==============================] - 1s 10ms/step - loss: 1.7510 - mse_loss: 1.7468 - kl_loss: 2.1842e-06\n",
            "Epoch 3/10\n",
            "55/55 [==============================] - 1s 11ms/step - loss: 1.7484 - mse_loss: 1.7466 - kl_loss: 2.3143e-06\n",
            "Epoch 4/10\n",
            "55/55 [==============================] - 1s 11ms/step - loss: 1.7410 - mse_loss: 1.7470 - kl_loss: 2.6362e-06\n",
            "Epoch 5/10\n",
            "55/55 [==============================] - 1s 10ms/step - loss: 1.7550 - mse_loss: 1.7437 - kl_loss: 1.3319e-06\n",
            "Epoch 6/10\n",
            "55/55 [==============================] - 1s 10ms/step - loss: 1.7340 - mse_loss: 1.7435 - kl_loss: 1.5551e-06\n",
            "Epoch 7/10\n",
            "55/55 [==============================] - 1s 10ms/step - loss: 1.7522 - mse_loss: 1.7437 - kl_loss: 1.2354e-06\n",
            "Epoch 8/10\n",
            "55/55 [==============================] - 1s 10ms/step - loss: 1.7486 - mse_loss: 1.7427 - kl_loss: 1.5849e-06\n",
            "Epoch 9/10\n",
            "55/55 [==============================] - 1s 11ms/step - loss: 1.7446 - mse_loss: 1.7437 - kl_loss: 1.5795e-06\n",
            "Epoch 10/10\n",
            "55/55 [==============================] - 1s 11ms/step - loss: 1.7525 - mse_loss: 1.7442 - kl_loss: 1.5752e-06\n",
            "55/55 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_valid, y_valid = VAE_augmentation(x_valid,y_valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKoClutKyWOv",
        "outputId": "6363a79c-83d8-461d-af05-59fe1588f7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "12/12 [==============================] - 6s 12ms/step - loss: 1.6028 - mse_loss: 1.6715 - kl_loss: 2.5779e-06\n",
            "Epoch 2/10\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.6585 - mse_loss: 1.6596 - kl_loss: 2.9256e-06\n",
            "Epoch 3/10\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.6139 - mse_loss: 1.6560 - kl_loss: 2.0762e-06\n",
            "Epoch 4/10\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.6634 - mse_loss: 1.6516 - kl_loss: 1.4851e-06\n",
            "Epoch 5/10\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.6238 - mse_loss: 1.6509 - kl_loss: 2.0936e-06\n",
            "Epoch 6/10\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.6746 - mse_loss: 1.6495 - kl_loss: 1.9173e-06\n",
            "Epoch 7/10\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.7016 - mse_loss: 1.6486 - kl_loss: 2.3271e-06\n",
            "Epoch 8/10\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.6881 - mse_loss: 1.6477 - kl_loss: 2.2302e-06\n",
            "Epoch 9/10\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.5747 - mse_loss: 1.6479 - kl_loss: 1.7385e-06\n",
            "Epoch 10/10\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.6530 - mse_loss: 1.6517 - kl_loss: 3.6409e-06\n",
            "12/12 [==============================] - 0s 2ms/step\n",
            "12/12 [==============================] - 0s 2ms/step\n",
            "Epoch 1/10\n",
            "10/10 [==============================] - 6s 12ms/step - loss: 1.5674 - mse_loss: 1.5751 - kl_loss: 2.7299e-06\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 1.5978 - mse_loss: 1.5685 - kl_loss: 1.5795e-06\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 1.5243 - mse_loss: 1.5652 - kl_loss: 2.1845e-06\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.5837 - mse_loss: 1.5622 - kl_loss: 2.0385e-06\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.5208 - mse_loss: 1.5589 - kl_loss: 1.4752e-06\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.5073 - mse_loss: 1.5572 - kl_loss: 3.3587e-06\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.5636 - mse_loss: 1.5577 - kl_loss: 2.2650e-06\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.5777 - mse_loss: 1.5579 - kl_loss: 3.0279e-06\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 1.5524 - mse_loss: 1.5540 - kl_loss: 1.7732e-06\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 1.5635 - mse_loss: 1.5531 - kl_loss: 7.8380e-07\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Epoch 1/10\n",
            "14/14 [==============================] - 6s 12ms/step - loss: 1.6869 - mse_loss: 1.7082 - kl_loss: 5.6177e-06\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.6605 - mse_loss: 1.6955 - kl_loss: 4.2873e-06\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.6838 - mse_loss: 1.6911 - kl_loss: 2.2799e-06\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.7187 - mse_loss: 1.6895 - kl_loss: 1.9521e-06\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.6790 - mse_loss: 1.6860 - kl_loss: 2.5119e-06\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.7427 - mse_loss: 1.6861 - kl_loss: 2.1373e-06\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.5918 - mse_loss: 1.6846 - kl_loss: 3.3826e-06\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.7021 - mse_loss: 1.6841 - kl_loss: 2.5077e-06\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.6303 - mse_loss: 1.6854 - kl_loss: 3.4358e-06\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.7020 - mse_loss: 1.6845 - kl_loss: 1.9691e-06\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Epoch 1/10\n",
            "12/12 [==============================] - 6s 12ms/step - loss: 1.8830 - mse_loss: 1.8840 - kl_loss: 4.8851e-06\n",
            "Epoch 2/10\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.9151 - mse_loss: 1.8665 - kl_loss: 1.5919e-06\n",
            "Epoch 3/10\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.8571 - mse_loss: 1.8634 - kl_loss: 2.4289e-06\n",
            "Epoch 4/10\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.8319 - mse_loss: 1.8601 - kl_loss: 1.4603e-06\n",
            "Epoch 5/10\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.8937 - mse_loss: 1.8575 - kl_loss: 1.9570e-06\n",
            "Epoch 6/10\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.9035 - mse_loss: 1.8559 - kl_loss: 1.6491e-06\n",
            "Epoch 7/10\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.8657 - mse_loss: 1.8641 - kl_loss: 4.0581e-06\n",
            "Epoch 8/10\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.8763 - mse_loss: 1.8560 - kl_loss: 2.2501e-06\n",
            "Epoch 9/10\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.8571 - mse_loss: 1.8558 - kl_loss: 1.9918e-06\n",
            "Epoch 10/10\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.8667 - mse_loss: 1.8567 - kl_loss: 2.1582e-06\n",
            "12/12 [==============================] - 0s 2ms/step\n",
            "12/12 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_prep, y_test_prep = VAE_augmentation(X_test_prep,y_test_prep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00JG2wLnyWQ6",
        "outputId": "6c3ac492-20da-4ba1-d7fb-224d5ffe8f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "14/14 [==============================] - 7s 12ms/step - loss: 1.6134 - mse_loss: 1.6411 - kl_loss: 1.6945e-06\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.6284 - mse_loss: 1.6279 - kl_loss: 2.0989e-06\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.6026 - mse_loss: 1.6230 - kl_loss: 1.3305e-06\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.6205 - mse_loss: 1.6229 - kl_loss: 1.3390e-06\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.6481 - mse_loss: 1.6198 - kl_loss: 1.2198e-06\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.6857 - mse_loss: 1.6215 - kl_loss: 2.3480e-06\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.6119 - mse_loss: 1.6213 - kl_loss: 1.4773e-06\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.5905 - mse_loss: 1.6182 - kl_loss: 1.6711e-06\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.6038 - mse_loss: 1.6187 - kl_loss: 1.7775e-06\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.6530 - mse_loss: 1.6199 - kl_loss: 2.6205e-06\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "Epoch 1/10\n",
            "16/16 [==============================] - 6s 12ms/step - loss: 1.5801 - mse_loss: 1.5397 - kl_loss: 2.2296e-06\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 1.5052 - mse_loss: 1.5331 - kl_loss: 2.6505e-06\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.4950 - mse_loss: 1.5290 - kl_loss: 2.4587e-06\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 1.5648 - mse_loss: 1.5311 - kl_loss: 3.3006e-06\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 1.5279 - mse_loss: 1.5292 - kl_loss: 2.0564e-06\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 1.5188 - mse_loss: 1.5262 - kl_loss: 1.6801e-06\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 1.4709 - mse_loss: 1.5249 - kl_loss: 1.0580e-06\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 1.5328 - mse_loss: 1.5247 - kl_loss: 1.5423e-06\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 1.5390 - mse_loss: 1.5264 - kl_loss: 1.2759e-06\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 1.5503 - mse_loss: 1.5287 - kl_loss: 3.2522e-06\n",
            "16/16 [==============================] - 0s 2ms/step\n",
            "16/16 [==============================] - 0s 2ms/step\n",
            "Epoch 1/10\n",
            "12/12 [==============================] - 6s 12ms/step - loss: 1.7327 - mse_loss: 1.7186 - kl_loss: 5.0515e-06\n",
            "Epoch 2/10\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.6905 - mse_loss: 1.7063 - kl_loss: 4.9373e-06\n",
            "Epoch 3/10\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.6558 - mse_loss: 1.7012 - kl_loss: 2.9951e-06\n",
            "Epoch 4/10\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.7092 - mse_loss: 1.6999 - kl_loss: 1.9868e-06\n",
            "Epoch 5/10\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.6829 - mse_loss: 1.6955 - kl_loss: 3.4024e-06\n",
            "Epoch 6/10\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.6976 - mse_loss: 1.6960 - kl_loss: 1.7981e-06\n",
            "Epoch 7/10\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.6875 - mse_loss: 1.6987 - kl_loss: 2.5208e-06\n",
            "Epoch 8/10\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.7023 - mse_loss: 1.6947 - kl_loss: 2.9728e-06\n",
            "Epoch 9/10\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.6570 - mse_loss: 1.6946 - kl_loss: 1.5075e-06\n",
            "Epoch 10/10\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.6704 - mse_loss: 1.6929 - kl_loss: 1.9297e-06\n",
            "12/12 [==============================] - 0s 2ms/step\n",
            "12/12 [==============================] - 0s 2ms/step\n",
            "Epoch 1/10\n",
            "14/14 [==============================] - 6s 12ms/step - loss: 1.8365 - mse_loss: 1.8195 - kl_loss: 5.8008e-06\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.8005 - mse_loss: 1.8047 - kl_loss: 3.9169e-06\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.7784 - mse_loss: 1.7970 - kl_loss: 1.7243e-06\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.8246 - mse_loss: 1.7958 - kl_loss: 2.6843e-06\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.8176 - mse_loss: 1.7956 - kl_loss: 3.4209e-06\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.7998 - mse_loss: 1.7933 - kl_loss: 3.9956e-06\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.7948 - mse_loss: 1.7898 - kl_loss: 1.8861e-06\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.7488 - mse_loss: 1.7897 - kl_loss: 2.1011e-06\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.7173 - mse_loss: 1.7906 - kl_loss: 3.8573e-06\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.8376 - mse_loss: 1.7910 - kl_loss: 2.1436e-06\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the labels to categorical variables for multiclass classification\n",
        "y_train = to_categorical(y_train, 4)\n",
        "y_valid = to_categorical(y_valid, 4)\n",
        "y_test = to_categorical(y_test_prep, 4)\n",
        "\n",
        "y_train_sub1 = to_categorical(y_train_sub1, 4)\n",
        "y_valid_sub1 = to_categorical(y_valid_sub1, 4)\n",
        "y_test_sub1 = to_categorical(y_test_prep_sub1, 4)\n",
        "\n",
        "# print('Shape of training labels after categorical conversion:',y_train.shape)\n",
        "# print('Shape of validation labels after categorical conversion:',y_valid.shape)\n",
        "# print('Shape of test labels after categorical conversion:',y_test.shape)\n",
        "\n",
        "# Adding width of the segment to be 1\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
        "x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
        "x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n",
        "\n",
        "x_train_sub1 = x_train_sub1.reshape(x_train_sub1.shape[0], x_train_sub1.shape[1], x_train_sub1.shape[2], 1)\n",
        "x_valid_sub1 = x_valid_sub1.reshape(x_valid_sub1.shape[0], x_valid_sub1.shape[1], x_train_sub1.shape[2], 1)\n",
        "x_test_sub1 = X_test_prep_sub1.reshape(X_test_prep_sub1.shape[0], X_test_prep_sub1.shape[1], X_test_prep_sub1.shape[2], 1)\n",
        "\n",
        "# print('Shape of training set after adding width info:',x_train.shape)\n",
        "# print('Shape of validation set after adding width info:',x_valid.shape)\n",
        "# print('Shape of test set after adding width info:',x_test.shape)\n",
        "\n",
        "# Reshaping the training and validation dataset\n",
        "x_train = np.swapaxes(x_train, 1,3)\n",
        "x_train = np.swapaxes(x_train, 1,2)\n",
        "x_valid = np.swapaxes(x_valid, 1,3)\n",
        "x_valid = np.swapaxes(x_valid, 1,2)\n",
        "x_test = np.swapaxes(x_test, 1,3)\n",
        "x_test = np.swapaxes(x_test, 1,2)\n",
        "\n",
        "x_train_sub1 = np.swapaxes(x_train_sub1, 1,3)\n",
        "x_train_sub1 = np.swapaxes(x_train_sub1, 1,2)\n",
        "x_valid_sub1 = np.swapaxes(x_valid_sub1, 1,3)\n",
        "x_valid_sub1 = np.swapaxes(x_valid_sub1, 1,2)\n",
        "x_test_sub1 = np.swapaxes(x_test_sub1, 1,3)\n",
        "x_test_sub1 = np.swapaxes(x_test_sub1, 1,2)\n",
        "# print('Shape of training set after dimension reshaping:',x_train.shape)\n",
        "# print('Shape of validation set after dimension reshaping:',x_valid.shape)\n",
        "# print('Shape of test set after dimension reshaping:',x_test.shape)"
      ],
      "metadata": {
        "id": "OGv7U8WNyWTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training on Subject 1 data"
      ],
      "metadata": {
        "id": "415107ZV0Hwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the CNN model using sequential class\n",
        "def cnn_func(dropout):\n",
        "  dp = dropout # set dropout value for tuning \n",
        "  basic_cnn_model = Sequential()\n",
        "\n",
        "  # Conv. block 1\n",
        "  basic_cnn_model.add(Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(250,1,22)))\n",
        "  basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) # Read the keras documentation\n",
        "  basic_cnn_model.add(BatchNormalization())\n",
        "  basic_cnn_model.add(Dropout(dp))\n",
        "\n",
        "  # Conv. block 2\n",
        "  basic_cnn_model.add(Conv2D(filters=50, kernel_size=(10,1), padding='same', activation='elu'))\n",
        "  basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
        "  basic_cnn_model.add(BatchNormalization())\n",
        "  basic_cnn_model.add(Dropout(dp))\n",
        "\n",
        "  # Conv. block 3\n",
        "  basic_cnn_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
        "  basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
        "  basic_cnn_model.add(BatchNormalization())\n",
        "  basic_cnn_model.add(Dropout(dp))\n",
        "\n",
        "  # Conv. block 4\n",
        "  basic_cnn_model.add(Conv2D(filters=200, kernel_size=(10,1), padding='same', activation='elu'))\n",
        "  basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
        "  basic_cnn_model.add(BatchNormalization())\n",
        "  basic_cnn_model.add(Dropout(dp))\n",
        "\n",
        "  # Output layer with Softmax activation\n",
        "  basic_cnn_model.add(Flatten()) # Flattens the input\n",
        "  basic_cnn_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
        "\n",
        "  return basic_cnn_model\n",
        "\n",
        "# Printing the model summary\n",
        "# basic_cnn_model.summary()"
      ],
      "metadata": {
        "id": "SOAM5JQBx3_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimal hyperparameters\n",
        "lr = 1e-3\n",
        "epoch = 75\n",
        "bs = 64\n",
        "drop = 0.6"
      ],
      "metadata": {
        "id": "SuI2gAcv0gAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train over only subject 1\n",
        "# Initiate CNN model \n",
        "basic_cnn_model = cnn_func(drop)\n",
        "# Setting model parameters \n",
        "cnn_optimizer = keras.optimizers.Adam(learning_rate = lr)\n",
        "basic_cnn_model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=cnn_optimizer,\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Training and validating the model\n",
        "basic_cnn_model_results = basic_cnn_model.fit(x_train_sub1,\n",
        "            y_train_sub1,\n",
        "            batch_size=bs,\n",
        "            epochs=epoch,\n",
        "            validation_data=(x_valid_sub1, y_valid_sub1), verbose=True)\n",
        "# Testing model\n",
        "cnn_score_1 = basic_cnn_model.evaluate(x_test_sub1, y_test_sub1, verbose=0)\n",
        "print('Test accuracy of the CNN model over subject 1 data:',cnn_score_1[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K73jZaNf3avI",
        "outputId": "4976fbb2-5b17-44c2-de2d-9b988bd70e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/75\n",
            "24/24 [==============================] - 4s 20ms/step - loss: 1.6731 - accuracy: 0.6078 - val_loss: 3.6283 - val_accuracy: 0.6354\n",
            "Epoch 2/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 1.3047 - accuracy: 0.6554 - val_loss: 1.5480 - val_accuracy: 0.6589\n",
            "Epoch 3/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 1.1383 - accuracy: 0.6561 - val_loss: 1.1076 - val_accuracy: 0.6901\n",
            "Epoch 4/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 1.0637 - accuracy: 0.6733 - val_loss: 0.7755 - val_accuracy: 0.6745\n",
            "Epoch 5/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 1.0205 - accuracy: 0.6720 - val_loss: 0.8753 - val_accuracy: 0.6797\n",
            "Epoch 6/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.9218 - accuracy: 0.6964 - val_loss: 0.9539 - val_accuracy: 0.6719\n",
            "Epoch 7/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.9092 - accuracy: 0.6706 - val_loss: 0.8971 - val_accuracy: 0.6771\n",
            "Epoch 8/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.8296 - accuracy: 0.6997 - val_loss: 0.8021 - val_accuracy: 0.7031\n",
            "Epoch 9/75\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.8004 - accuracy: 0.7196 - val_loss: 0.8726 - val_accuracy: 0.6432\n",
            "Epoch 10/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.7110 - accuracy: 0.7229 - val_loss: 0.9216 - val_accuracy: 0.6120\n",
            "Epoch 11/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.6870 - accuracy: 0.7414 - val_loss: 1.0996 - val_accuracy: 0.6146\n",
            "Epoch 12/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.6788 - accuracy: 0.7381 - val_loss: 1.1275 - val_accuracy: 0.5964\n",
            "Epoch 13/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.6426 - accuracy: 0.7573 - val_loss: 1.1303 - val_accuracy: 0.6172\n",
            "Epoch 14/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.5305 - accuracy: 0.7923 - val_loss: 1.1997 - val_accuracy: 0.6406\n",
            "Epoch 15/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.5362 - accuracy: 0.7937 - val_loss: 1.2827 - val_accuracy: 0.6510\n",
            "Epoch 16/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.5089 - accuracy: 0.7917 - val_loss: 1.1821 - val_accuracy: 0.6302\n",
            "Epoch 17/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.4453 - accuracy: 0.8241 - val_loss: 1.0512 - val_accuracy: 0.6667\n",
            "Epoch 18/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.4135 - accuracy: 0.8333 - val_loss: 1.2151 - val_accuracy: 0.6536\n",
            "Epoch 19/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.4276 - accuracy: 0.8366 - val_loss: 1.1609 - val_accuracy: 0.6589\n",
            "Epoch 20/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.3918 - accuracy: 0.8472 - val_loss: 1.1067 - val_accuracy: 0.6693\n",
            "Epoch 21/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.3583 - accuracy: 0.8532 - val_loss: 1.1088 - val_accuracy: 0.6849\n",
            "Epoch 22/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.3215 - accuracy: 0.8710 - val_loss: 1.3407 - val_accuracy: 0.6562\n",
            "Epoch 23/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.3795 - accuracy: 0.8585 - val_loss: 1.1966 - val_accuracy: 0.6745\n",
            "Epoch 24/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.3315 - accuracy: 0.8724 - val_loss: 1.2123 - val_accuracy: 0.6641\n",
            "Epoch 25/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.3004 - accuracy: 0.8843 - val_loss: 1.1789 - val_accuracy: 0.6406\n",
            "Epoch 26/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2677 - accuracy: 0.8981 - val_loss: 1.2930 - val_accuracy: 0.6823\n",
            "Epoch 27/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.2951 - accuracy: 0.8942 - val_loss: 1.2429 - val_accuracy: 0.6641\n",
            "Epoch 28/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2669 - accuracy: 0.8929 - val_loss: 1.2458 - val_accuracy: 0.6745\n",
            "Epoch 29/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.2402 - accuracy: 0.9021 - val_loss: 1.1738 - val_accuracy: 0.6953\n",
            "Epoch 30/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.2253 - accuracy: 0.9094 - val_loss: 1.1038 - val_accuracy: 0.6927\n",
            "Epoch 31/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.2548 - accuracy: 0.9048 - val_loss: 1.2088 - val_accuracy: 0.6849\n",
            "Epoch 32/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.2190 - accuracy: 0.9253 - val_loss: 1.1271 - val_accuracy: 0.6979\n",
            "Epoch 33/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.2280 - accuracy: 0.9120 - val_loss: 0.9467 - val_accuracy: 0.7083\n",
            "Epoch 34/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.1964 - accuracy: 0.9193 - val_loss: 0.9456 - val_accuracy: 0.7240\n",
            "Epoch 35/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.1845 - accuracy: 0.9272 - val_loss: 1.0309 - val_accuracy: 0.7214\n",
            "Epoch 36/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.1931 - accuracy: 0.9259 - val_loss: 1.0151 - val_accuracy: 0.7188\n",
            "Epoch 37/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1925 - accuracy: 0.9259 - val_loss: 1.1020 - val_accuracy: 0.7214\n",
            "Epoch 38/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1708 - accuracy: 0.9312 - val_loss: 0.9088 - val_accuracy: 0.7188\n",
            "Epoch 39/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.1534 - accuracy: 0.9372 - val_loss: 1.0079 - val_accuracy: 0.7083\n",
            "Epoch 40/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1868 - accuracy: 0.9345 - val_loss: 0.9664 - val_accuracy: 0.7292\n",
            "Epoch 41/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1440 - accuracy: 0.9471 - val_loss: 1.0163 - val_accuracy: 0.7188\n",
            "Epoch 42/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1489 - accuracy: 0.9444 - val_loss: 0.8682 - val_accuracy: 0.7344\n",
            "Epoch 43/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1352 - accuracy: 0.9491 - val_loss: 0.9457 - val_accuracy: 0.7266\n",
            "Epoch 44/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.1344 - accuracy: 0.9517 - val_loss: 0.8686 - val_accuracy: 0.7396\n",
            "Epoch 45/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.1353 - accuracy: 0.9451 - val_loss: 1.1176 - val_accuracy: 0.7474\n",
            "Epoch 46/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1543 - accuracy: 0.9458 - val_loss: 1.0029 - val_accuracy: 0.7552\n",
            "Epoch 47/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1235 - accuracy: 0.9577 - val_loss: 1.0002 - val_accuracy: 0.7604\n",
            "Epoch 48/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1258 - accuracy: 0.9537 - val_loss: 0.9538 - val_accuracy: 0.7500\n",
            "Epoch 49/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.1361 - accuracy: 0.9491 - val_loss: 1.1350 - val_accuracy: 0.7292\n",
            "Epoch 50/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.1171 - accuracy: 0.9524 - val_loss: 1.0192 - val_accuracy: 0.7734\n",
            "Epoch 51/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.1111 - accuracy: 0.9636 - val_loss: 1.1439 - val_accuracy: 0.7318\n",
            "Epoch 52/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.1194 - accuracy: 0.9563 - val_loss: 1.0625 - val_accuracy: 0.7396\n",
            "Epoch 53/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1333 - accuracy: 0.9464 - val_loss: 1.0033 - val_accuracy: 0.7500\n",
            "Epoch 54/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.1068 - accuracy: 0.9577 - val_loss: 1.1348 - val_accuracy: 0.7344\n",
            "Epoch 55/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.0913 - accuracy: 0.9656 - val_loss: 1.0826 - val_accuracy: 0.7578\n",
            "Epoch 56/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.0899 - accuracy: 0.9676 - val_loss: 1.1302 - val_accuracy: 0.7448\n",
            "Epoch 57/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.0906 - accuracy: 0.9689 - val_loss: 1.0444 - val_accuracy: 0.7578\n",
            "Epoch 58/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0806 - accuracy: 0.9696 - val_loss: 0.9272 - val_accuracy: 0.7500\n",
            "Epoch 59/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.0868 - accuracy: 0.9702 - val_loss: 0.9449 - val_accuracy: 0.7474\n",
            "Epoch 60/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.0790 - accuracy: 0.9729 - val_loss: 0.9841 - val_accuracy: 0.7656\n",
            "Epoch 61/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.0855 - accuracy: 0.9656 - val_loss: 1.0561 - val_accuracy: 0.7500\n",
            "Epoch 62/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0884 - accuracy: 0.9689 - val_loss: 1.0905 - val_accuracy: 0.7630\n",
            "Epoch 63/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0996 - accuracy: 0.9636 - val_loss: 0.9438 - val_accuracy: 0.7734\n",
            "Epoch 64/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1160 - accuracy: 0.9597 - val_loss: 0.8680 - val_accuracy: 0.7812\n",
            "Epoch 65/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1051 - accuracy: 0.9603 - val_loss: 0.9802 - val_accuracy: 0.7422\n",
            "Epoch 66/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.0610 - accuracy: 0.9769 - val_loss: 0.9689 - val_accuracy: 0.7552\n",
            "Epoch 67/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0617 - accuracy: 0.9775 - val_loss: 1.0351 - val_accuracy: 0.7448\n",
            "Epoch 68/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.0674 - accuracy: 0.9795 - val_loss: 1.1110 - val_accuracy: 0.7604\n",
            "Epoch 69/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.0703 - accuracy: 0.9729 - val_loss: 1.0792 - val_accuracy: 0.7708\n",
            "Epoch 70/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.0687 - accuracy: 0.9742 - val_loss: 1.0360 - val_accuracy: 0.7526\n",
            "Epoch 71/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.0702 - accuracy: 0.9716 - val_loss: 1.0989 - val_accuracy: 0.7422\n",
            "Epoch 72/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0736 - accuracy: 0.9749 - val_loss: 1.0378 - val_accuracy: 0.7526\n",
            "Epoch 73/75\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0795 - accuracy: 0.9722 - val_loss: 1.1543 - val_accuracy: 0.7500\n",
            "Epoch 74/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.0888 - accuracy: 0.9709 - val_loss: 1.1398 - val_accuracy: 0.7266\n",
            "Epoch 75/75\n",
            "24/24 [==============================] - 0s 8ms/step - loss: 0.0683 - accuracy: 0.9769 - val_loss: 1.0369 - val_accuracy: 0.7734\n",
            "Test accuracy of the CNN model over subject 1 data: 0.8475000262260437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting accuracy trajectory\n",
        "plt.plot(basic_cnn_model_results.history['accuracy'])\n",
        "plt.plot(basic_cnn_model_results.history['val_accuracy'])\n",
        "plt.title('Basic CNN model accuracy trajectory')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.title('Basic CNN model trained only on subject 1 data')\n",
        "plt.show()\n",
        "\n",
        "# Plotting loss trajectory\n",
        "plt.plot(basic_cnn_model_results.history['loss'],'o')\n",
        "plt.plot(basic_cnn_model_results.history['val_loss'],'o')\n",
        "plt.title('Basic CNN model loss trajectory')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.title('Basic CNN model trained only on subject 1 data')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "eu-tHELd4sOq",
        "outputId": "d5126ea3-6b40-4aa2-c086-008729a37657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training on all data"
      ],
      "metadata": {
        "id": "0zig-cEy5Hkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train over all subjects \n",
        "# Initiate CNN model \n",
        "basic_cnn_model = cnn_func(drop)\n",
        "# Setting model parameters \n",
        "cnn_optimizer = keras.optimizers.Adam(learning_rate = lr)\n",
        "basic_cnn_model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=cnn_optimizer,\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Training and validating the model\n",
        "basic_cnn_model_results = basic_cnn_model.fit(x_train,\n",
        "            y_train,\n",
        "            batch_size=bs,\n",
        "            epochs=epoch,\n",
        "            validation_data=(x_valid, y_valid), verbose=False)\n",
        "# Testing model\n",
        "cnn_score_all = basic_cnn_model.evaluate(x_test_sub1, y_test_sub1, verbose=0)\n",
        "print('Test accuracy of the CNN model over all data:',cnn_score_all[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkM8Zdtu0YPO",
        "outputId": "0c9a8934-4d80-4710-c8a8-41d07658f86c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy of the CNN model over all data: 0.8100000023841858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting accuracy trajectory\n",
        "plt.plot(basic_cnn_model_results.history['accuracy'])\n",
        "plt.plot(basic_cnn_model_results.history['val_accuracy'])\n",
        "plt.title('Basic CNN model accuracy trajectory')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.title('Basic CNN model trained only all subject data')\n",
        "plt.show()\n",
        "\n",
        "# Plotting loss trajectory\n",
        "plt.plot(basic_cnn_model_results.history['loss'],'o')\n",
        "plt.plot(basic_cnn_model_results.history['val_loss'],'o')\n",
        "plt.title('Basic CNN model loss trajectory')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.title('Basic CNN model trained only all subject data')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "V_qXCjGW4_P5",
        "outputId": "d5404862-d03d-42c1-f8e1-d684abf7835b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test accuracy of the CNN model over subject 1 data:',cnn_score_1[1])\n",
        "print('Test accuracy of the CNN model over all data:',cnn_score_all[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIOwzntf6NOc",
        "outputId": "eae0d177-e968-4b46-b0e5-3786724cc005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy of the CNN model over subject 1 data: 0.8475000262260437\n",
            "Test accuracy of the CNN model over all data: 0.8100000023841858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CNN model performed better while only training on subject 1 data."
      ],
      "metadata": {
        "id": "aOMqvs5e5Nxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: Classifier Accuracy\n",
        "\n",
        "Optimize the classification accuracy across all subjects. How does the classifier do? Do you notice any interesting trends?\n",
        "\n",
        "*   Can refer to Neural NEtwork Models with VAE section in EEG_loading notebook\n"
      ],
      "metadata": {
        "id": "sdhsFrR3z31L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: Time Accuracy\n",
        "\n",
        "\n",
        "\n",
        "Evaluate the classification accuracy as a function of time (e.g., does it increase as you have data over longer periods of time? How much time is required to get a reasonable classification accuracy?\n",
        "* Basic CNN (with best parameters) Testing Accuracies:\n",
        "  * 50: 0.7598758339881897\n",
        "  * 100: 0.8259029388427734\n",
        "  * 150: 0.8481941223144531\n",
        "  * 200: 0.8459367752075195\n",
        "  * 250: 0.8470654487609863\n",
        "* Shallow CNN Testing Accuracies\n",
        "  * 50: 0.7488713264465332\n",
        "  * 100: 0.7573363184928894\n",
        "  * 150: 0.7793453931808472\n",
        "  * 200: 0.7914785742759705\n",
        "  * 250: 0.7835778594017029\n",
        "* RNN-LSTM Testing Accuracies\n",
        "  * 50: 0.7291196584701538\n",
        "  * 100: 0.7542325258255005\n",
        "  * 150: 0.765237033367157\n",
        "  * 200: 0.7765237092971802\n",
        "  * 250: 0.7703160047531128\n"
      ],
      "metadata": {
        "id": "2rApvDzxz4_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_vae = np.load(\"drive/MyDrive/ecengr 247/Preprocessed_Data/x_train_vae.npy\")\n",
        "y_train_vae = np.load(\"drive/MyDrive/ecengr 247/Preprocessed_Data/y_train_vae.npy\")\n",
        "x_valid_vae = np.load(\"drive/MyDrive/ecengr 247/Preprocessed_Data/x_valid_vae.npy\")\n",
        "y_valid_vae = np.load(\"drive/MyDrive/ecengr 247/Preprocessed_Data/y_valid_vae.npy\")\n",
        "x_test_vae = np.load(\"drive/MyDrive/ecengr 247/Preprocessed_Data/x_test_vae.npy\")\n",
        "y_test_vae = np.load(\"drive/MyDrive/ecengr 247/Preprocessed_Data/y_test_vae.npy\")"
      ],
      "metadata": {
        "id": "xEszmOnlrAeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters from best performing CNN model\n",
        "learning_rate: 0.001\n",
        "epochs = 75\n",
        "cnn_optimizer = keras.optimizers.Adam(lr=learning_rate)"
      ],
      "metadata": {
        "id": "s8smy7g6640-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic CNN"
      ],
      "metadata": {
        "id": "zJbRBpWE6SrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_cnn_time(n):\n",
        "  # Building the CNN model using sequential class\n",
        "  basic_cnn_model = Sequential()\n",
        "\n",
        "  # Conv. block 1\n",
        "  basic_cnn_model.add(Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(n,1,22)))\n",
        "  basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) # Read the keras documentation\n",
        "  basic_cnn_model.add(BatchNormalization())\n",
        "  basic_cnn_model.add(Dropout(0.6))\n",
        "\n",
        "  # Conv. block 2\n",
        "  basic_cnn_model.add(Conv2D(filters=50, kernel_size=(10,1), padding='same', activation='elu'))\n",
        "  basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
        "  basic_cnn_model.add(BatchNormalization())\n",
        "  basic_cnn_model.add(Dropout(0.6))\n",
        "\n",
        "  # Conv. block 3\n",
        "  basic_cnn_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
        "  basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
        "  basic_cnn_model.add(BatchNormalization())\n",
        "  basic_cnn_model.add(Dropout(0.6))\n",
        "\n",
        "  # Conv. block 4\n",
        "  basic_cnn_model.add(Conv2D(filters=200, kernel_size=(10,1), padding='same', activation='elu'))\n",
        "  basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
        "  basic_cnn_model.add(BatchNormalization())\n",
        "  basic_cnn_model.add(Dropout(0.6))\n",
        "\n",
        "  # Output layer with Softmax activation\n",
        "  basic_cnn_model.add(Flatten()) # Flattens the input\n",
        "  basic_cnn_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
        "\n",
        "  # Compiling the model\n",
        "  basic_cnn_model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=cnn_optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "  \n",
        "  return basic_cnn_model\n",
        "  "
      ],
      "metadata": {
        "id": "xH30oZNtx22E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num = [50, 100, 150, 200, 250]\n",
        "\n",
        "basic_cnn_dict = {}\n",
        "\n",
        "for n in num:\n",
        "  x_train = x_train_vae[:, :n, :, :]\n",
        "  x_train = np.reshape(x_train, (13920, n, 1, 22))\n",
        "\n",
        "  x_valid = x_valid_vae[:, :n, :, :]\n",
        "  x_valid = np.reshape(x_valid, (3000, n, 1, 22))\n",
        "\n",
        "  x_test = x_test_vae[:, :n, :, :]\n",
        "  x_test = np.reshape(x_test, (3544, n, 1, 22)) \n",
        "  \n",
        "  basic_cnn_model = basic_cnn_time(n)\n",
        "\n",
        "  basic_cnn_model_results = basic_cnn_model.fit(x_train,\n",
        "              y_train_vae,\n",
        "              batch_size=64,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_valid, y_valid_vae), verbose=True)\n",
        "  \n",
        "  cnn_score = basic_cnn_model.evaluate(x_test, y_test_vae, verbose=0)\n",
        "\n",
        "  basic_cnn_dict[n] = cnn_score[1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcTZxTR6xebe",
        "outputId": "615ecb35-e12c-4e8d-e1d3-e624ea867a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 1.3323 - accuracy: 0.6099 - val_loss: 0.7074 - val_accuracy: 0.6447\n",
            "Epoch 2/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.9058 - accuracy: 0.6364 - val_loss: 0.7133 - val_accuracy: 0.6413\n",
            "Epoch 3/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.8051 - accuracy: 0.6438 - val_loss: 0.6835 - val_accuracy: 0.6660\n",
            "Epoch 4/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.7528 - accuracy: 0.6434 - val_loss: 0.6930 - val_accuracy: 0.6523\n",
            "Epoch 5/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.7344 - accuracy: 0.6478 - val_loss: 0.6751 - val_accuracy: 0.6897\n",
            "Epoch 6/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.7187 - accuracy: 0.6570 - val_loss: 0.6666 - val_accuracy: 0.6977\n",
            "Epoch 7/75\n",
            "218/218 [==============================] - 2s 8ms/step - loss: 0.7019 - accuracy: 0.6655 - val_loss: 0.6543 - val_accuracy: 0.7143\n",
            "Epoch 8/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.6958 - accuracy: 0.6687 - val_loss: 0.6482 - val_accuracy: 0.7193\n",
            "Epoch 9/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6878 - accuracy: 0.6731 - val_loss: 0.6421 - val_accuracy: 0.7200\n",
            "Epoch 10/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.6816 - accuracy: 0.6794 - val_loss: 0.6276 - val_accuracy: 0.7353\n",
            "Epoch 11/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6731 - accuracy: 0.6889 - val_loss: 0.6263 - val_accuracy: 0.7307\n",
            "Epoch 12/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6706 - accuracy: 0.6894 - val_loss: 0.6202 - val_accuracy: 0.7290\n",
            "Epoch 13/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6562 - accuracy: 0.6983 - val_loss: 0.6117 - val_accuracy: 0.7467\n",
            "Epoch 14/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6527 - accuracy: 0.7022 - val_loss: 0.5963 - val_accuracy: 0.7493\n",
            "Epoch 15/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6459 - accuracy: 0.7050 - val_loss: 0.5847 - val_accuracy: 0.7537\n",
            "Epoch 16/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.6367 - accuracy: 0.7111 - val_loss: 0.6020 - val_accuracy: 0.7497\n",
            "Epoch 17/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.6332 - accuracy: 0.7125 - val_loss: 0.5950 - val_accuracy: 0.7377\n",
            "Epoch 18/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6313 - accuracy: 0.7210 - val_loss: 0.5770 - val_accuracy: 0.7480\n",
            "Epoch 19/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6116 - accuracy: 0.7287 - val_loss: 0.5762 - val_accuracy: 0.7617\n",
            "Epoch 20/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6094 - accuracy: 0.7292 - val_loss: 0.5583 - val_accuracy: 0.7780\n",
            "Epoch 21/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6072 - accuracy: 0.7340 - val_loss: 0.5686 - val_accuracy: 0.7667\n",
            "Epoch 22/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6015 - accuracy: 0.7331 - val_loss: 0.5718 - val_accuracy: 0.7553\n",
            "Epoch 23/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5955 - accuracy: 0.7411 - val_loss: 0.5481 - val_accuracy: 0.7847\n",
            "Epoch 24/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5895 - accuracy: 0.7441 - val_loss: 0.5343 - val_accuracy: 0.7810\n",
            "Epoch 25/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5871 - accuracy: 0.7445 - val_loss: 0.5458 - val_accuracy: 0.7743\n",
            "Epoch 26/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.5822 - accuracy: 0.7478 - val_loss: 0.5595 - val_accuracy: 0.7677\n",
            "Epoch 27/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5780 - accuracy: 0.7515 - val_loss: 0.5526 - val_accuracy: 0.7727\n",
            "Epoch 28/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5693 - accuracy: 0.7560 - val_loss: 0.5592 - val_accuracy: 0.7620\n",
            "Epoch 29/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5635 - accuracy: 0.7569 - val_loss: 0.5384 - val_accuracy: 0.7677\n",
            "Epoch 30/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5688 - accuracy: 0.7557 - val_loss: 0.5372 - val_accuracy: 0.7783\n",
            "Epoch 31/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5579 - accuracy: 0.7649 - val_loss: 0.5388 - val_accuracy: 0.7720\n",
            "Epoch 32/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5571 - accuracy: 0.7633 - val_loss: 0.5524 - val_accuracy: 0.7623\n",
            "Epoch 33/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5528 - accuracy: 0.7612 - val_loss: 0.5372 - val_accuracy: 0.7667\n",
            "Epoch 34/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5527 - accuracy: 0.7670 - val_loss: 0.5598 - val_accuracy: 0.7603\n",
            "Epoch 35/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5510 - accuracy: 0.7600 - val_loss: 0.5388 - val_accuracy: 0.7783\n",
            "Epoch 36/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5466 - accuracy: 0.7675 - val_loss: 0.5522 - val_accuracy: 0.7683\n",
            "Epoch 37/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5402 - accuracy: 0.7707 - val_loss: 0.5397 - val_accuracy: 0.7647\n",
            "Epoch 38/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5332 - accuracy: 0.7712 - val_loss: 0.5467 - val_accuracy: 0.7723\n",
            "Epoch 39/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5402 - accuracy: 0.7688 - val_loss: 0.5317 - val_accuracy: 0.7847\n",
            "Epoch 40/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5306 - accuracy: 0.7732 - val_loss: 0.5312 - val_accuracy: 0.7760\n",
            "Epoch 41/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5320 - accuracy: 0.7753 - val_loss: 0.5363 - val_accuracy: 0.7810\n",
            "Epoch 42/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5359 - accuracy: 0.7713 - val_loss: 0.5500 - val_accuracy: 0.7737\n",
            "Epoch 43/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5202 - accuracy: 0.7796 - val_loss: 0.5451 - val_accuracy: 0.7740\n",
            "Epoch 44/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5249 - accuracy: 0.7764 - val_loss: 0.5415 - val_accuracy: 0.7783\n",
            "Epoch 45/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5279 - accuracy: 0.7734 - val_loss: 0.5397 - val_accuracy: 0.7767\n",
            "Epoch 46/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5204 - accuracy: 0.7765 - val_loss: 0.5521 - val_accuracy: 0.7720\n",
            "Epoch 47/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5202 - accuracy: 0.7820 - val_loss: 0.5347 - val_accuracy: 0.7790\n",
            "Epoch 48/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5159 - accuracy: 0.7828 - val_loss: 0.5299 - val_accuracy: 0.7850\n",
            "Epoch 49/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5157 - accuracy: 0.7866 - val_loss: 0.5307 - val_accuracy: 0.7803\n",
            "Epoch 50/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5164 - accuracy: 0.7819 - val_loss: 0.5477 - val_accuracy: 0.7750\n",
            "Epoch 51/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5024 - accuracy: 0.7873 - val_loss: 0.5401 - val_accuracy: 0.7743\n",
            "Epoch 52/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5116 - accuracy: 0.7835 - val_loss: 0.5578 - val_accuracy: 0.7640\n",
            "Epoch 53/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5156 - accuracy: 0.7805 - val_loss: 0.5428 - val_accuracy: 0.7727\n",
            "Epoch 54/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5031 - accuracy: 0.7874 - val_loss: 0.5318 - val_accuracy: 0.7783\n",
            "Epoch 55/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5015 - accuracy: 0.7907 - val_loss: 0.5254 - val_accuracy: 0.7867\n",
            "Epoch 56/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4985 - accuracy: 0.7917 - val_loss: 0.5360 - val_accuracy: 0.7817\n",
            "Epoch 57/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5023 - accuracy: 0.7898 - val_loss: 0.5288 - val_accuracy: 0.7823\n",
            "Epoch 58/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4945 - accuracy: 0.7958 - val_loss: 0.5448 - val_accuracy: 0.7747\n",
            "Epoch 59/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4986 - accuracy: 0.7904 - val_loss: 0.5331 - val_accuracy: 0.7870\n",
            "Epoch 60/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5021 - accuracy: 0.7921 - val_loss: 0.5285 - val_accuracy: 0.7760\n",
            "Epoch 61/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4996 - accuracy: 0.7896 - val_loss: 0.5245 - val_accuracy: 0.7863\n",
            "Epoch 62/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4879 - accuracy: 0.7921 - val_loss: 0.5255 - val_accuracy: 0.7890\n",
            "Epoch 63/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5006 - accuracy: 0.7894 - val_loss: 0.5313 - val_accuracy: 0.7793\n",
            "Epoch 64/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4850 - accuracy: 0.7985 - val_loss: 0.5277 - val_accuracy: 0.7860\n",
            "Epoch 65/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4825 - accuracy: 0.7996 - val_loss: 0.5361 - val_accuracy: 0.7853\n",
            "Epoch 66/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4934 - accuracy: 0.7961 - val_loss: 0.5289 - val_accuracy: 0.7797\n",
            "Epoch 67/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4860 - accuracy: 0.7977 - val_loss: 0.5343 - val_accuracy: 0.7827\n",
            "Epoch 68/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4858 - accuracy: 0.7971 - val_loss: 0.5415 - val_accuracy: 0.7713\n",
            "Epoch 69/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4844 - accuracy: 0.7976 - val_loss: 0.5290 - val_accuracy: 0.7897\n",
            "Epoch 70/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4821 - accuracy: 0.8005 - val_loss: 0.5149 - val_accuracy: 0.7823\n",
            "Epoch 71/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4812 - accuracy: 0.8022 - val_loss: 0.5263 - val_accuracy: 0.7833\n",
            "Epoch 72/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4783 - accuracy: 0.8019 - val_loss: 0.5382 - val_accuracy: 0.7770\n",
            "Epoch 73/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4834 - accuracy: 0.8016 - val_loss: 0.5284 - val_accuracy: 0.7840\n",
            "Epoch 74/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4738 - accuracy: 0.8027 - val_loss: 0.5305 - val_accuracy: 0.7813\n",
            "Epoch 75/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.4795 - accuracy: 0.8001 - val_loss: 0.5347 - val_accuracy: 0.7773\n",
            "Epoch 1/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.9584 - accuracy: 0.6346 - val_loss: 0.6998 - val_accuracy: 0.6577\n",
            "Epoch 2/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.7336 - accuracy: 0.6531 - val_loss: 0.6673 - val_accuracy: 0.6860\n",
            "Epoch 3/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.7098 - accuracy: 0.6607 - val_loss: 0.6685 - val_accuracy: 0.6717\n",
            "Epoch 4/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6951 - accuracy: 0.6709 - val_loss: 0.6581 - val_accuracy: 0.6993\n",
            "Epoch 5/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6700 - accuracy: 0.6856 - val_loss: 0.6500 - val_accuracy: 0.7060\n",
            "Epoch 6/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6555 - accuracy: 0.7002 - val_loss: 0.6279 - val_accuracy: 0.7217\n",
            "Epoch 7/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6389 - accuracy: 0.7121 - val_loss: 0.6019 - val_accuracy: 0.7443\n",
            "Epoch 8/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6328 - accuracy: 0.7135 - val_loss: 0.6078 - val_accuracy: 0.7410\n",
            "Epoch 9/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6099 - accuracy: 0.7292 - val_loss: 0.5705 - val_accuracy: 0.7413\n",
            "Epoch 10/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5976 - accuracy: 0.7335 - val_loss: 0.5540 - val_accuracy: 0.7737\n",
            "Epoch 11/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5932 - accuracy: 0.7381 - val_loss: 0.5650 - val_accuracy: 0.7653\n",
            "Epoch 12/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5782 - accuracy: 0.7457 - val_loss: 0.5580 - val_accuracy: 0.7563\n",
            "Epoch 13/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.5708 - accuracy: 0.7522 - val_loss: 0.5501 - val_accuracy: 0.7533\n",
            "Epoch 14/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5648 - accuracy: 0.7579 - val_loss: 0.5333 - val_accuracy: 0.7793\n",
            "Epoch 15/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5510 - accuracy: 0.7596 - val_loss: 0.5519 - val_accuracy: 0.7690\n",
            "Epoch 16/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5474 - accuracy: 0.7669 - val_loss: 0.5681 - val_accuracy: 0.7533\n",
            "Epoch 17/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5428 - accuracy: 0.7634 - val_loss: 0.5215 - val_accuracy: 0.7873\n",
            "Epoch 18/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5366 - accuracy: 0.7720 - val_loss: 0.5146 - val_accuracy: 0.7903\n",
            "Epoch 19/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5287 - accuracy: 0.7756 - val_loss: 0.5100 - val_accuracy: 0.7933\n",
            "Epoch 20/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5212 - accuracy: 0.7790 - val_loss: 0.5124 - val_accuracy: 0.7897\n",
            "Epoch 21/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5160 - accuracy: 0.7790 - val_loss: 0.5046 - val_accuracy: 0.7993\n",
            "Epoch 22/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5070 - accuracy: 0.7846 - val_loss: 0.5016 - val_accuracy: 0.7973\n",
            "Epoch 23/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.5094 - accuracy: 0.7848 - val_loss: 0.5125 - val_accuracy: 0.7830\n",
            "Epoch 24/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4959 - accuracy: 0.7932 - val_loss: 0.4875 - val_accuracy: 0.7993\n",
            "Epoch 25/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4904 - accuracy: 0.7971 - val_loss: 0.5293 - val_accuracy: 0.7780\n",
            "Epoch 26/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4921 - accuracy: 0.7951 - val_loss: 0.4842 - val_accuracy: 0.8057\n",
            "Epoch 27/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4850 - accuracy: 0.7957 - val_loss: 0.4813 - val_accuracy: 0.8010\n",
            "Epoch 28/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4769 - accuracy: 0.8027 - val_loss: 0.4696 - val_accuracy: 0.8013\n",
            "Epoch 29/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4815 - accuracy: 0.7986 - val_loss: 0.4750 - val_accuracy: 0.8053\n",
            "Epoch 30/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4725 - accuracy: 0.8057 - val_loss: 0.4826 - val_accuracy: 0.8057\n",
            "Epoch 31/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4715 - accuracy: 0.8057 - val_loss: 0.4656 - val_accuracy: 0.8210\n",
            "Epoch 32/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.4619 - accuracy: 0.8096 - val_loss: 0.5055 - val_accuracy: 0.7950\n",
            "Epoch 33/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4617 - accuracy: 0.8119 - val_loss: 0.4556 - val_accuracy: 0.8240\n",
            "Epoch 34/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4511 - accuracy: 0.8162 - val_loss: 0.4785 - val_accuracy: 0.8087\n",
            "Epoch 35/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4553 - accuracy: 0.8137 - val_loss: 0.4763 - val_accuracy: 0.8117\n",
            "Epoch 36/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4562 - accuracy: 0.8103 - val_loss: 0.4556 - val_accuracy: 0.8197\n",
            "Epoch 37/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4431 - accuracy: 0.8221 - val_loss: 0.4692 - val_accuracy: 0.8153\n",
            "Epoch 38/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4390 - accuracy: 0.8190 - val_loss: 0.4526 - val_accuracy: 0.8343\n",
            "Epoch 39/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4427 - accuracy: 0.8190 - val_loss: 0.4652 - val_accuracy: 0.8150\n",
            "Epoch 40/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4380 - accuracy: 0.8206 - val_loss: 0.4827 - val_accuracy: 0.8103\n",
            "Epoch 41/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4372 - accuracy: 0.8210 - val_loss: 0.4566 - val_accuracy: 0.8207\n",
            "Epoch 42/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.4366 - accuracy: 0.8188 - val_loss: 0.4600 - val_accuracy: 0.8103\n",
            "Epoch 43/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4304 - accuracy: 0.8198 - val_loss: 0.4544 - val_accuracy: 0.8233\n",
            "Epoch 44/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4321 - accuracy: 0.8233 - val_loss: 0.4431 - val_accuracy: 0.8350\n",
            "Epoch 45/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4266 - accuracy: 0.8271 - val_loss: 0.4666 - val_accuracy: 0.8130\n",
            "Epoch 46/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4248 - accuracy: 0.8251 - val_loss: 0.4345 - val_accuracy: 0.8340\n",
            "Epoch 47/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4201 - accuracy: 0.8279 - val_loss: 0.4466 - val_accuracy: 0.8323\n",
            "Epoch 48/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4214 - accuracy: 0.8298 - val_loss: 0.4378 - val_accuracy: 0.8310\n",
            "Epoch 49/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4148 - accuracy: 0.8315 - val_loss: 0.4597 - val_accuracy: 0.8143\n",
            "Epoch 50/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4184 - accuracy: 0.8293 - val_loss: 0.4438 - val_accuracy: 0.8323\n",
            "Epoch 51/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4021 - accuracy: 0.8382 - val_loss: 0.4406 - val_accuracy: 0.8350\n",
            "Epoch 52/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.4114 - accuracy: 0.8328 - val_loss: 0.4332 - val_accuracy: 0.8380\n",
            "Epoch 53/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4141 - accuracy: 0.8355 - val_loss: 0.4297 - val_accuracy: 0.8467\n",
            "Epoch 54/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4095 - accuracy: 0.8334 - val_loss: 0.4317 - val_accuracy: 0.8370\n",
            "Epoch 55/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4044 - accuracy: 0.8374 - val_loss: 0.4535 - val_accuracy: 0.8233\n",
            "Epoch 56/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4056 - accuracy: 0.8345 - val_loss: 0.4449 - val_accuracy: 0.8300\n",
            "Epoch 57/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3941 - accuracy: 0.8405 - val_loss: 0.4635 - val_accuracy: 0.8217\n",
            "Epoch 58/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4012 - accuracy: 0.8372 - val_loss: 0.4264 - val_accuracy: 0.8467\n",
            "Epoch 59/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3895 - accuracy: 0.8399 - val_loss: 0.4331 - val_accuracy: 0.8337\n",
            "Epoch 60/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3904 - accuracy: 0.8433 - val_loss: 0.4282 - val_accuracy: 0.8370\n",
            "Epoch 61/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3957 - accuracy: 0.8390 - val_loss: 0.4346 - val_accuracy: 0.8377\n",
            "Epoch 62/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3869 - accuracy: 0.8458 - val_loss: 0.4446 - val_accuracy: 0.8297\n",
            "Epoch 63/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3897 - accuracy: 0.8439 - val_loss: 0.4227 - val_accuracy: 0.8453\n",
            "Epoch 64/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3908 - accuracy: 0.8460 - val_loss: 0.4480 - val_accuracy: 0.8227\n",
            "Epoch 65/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3859 - accuracy: 0.8475 - val_loss: 0.4316 - val_accuracy: 0.8350\n",
            "Epoch 66/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3805 - accuracy: 0.8442 - val_loss: 0.4643 - val_accuracy: 0.8283\n",
            "Epoch 67/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3769 - accuracy: 0.8484 - val_loss: 0.4272 - val_accuracy: 0.8407\n",
            "Epoch 68/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3801 - accuracy: 0.8493 - val_loss: 0.4397 - val_accuracy: 0.8350\n",
            "Epoch 69/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3859 - accuracy: 0.8427 - val_loss: 0.4497 - val_accuracy: 0.8290\n",
            "Epoch 70/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3817 - accuracy: 0.8455 - val_loss: 0.4130 - val_accuracy: 0.8477\n",
            "Epoch 71/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.3733 - accuracy: 0.8520 - val_loss: 0.4295 - val_accuracy: 0.8323\n",
            "Epoch 72/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3772 - accuracy: 0.8499 - val_loss: 0.4346 - val_accuracy: 0.8377\n",
            "Epoch 73/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3752 - accuracy: 0.8503 - val_loss: 0.4231 - val_accuracy: 0.8327\n",
            "Epoch 74/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3698 - accuracy: 0.8514 - val_loss: 0.4499 - val_accuracy: 0.8277\n",
            "Epoch 75/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3610 - accuracy: 0.8571 - val_loss: 0.4287 - val_accuracy: 0.8377\n",
            "Epoch 1/75\n",
            "218/218 [==============================] - 3s 8ms/step - loss: 0.9590 - accuracy: 0.6433 - val_loss: 0.7060 - val_accuracy: 0.6613\n",
            "Epoch 2/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.7375 - accuracy: 0.6560 - val_loss: 0.6752 - val_accuracy: 0.6747\n",
            "Epoch 3/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.7097 - accuracy: 0.6633 - val_loss: 0.6773 - val_accuracy: 0.6527\n",
            "Epoch 4/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.6834 - accuracy: 0.6762 - val_loss: 0.6662 - val_accuracy: 0.6737\n",
            "Epoch 5/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6577 - accuracy: 0.6976 - val_loss: 0.6326 - val_accuracy: 0.7063\n",
            "Epoch 6/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6392 - accuracy: 0.7106 - val_loss: 0.5988 - val_accuracy: 0.7193\n",
            "Epoch 7/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6214 - accuracy: 0.7241 - val_loss: 0.5806 - val_accuracy: 0.7543\n",
            "Epoch 8/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6001 - accuracy: 0.7357 - val_loss: 0.5547 - val_accuracy: 0.7697\n",
            "Epoch 9/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5826 - accuracy: 0.7460 - val_loss: 0.5585 - val_accuracy: 0.7570\n",
            "Epoch 10/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5746 - accuracy: 0.7496 - val_loss: 0.5342 - val_accuracy: 0.7807\n",
            "Epoch 11/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5569 - accuracy: 0.7613 - val_loss: 0.5151 - val_accuracy: 0.7867\n",
            "Epoch 12/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5520 - accuracy: 0.7639 - val_loss: 0.5267 - val_accuracy: 0.7767\n",
            "Epoch 13/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.5332 - accuracy: 0.7784 - val_loss: 0.4994 - val_accuracy: 0.7843\n",
            "Epoch 14/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5240 - accuracy: 0.7794 - val_loss: 0.4912 - val_accuracy: 0.7897\n",
            "Epoch 15/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5044 - accuracy: 0.7902 - val_loss: 0.4656 - val_accuracy: 0.8050\n",
            "Epoch 16/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4987 - accuracy: 0.7947 - val_loss: 0.4556 - val_accuracy: 0.8180\n",
            "Epoch 17/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4929 - accuracy: 0.7917 - val_loss: 0.4863 - val_accuracy: 0.7843\n",
            "Epoch 18/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4785 - accuracy: 0.8027 - val_loss: 0.4457 - val_accuracy: 0.8137\n",
            "Epoch 19/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4699 - accuracy: 0.8060 - val_loss: 0.4482 - val_accuracy: 0.8137\n",
            "Epoch 20/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4636 - accuracy: 0.8103 - val_loss: 0.4544 - val_accuracy: 0.8067\n",
            "Epoch 21/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4608 - accuracy: 0.8121 - val_loss: 0.4360 - val_accuracy: 0.8210\n",
            "Epoch 22/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.4508 - accuracy: 0.8155 - val_loss: 0.4243 - val_accuracy: 0.8230\n",
            "Epoch 23/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.4500 - accuracy: 0.8186 - val_loss: 0.4287 - val_accuracy: 0.8197\n",
            "Epoch 24/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4408 - accuracy: 0.8203 - val_loss: 0.4246 - val_accuracy: 0.8220\n",
            "Epoch 25/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4397 - accuracy: 0.8182 - val_loss: 0.4188 - val_accuracy: 0.8253\n",
            "Epoch 26/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4299 - accuracy: 0.8250 - val_loss: 0.4499 - val_accuracy: 0.8177\n",
            "Epoch 27/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4160 - accuracy: 0.8310 - val_loss: 0.4232 - val_accuracy: 0.8290\n",
            "Epoch 28/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4277 - accuracy: 0.8276 - val_loss: 0.4300 - val_accuracy: 0.8173\n",
            "Epoch 29/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4093 - accuracy: 0.8361 - val_loss: 0.4303 - val_accuracy: 0.8267\n",
            "Epoch 30/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4127 - accuracy: 0.8324 - val_loss: 0.4139 - val_accuracy: 0.8377\n",
            "Epoch 31/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.3999 - accuracy: 0.8391 - val_loss: 0.4103 - val_accuracy: 0.8327\n",
            "Epoch 32/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.4065 - accuracy: 0.8364 - val_loss: 0.3949 - val_accuracy: 0.8440\n",
            "Epoch 33/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4064 - accuracy: 0.8371 - val_loss: 0.3938 - val_accuracy: 0.8487\n",
            "Epoch 34/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3993 - accuracy: 0.8417 - val_loss: 0.4006 - val_accuracy: 0.8360\n",
            "Epoch 35/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3985 - accuracy: 0.8417 - val_loss: 0.4305 - val_accuracy: 0.8230\n",
            "Epoch 36/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3915 - accuracy: 0.8434 - val_loss: 0.4021 - val_accuracy: 0.8433\n",
            "Epoch 37/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3815 - accuracy: 0.8503 - val_loss: 0.3768 - val_accuracy: 0.8490\n",
            "Epoch 38/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3775 - accuracy: 0.8499 - val_loss: 0.3973 - val_accuracy: 0.8430\n",
            "Epoch 39/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3830 - accuracy: 0.8492 - val_loss: 0.3887 - val_accuracy: 0.8503\n",
            "Epoch 40/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3730 - accuracy: 0.8529 - val_loss: 0.3930 - val_accuracy: 0.8503\n",
            "Epoch 41/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.3750 - accuracy: 0.8517 - val_loss: 0.3873 - val_accuracy: 0.8533\n",
            "Epoch 42/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3694 - accuracy: 0.8527 - val_loss: 0.3800 - val_accuracy: 0.8560\n",
            "Epoch 43/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3679 - accuracy: 0.8527 - val_loss: 0.3934 - val_accuracy: 0.8487\n",
            "Epoch 44/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3622 - accuracy: 0.8597 - val_loss: 0.3901 - val_accuracy: 0.8453\n",
            "Epoch 45/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3541 - accuracy: 0.8589 - val_loss: 0.3810 - val_accuracy: 0.8587\n",
            "Epoch 46/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3552 - accuracy: 0.8616 - val_loss: 0.3718 - val_accuracy: 0.8590\n",
            "Epoch 47/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3637 - accuracy: 0.8576 - val_loss: 0.3675 - val_accuracy: 0.8567\n",
            "Epoch 48/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3539 - accuracy: 0.8601 - val_loss: 0.3818 - val_accuracy: 0.8450\n",
            "Epoch 49/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3565 - accuracy: 0.8598 - val_loss: 0.3781 - val_accuracy: 0.8500\n",
            "Epoch 50/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.3454 - accuracy: 0.8644 - val_loss: 0.3817 - val_accuracy: 0.8587\n",
            "Epoch 51/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.3512 - accuracy: 0.8654 - val_loss: 0.3828 - val_accuracy: 0.8487\n",
            "Epoch 52/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3424 - accuracy: 0.8642 - val_loss: 0.3854 - val_accuracy: 0.8557\n",
            "Epoch 53/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3434 - accuracy: 0.8642 - val_loss: 0.3762 - val_accuracy: 0.8530\n",
            "Epoch 54/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3420 - accuracy: 0.8640 - val_loss: 0.3797 - val_accuracy: 0.8607\n",
            "Epoch 55/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3345 - accuracy: 0.8677 - val_loss: 0.3768 - val_accuracy: 0.8537\n",
            "Epoch 56/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3313 - accuracy: 0.8691 - val_loss: 0.3823 - val_accuracy: 0.8560\n",
            "Epoch 57/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3337 - accuracy: 0.8696 - val_loss: 0.3747 - val_accuracy: 0.8493\n",
            "Epoch 58/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3323 - accuracy: 0.8672 - val_loss: 0.3627 - val_accuracy: 0.8643\n",
            "Epoch 59/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.3222 - accuracy: 0.8762 - val_loss: 0.3552 - val_accuracy: 0.8670\n",
            "Epoch 60/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.3251 - accuracy: 0.8738 - val_loss: 0.3661 - val_accuracy: 0.8577\n",
            "Epoch 61/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3319 - accuracy: 0.8726 - val_loss: 0.3673 - val_accuracy: 0.8573\n",
            "Epoch 62/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3334 - accuracy: 0.8686 - val_loss: 0.3620 - val_accuracy: 0.8667\n",
            "Epoch 63/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3266 - accuracy: 0.8705 - val_loss: 0.3710 - val_accuracy: 0.8623\n",
            "Epoch 64/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3215 - accuracy: 0.8745 - val_loss: 0.3748 - val_accuracy: 0.8680\n",
            "Epoch 65/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3237 - accuracy: 0.8726 - val_loss: 0.3700 - val_accuracy: 0.8633\n",
            "Epoch 66/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3217 - accuracy: 0.8732 - val_loss: 0.3674 - val_accuracy: 0.8597\n",
            "Epoch 67/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3194 - accuracy: 0.8770 - val_loss: 0.3641 - val_accuracy: 0.8600\n",
            "Epoch 68/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3170 - accuracy: 0.8735 - val_loss: 0.3669 - val_accuracy: 0.8660\n",
            "Epoch 69/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.3142 - accuracy: 0.8777 - val_loss: 0.3582 - val_accuracy: 0.8617\n",
            "Epoch 70/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.3082 - accuracy: 0.8774 - val_loss: 0.3590 - val_accuracy: 0.8703\n",
            "Epoch 71/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3114 - accuracy: 0.8796 - val_loss: 0.3486 - val_accuracy: 0.8753\n",
            "Epoch 72/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3079 - accuracy: 0.8821 - val_loss: 0.3462 - val_accuracy: 0.8743\n",
            "Epoch 73/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3144 - accuracy: 0.8770 - val_loss: 0.3593 - val_accuracy: 0.8693\n",
            "Epoch 74/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3058 - accuracy: 0.8817 - val_loss: 0.3622 - val_accuracy: 0.8597\n",
            "Epoch 75/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3068 - accuracy: 0.8815 - val_loss: 0.3661 - val_accuracy: 0.8617\n",
            "Epoch 1/75\n",
            "218/218 [==============================] - 3s 8ms/step - loss: 1.0449 - accuracy: 0.6455 - val_loss: 0.6786 - val_accuracy: 0.6687\n",
            "Epoch 2/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.7518 - accuracy: 0.6605 - val_loss: 0.6592 - val_accuracy: 0.6877\n",
            "Epoch 3/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6946 - accuracy: 0.6769 - val_loss: 0.6382 - val_accuracy: 0.7033\n",
            "Epoch 4/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6637 - accuracy: 0.6935 - val_loss: 0.6139 - val_accuracy: 0.7227\n",
            "Epoch 5/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6406 - accuracy: 0.7073 - val_loss: 0.6045 - val_accuracy: 0.7257\n",
            "Epoch 6/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6209 - accuracy: 0.7195 - val_loss: 0.5975 - val_accuracy: 0.7363\n",
            "Epoch 7/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6070 - accuracy: 0.7339 - val_loss: 0.6017 - val_accuracy: 0.7183\n",
            "Epoch 8/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5966 - accuracy: 0.7382 - val_loss: 0.5661 - val_accuracy: 0.7460\n",
            "Epoch 9/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5707 - accuracy: 0.7552 - val_loss: 0.5399 - val_accuracy: 0.7613\n",
            "Epoch 10/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5595 - accuracy: 0.7606 - val_loss: 0.5395 - val_accuracy: 0.7613\n",
            "Epoch 11/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.5393 - accuracy: 0.7726 - val_loss: 0.5139 - val_accuracy: 0.7773\n",
            "Epoch 12/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.5232 - accuracy: 0.7792 - val_loss: 0.5174 - val_accuracy: 0.7823\n",
            "Epoch 13/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5095 - accuracy: 0.7889 - val_loss: 0.4856 - val_accuracy: 0.8027\n",
            "Epoch 14/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5002 - accuracy: 0.7930 - val_loss: 0.4834 - val_accuracy: 0.8060\n",
            "Epoch 15/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4870 - accuracy: 0.7999 - val_loss: 0.4766 - val_accuracy: 0.7987\n",
            "Epoch 16/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4753 - accuracy: 0.8081 - val_loss: 0.4692 - val_accuracy: 0.8180\n",
            "Epoch 17/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4643 - accuracy: 0.8073 - val_loss: 0.4586 - val_accuracy: 0.8147\n",
            "Epoch 18/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4519 - accuracy: 0.8173 - val_loss: 0.4328 - val_accuracy: 0.8270\n",
            "Epoch 19/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.4437 - accuracy: 0.8190 - val_loss: 0.4230 - val_accuracy: 0.8303\n",
            "Epoch 20/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.4283 - accuracy: 0.8237 - val_loss: 0.4315 - val_accuracy: 0.8207\n",
            "Epoch 21/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.4308 - accuracy: 0.8287 - val_loss: 0.4328 - val_accuracy: 0.8210\n",
            "Epoch 22/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4209 - accuracy: 0.8327 - val_loss: 0.4227 - val_accuracy: 0.8253\n",
            "Epoch 23/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4204 - accuracy: 0.8315 - val_loss: 0.4112 - val_accuracy: 0.8343\n",
            "Epoch 24/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4114 - accuracy: 0.8351 - val_loss: 0.4098 - val_accuracy: 0.8320\n",
            "Epoch 25/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4026 - accuracy: 0.8384 - val_loss: 0.4133 - val_accuracy: 0.8353\n",
            "Epoch 26/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3993 - accuracy: 0.8433 - val_loss: 0.4002 - val_accuracy: 0.8370\n",
            "Epoch 27/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3934 - accuracy: 0.8440 - val_loss: 0.4005 - val_accuracy: 0.8387\n",
            "Epoch 28/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3934 - accuracy: 0.8452 - val_loss: 0.3842 - val_accuracy: 0.8483\n",
            "Epoch 29/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.3825 - accuracy: 0.8471 - val_loss: 0.4032 - val_accuracy: 0.8290\n",
            "Epoch 30/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.3694 - accuracy: 0.8545 - val_loss: 0.4121 - val_accuracy: 0.8337\n",
            "Epoch 31/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3764 - accuracy: 0.8507 - val_loss: 0.4059 - val_accuracy: 0.8417\n",
            "Epoch 32/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3673 - accuracy: 0.8535 - val_loss: 0.4001 - val_accuracy: 0.8417\n",
            "Epoch 33/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3579 - accuracy: 0.8583 - val_loss: 0.3904 - val_accuracy: 0.8423\n",
            "Epoch 34/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3655 - accuracy: 0.8550 - val_loss: 0.3902 - val_accuracy: 0.8400\n",
            "Epoch 35/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3525 - accuracy: 0.8601 - val_loss: 0.4275 - val_accuracy: 0.8393\n",
            "Epoch 36/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3460 - accuracy: 0.8671 - val_loss: 0.3884 - val_accuracy: 0.8440\n",
            "Epoch 37/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3447 - accuracy: 0.8629 - val_loss: 0.3814 - val_accuracy: 0.8427\n",
            "Epoch 38/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.3453 - accuracy: 0.8636 - val_loss: 0.3937 - val_accuracy: 0.8443\n",
            "Epoch 39/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.3372 - accuracy: 0.8690 - val_loss: 0.3832 - val_accuracy: 0.8467\n",
            "Epoch 40/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3349 - accuracy: 0.8668 - val_loss: 0.3859 - val_accuracy: 0.8503\n",
            "Epoch 41/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3335 - accuracy: 0.8715 - val_loss: 0.3940 - val_accuracy: 0.8397\n",
            "Epoch 42/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3295 - accuracy: 0.8692 - val_loss: 0.3961 - val_accuracy: 0.8357\n",
            "Epoch 43/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3335 - accuracy: 0.8680 - val_loss: 0.3956 - val_accuracy: 0.8413\n",
            "Epoch 44/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3268 - accuracy: 0.8694 - val_loss: 0.3814 - val_accuracy: 0.8383\n",
            "Epoch 45/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3250 - accuracy: 0.8730 - val_loss: 0.3858 - val_accuracy: 0.8460\n",
            "Epoch 46/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3197 - accuracy: 0.8738 - val_loss: 0.3787 - val_accuracy: 0.8460\n",
            "Epoch 47/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3111 - accuracy: 0.8788 - val_loss: 0.4053 - val_accuracy: 0.8403\n",
            "Epoch 48/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.3180 - accuracy: 0.8779 - val_loss: 0.3665 - val_accuracy: 0.8530\n",
            "Epoch 49/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3119 - accuracy: 0.8793 - val_loss: 0.3777 - val_accuracy: 0.8477\n",
            "Epoch 50/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3098 - accuracy: 0.8775 - val_loss: 0.3682 - val_accuracy: 0.8540\n",
            "Epoch 51/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3038 - accuracy: 0.8800 - val_loss: 0.3831 - val_accuracy: 0.8477\n",
            "Epoch 52/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3035 - accuracy: 0.8794 - val_loss: 0.3824 - val_accuracy: 0.8527\n",
            "Epoch 53/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3106 - accuracy: 0.8780 - val_loss: 0.3717 - val_accuracy: 0.8533\n",
            "Epoch 54/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2948 - accuracy: 0.8861 - val_loss: 0.3724 - val_accuracy: 0.8483\n",
            "Epoch 55/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2971 - accuracy: 0.8830 - val_loss: 0.4076 - val_accuracy: 0.8453\n",
            "Epoch 56/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3000 - accuracy: 0.8841 - val_loss: 0.3846 - val_accuracy: 0.8433\n",
            "Epoch 57/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.3024 - accuracy: 0.8835 - val_loss: 0.3754 - val_accuracy: 0.8540\n",
            "Epoch 58/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.2936 - accuracy: 0.8835 - val_loss: 0.3658 - val_accuracy: 0.8480\n",
            "Epoch 59/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2910 - accuracy: 0.8872 - val_loss: 0.3696 - val_accuracy: 0.8443\n",
            "Epoch 60/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2933 - accuracy: 0.8876 - val_loss: 0.3630 - val_accuracy: 0.8510\n",
            "Epoch 61/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2859 - accuracy: 0.8869 - val_loss: 0.3891 - val_accuracy: 0.8487\n",
            "Epoch 62/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2899 - accuracy: 0.8879 - val_loss: 0.3685 - val_accuracy: 0.8500\n",
            "Epoch 63/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2869 - accuracy: 0.8891 - val_loss: 0.3684 - val_accuracy: 0.8500\n",
            "Epoch 64/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2826 - accuracy: 0.8882 - val_loss: 0.3764 - val_accuracy: 0.8483\n",
            "Epoch 65/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2852 - accuracy: 0.8886 - val_loss: 0.3716 - val_accuracy: 0.8530\n",
            "Epoch 66/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.2787 - accuracy: 0.8889 - val_loss: 0.3823 - val_accuracy: 0.8483\n",
            "Epoch 67/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.2839 - accuracy: 0.8915 - val_loss: 0.3760 - val_accuracy: 0.8520\n",
            "Epoch 68/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2748 - accuracy: 0.8930 - val_loss: 0.3813 - val_accuracy: 0.8483\n",
            "Epoch 69/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2806 - accuracy: 0.8940 - val_loss: 0.3989 - val_accuracy: 0.8413\n",
            "Epoch 70/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2644 - accuracy: 0.8964 - val_loss: 0.3787 - val_accuracy: 0.8483\n",
            "Epoch 71/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2699 - accuracy: 0.8981 - val_loss: 0.3860 - val_accuracy: 0.8490\n",
            "Epoch 72/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2769 - accuracy: 0.8939 - val_loss: 0.3725 - val_accuracy: 0.8557\n",
            "Epoch 73/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2658 - accuracy: 0.8963 - val_loss: 0.3828 - val_accuracy: 0.8603\n",
            "Epoch 74/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2701 - accuracy: 0.8945 - val_loss: 0.3745 - val_accuracy: 0.8520\n",
            "Epoch 75/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.2681 - accuracy: 0.8966 - val_loss: 0.3882 - val_accuracy: 0.8510\n",
            "Epoch 1/75\n",
            "218/218 [==============================] - 3s 9ms/step - loss: 1.0240 - accuracy: 0.6438 - val_loss: 0.7344 - val_accuracy: 0.6643\n",
            "Epoch 2/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.7589 - accuracy: 0.6700 - val_loss: 0.6476 - val_accuracy: 0.6907\n",
            "Epoch 3/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6740 - accuracy: 0.6991 - val_loss: 0.6040 - val_accuracy: 0.7077\n",
            "Epoch 4/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.6354 - accuracy: 0.7157 - val_loss: 0.5855 - val_accuracy: 0.7293\n",
            "Epoch 5/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.6055 - accuracy: 0.7353 - val_loss: 0.5720 - val_accuracy: 0.7577\n",
            "Epoch 6/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.5865 - accuracy: 0.7489 - val_loss: 0.5500 - val_accuracy: 0.7653\n",
            "Epoch 7/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.5662 - accuracy: 0.7575 - val_loss: 0.5250 - val_accuracy: 0.7763\n",
            "Epoch 8/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.5456 - accuracy: 0.7732 - val_loss: 0.5197 - val_accuracy: 0.7773\n",
            "Epoch 9/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5360 - accuracy: 0.7754 - val_loss: 0.4932 - val_accuracy: 0.7897\n",
            "Epoch 10/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5095 - accuracy: 0.7889 - val_loss: 0.4903 - val_accuracy: 0.7897\n",
            "Epoch 11/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.4962 - accuracy: 0.7972 - val_loss: 0.4839 - val_accuracy: 0.7953\n",
            "Epoch 12/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4881 - accuracy: 0.8011 - val_loss: 0.4645 - val_accuracy: 0.8090\n",
            "Epoch 13/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4755 - accuracy: 0.8043 - val_loss: 0.4875 - val_accuracy: 0.7970\n",
            "Epoch 14/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.4535 - accuracy: 0.8166 - val_loss: 0.4585 - val_accuracy: 0.8103\n",
            "Epoch 15/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.4551 - accuracy: 0.8154 - val_loss: 0.4369 - val_accuracy: 0.8247\n",
            "Epoch 16/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.4374 - accuracy: 0.8220 - val_loss: 0.4396 - val_accuracy: 0.8203\n",
            "Epoch 17/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.4311 - accuracy: 0.8257 - val_loss: 0.4346 - val_accuracy: 0.8257\n",
            "Epoch 18/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4210 - accuracy: 0.8344 - val_loss: 0.4105 - val_accuracy: 0.8377\n",
            "Epoch 19/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4186 - accuracy: 0.8323 - val_loss: 0.4291 - val_accuracy: 0.8267\n",
            "Epoch 20/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4041 - accuracy: 0.8394 - val_loss: 0.4259 - val_accuracy: 0.8357\n",
            "Epoch 21/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3982 - accuracy: 0.8415 - val_loss: 0.3981 - val_accuracy: 0.8453\n",
            "Epoch 22/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3930 - accuracy: 0.8432 - val_loss: 0.4178 - val_accuracy: 0.8317\n",
            "Epoch 23/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.3803 - accuracy: 0.8509 - val_loss: 0.3905 - val_accuracy: 0.8443\n",
            "Epoch 24/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.3802 - accuracy: 0.8524 - val_loss: 0.4107 - val_accuracy: 0.8353\n",
            "Epoch 25/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3693 - accuracy: 0.8535 - val_loss: 0.4055 - val_accuracy: 0.8287\n",
            "Epoch 26/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3611 - accuracy: 0.8572 - val_loss: 0.3971 - val_accuracy: 0.8320\n",
            "Epoch 27/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3614 - accuracy: 0.8556 - val_loss: 0.4091 - val_accuracy: 0.8337\n",
            "Epoch 28/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3566 - accuracy: 0.8580 - val_loss: 0.3948 - val_accuracy: 0.8463\n",
            "Epoch 29/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3521 - accuracy: 0.8649 - val_loss: 0.3829 - val_accuracy: 0.8463\n",
            "Epoch 30/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3541 - accuracy: 0.8607 - val_loss: 0.4029 - val_accuracy: 0.8367\n",
            "Epoch 31/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.3428 - accuracy: 0.8668 - val_loss: 0.3834 - val_accuracy: 0.8420\n",
            "Epoch 32/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.3442 - accuracy: 0.8634 - val_loss: 0.3889 - val_accuracy: 0.8427\n",
            "Epoch 33/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.3381 - accuracy: 0.8684 - val_loss: 0.3790 - val_accuracy: 0.8480\n",
            "Epoch 34/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.3305 - accuracy: 0.8700 - val_loss: 0.3803 - val_accuracy: 0.8540\n",
            "Epoch 35/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.3284 - accuracy: 0.8721 - val_loss: 0.3755 - val_accuracy: 0.8483\n",
            "Epoch 36/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.3338 - accuracy: 0.8700 - val_loss: 0.3765 - val_accuracy: 0.8503\n",
            "Epoch 37/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3243 - accuracy: 0.8766 - val_loss: 0.3696 - val_accuracy: 0.8530\n",
            "Epoch 38/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3183 - accuracy: 0.8761 - val_loss: 0.3626 - val_accuracy: 0.8513\n",
            "Epoch 39/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3103 - accuracy: 0.8796 - val_loss: 0.3742 - val_accuracy: 0.8500\n",
            "Epoch 40/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.3149 - accuracy: 0.8741 - val_loss: 0.3788 - val_accuracy: 0.8500\n",
            "Epoch 41/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.3050 - accuracy: 0.8830 - val_loss: 0.3874 - val_accuracy: 0.8570\n",
            "Epoch 42/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.3007 - accuracy: 0.8822 - val_loss: 0.3589 - val_accuracy: 0.8633\n",
            "Epoch 43/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.3120 - accuracy: 0.8750 - val_loss: 0.3749 - val_accuracy: 0.8473\n",
            "Epoch 44/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2994 - accuracy: 0.8843 - val_loss: 0.3694 - val_accuracy: 0.8593\n",
            "Epoch 45/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2956 - accuracy: 0.8857 - val_loss: 0.3940 - val_accuracy: 0.8440\n",
            "Epoch 46/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3034 - accuracy: 0.8863 - val_loss: 0.3600 - val_accuracy: 0.8540\n",
            "Epoch 47/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2903 - accuracy: 0.8884 - val_loss: 0.3713 - val_accuracy: 0.8560\n",
            "Epoch 48/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2901 - accuracy: 0.8870 - val_loss: 0.3648 - val_accuracy: 0.8617\n",
            "Epoch 49/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.2902 - accuracy: 0.8864 - val_loss: 0.3649 - val_accuracy: 0.8577\n",
            "Epoch 50/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.2881 - accuracy: 0.8870 - val_loss: 0.3701 - val_accuracy: 0.8587\n",
            "Epoch 51/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2761 - accuracy: 0.8931 - val_loss: 0.3996 - val_accuracy: 0.8533\n",
            "Epoch 52/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2765 - accuracy: 0.8940 - val_loss: 0.3738 - val_accuracy: 0.8573\n",
            "Epoch 53/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2772 - accuracy: 0.8949 - val_loss: 0.3707 - val_accuracy: 0.8567\n",
            "Epoch 54/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2865 - accuracy: 0.8865 - val_loss: 0.3701 - val_accuracy: 0.8507\n",
            "Epoch 55/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2813 - accuracy: 0.8932 - val_loss: 0.3615 - val_accuracy: 0.8643\n",
            "Epoch 56/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2609 - accuracy: 0.9015 - val_loss: 0.3682 - val_accuracy: 0.8607\n",
            "Epoch 57/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.2741 - accuracy: 0.8950 - val_loss: 0.3632 - val_accuracy: 0.8557\n",
            "Epoch 58/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.2714 - accuracy: 0.8977 - val_loss: 0.3596 - val_accuracy: 0.8580\n",
            "Epoch 59/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.2712 - accuracy: 0.8966 - val_loss: 0.3672 - val_accuracy: 0.8617\n",
            "Epoch 60/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2652 - accuracy: 0.8981 - val_loss: 0.3637 - val_accuracy: 0.8680\n",
            "Epoch 61/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2650 - accuracy: 0.8984 - val_loss: 0.3723 - val_accuracy: 0.8603\n",
            "Epoch 62/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2647 - accuracy: 0.8998 - val_loss: 0.3576 - val_accuracy: 0.8670\n",
            "Epoch 63/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.2561 - accuracy: 0.9031 - val_loss: 0.3614 - val_accuracy: 0.8533\n",
            "Epoch 64/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.2566 - accuracy: 0.9009 - val_loss: 0.3870 - val_accuracy: 0.8530\n",
            "Epoch 65/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.2581 - accuracy: 0.9006 - val_loss: 0.3745 - val_accuracy: 0.8533\n",
            "Epoch 66/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.2607 - accuracy: 0.9009 - val_loss: 0.3708 - val_accuracy: 0.8623\n",
            "Epoch 67/75\n",
            "218/218 [==============================] - 2s 8ms/step - loss: 0.2553 - accuracy: 0.9015 - val_loss: 0.3644 - val_accuracy: 0.8650\n",
            "Epoch 68/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.2532 - accuracy: 0.9014 - val_loss: 0.3552 - val_accuracy: 0.8543\n",
            "Epoch 69/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.2391 - accuracy: 0.9055 - val_loss: 0.3668 - val_accuracy: 0.8560\n",
            "Epoch 70/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.2478 - accuracy: 0.9065 - val_loss: 0.3497 - val_accuracy: 0.8680\n",
            "Epoch 71/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.2472 - accuracy: 0.9053 - val_loss: 0.3618 - val_accuracy: 0.8637\n",
            "Epoch 72/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.2474 - accuracy: 0.9047 - val_loss: 0.3801 - val_accuracy: 0.8577\n",
            "Epoch 73/75\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.2390 - accuracy: 0.9068 - val_loss: 0.3656 - val_accuracy: 0.8607\n",
            "Epoch 74/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.2389 - accuracy: 0.9096 - val_loss: 0.3747 - val_accuracy: 0.8567\n",
            "Epoch 75/75\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.2418 - accuracy: 0.9055 - val_loss: 0.3545 - val_accuracy: 0.8697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basic_cnn_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL7bHz1c4gA4",
        "outputId": "13b29ee1-e0ca-4247-8e3a-4b3c2f815aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{50: 0.7598758339881897,\n",
              " 100: 0.8259029388427734,\n",
              " 150: 0.8481941223144531,\n",
              " 200: 0.8459367752075195,\n",
              " 250: 0.8470654487609863}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shallow CNN"
      ],
      "metadata": {
        "id": "BFcVPGps6YHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, AveragePooling2D, BatchNormalization, Dropout, Flatten, Dense, Activation, Lambda\n",
        "from keras import backend as K\n",
        "\n",
        "def square_activation(x):\n",
        "    return K.square(x)\n",
        "\n",
        "def log_activation(x):\n",
        "    return K.log(K.abs(x) + 1e-6)\n",
        "\n",
        "def shallow_cnn_time(n):\n",
        "  shallow_cnn_model = Sequential()\n",
        "\n",
        "  # Conv. block 1\n",
        "  shallow_cnn_model.add(Conv2D(filters=40, kernel_size=(25,1), padding='same', activation='elu', input_shape=(n,1,22)))\n",
        "  shallow_cnn_model.add(BatchNormalization())\n",
        "  shallow_cnn_model.add(Dropout(0.5))\n",
        "\n",
        "  # Conv. block 2\n",
        "  shallow_cnn_model.add(Conv2D(filters=40, kernel_size=(25,1), padding='same', activation='elu'))\n",
        "  shallow_cnn_model.add(BatchNormalization())\n",
        "  shallow_cnn_model.add(Activation(square_activation))\n",
        "  shallow_cnn_model.add(AveragePooling2D(pool_size=(75,1), strides=(15,1), padding='same'))\n",
        "  shallow_cnn_model.add(Lambda(log_activation))\n",
        "\n",
        "  # FC layers\n",
        "  shallow_cnn_model.add(Flatten())\n",
        "  shallow_cnn_model.add(Dense(100, activation='elu'))\n",
        "\n",
        "  # Output layer with Softmax activation \n",
        "  shallow_cnn_model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "  # Compiling the model\n",
        "  shallow_cnn_model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=cnn_optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "  \n",
        "  return shallow_cnn_model"
      ],
      "metadata": {
        "id": "myiUGLt43lRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num = [50, 100, 150, 200, 250]\n",
        "\n",
        "shallow_cnn_dict = {}\n",
        "\n",
        "for n in num:\n",
        "  x_train = x_train_vae[:, :n, :, :]\n",
        "  x_train = np.reshape(x_train, (13920, n, 1, 22))\n",
        "\n",
        "  x_valid = x_valid_vae[:, :n, :, :]\n",
        "  x_valid = np.reshape(x_valid, (3000, n, 1, 22))\n",
        "\n",
        "  x_test = x_test_vae[:, :n, :, :]\n",
        "  x_test = np.reshape(x_test, (3544, n, 1, 22)) \n",
        "  \n",
        "  shallow_cnn_model = shallow_cnn_time(n)\n",
        "\n",
        "  shallow_cnn_model_results = shallow_cnn_model.fit(x_train,\n",
        "              y_train_vae,\n",
        "              batch_size=64,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_valid, y_valid_vae), verbose=True)\n",
        "  \n",
        "  shallow_cnn_score = shallow_cnn_model.evaluate(x_test, y_test_vae, verbose=0)\n",
        "\n",
        "  shallow_cnn_dict[n] = shallow_cnn_score[1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayhGCmxU4Gp_",
        "outputId": "478fc2d2-c422-4005-a3a2-56d0a04051cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "218/218 [==============================] - 2s 6ms/step - loss: 0.7349 - accuracy: 0.6578 - val_loss: 0.6852 - val_accuracy: 0.6693\n",
            "Epoch 2/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.6433 - accuracy: 0.7001 - val_loss: 0.6867 - val_accuracy: 0.6777\n",
            "Epoch 3/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.6081 - accuracy: 0.7261 - val_loss: 0.6555 - val_accuracy: 0.7193\n",
            "Epoch 4/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.5669 - accuracy: 0.7524 - val_loss: 0.6873 - val_accuracy: 0.6933\n",
            "Epoch 5/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.5378 - accuracy: 0.7649 - val_loss: 0.6660 - val_accuracy: 0.7107\n",
            "Epoch 6/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.4980 - accuracy: 0.7871 - val_loss: 0.6524 - val_accuracy: 0.7270\n",
            "Epoch 7/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.4616 - accuracy: 0.8089 - val_loss: 0.6650 - val_accuracy: 0.7280\n",
            "Epoch 8/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.4453 - accuracy: 0.8156 - val_loss: 0.6783 - val_accuracy: 0.7387\n",
            "Epoch 9/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.4077 - accuracy: 0.8357 - val_loss: 0.6703 - val_accuracy: 0.7413\n",
            "Epoch 10/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.3764 - accuracy: 0.8468 - val_loss: 0.7396 - val_accuracy: 0.7333\n",
            "Epoch 11/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.3542 - accuracy: 0.8570 - val_loss: 0.7873 - val_accuracy: 0.7353\n",
            "Epoch 12/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.3349 - accuracy: 0.8671 - val_loss: 0.7627 - val_accuracy: 0.7320\n",
            "Epoch 13/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.3127 - accuracy: 0.8741 - val_loss: 0.7894 - val_accuracy: 0.7343\n",
            "Epoch 14/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.2932 - accuracy: 0.8842 - val_loss: 0.8213 - val_accuracy: 0.7337\n",
            "Epoch 15/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.2861 - accuracy: 0.8865 - val_loss: 0.9030 - val_accuracy: 0.7213\n",
            "Epoch 16/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.2604 - accuracy: 0.8973 - val_loss: 0.8499 - val_accuracy: 0.7360\n",
            "Epoch 17/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.2454 - accuracy: 0.9028 - val_loss: 0.8699 - val_accuracy: 0.7427\n",
            "Epoch 18/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.2397 - accuracy: 0.9057 - val_loss: 0.9177 - val_accuracy: 0.7327\n",
            "Epoch 19/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.2191 - accuracy: 0.9174 - val_loss: 0.9472 - val_accuracy: 0.7383\n",
            "Epoch 20/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.2120 - accuracy: 0.9170 - val_loss: 0.9693 - val_accuracy: 0.7310\n",
            "Epoch 21/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.2055 - accuracy: 0.9207 - val_loss: 0.9864 - val_accuracy: 0.7307\n",
            "Epoch 22/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1947 - accuracy: 0.9272 - val_loss: 0.9800 - val_accuracy: 0.7317\n",
            "Epoch 23/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1902 - accuracy: 0.9277 - val_loss: 1.0322 - val_accuracy: 0.7297\n",
            "Epoch 24/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1854 - accuracy: 0.9287 - val_loss: 1.0285 - val_accuracy: 0.7370\n",
            "Epoch 25/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.1775 - accuracy: 0.9346 - val_loss: 1.0879 - val_accuracy: 0.7367\n",
            "Epoch 26/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.1567 - accuracy: 0.9405 - val_loss: 1.1026 - val_accuracy: 0.7320\n",
            "Epoch 27/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.1572 - accuracy: 0.9414 - val_loss: 1.1143 - val_accuracy: 0.7377\n",
            "Epoch 28/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1724 - accuracy: 0.9366 - val_loss: 1.0739 - val_accuracy: 0.7370\n",
            "Epoch 29/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1476 - accuracy: 0.9457 - val_loss: 1.0859 - val_accuracy: 0.7303\n",
            "Epoch 30/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1504 - accuracy: 0.9443 - val_loss: 1.0932 - val_accuracy: 0.7337\n",
            "Epoch 31/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1426 - accuracy: 0.9472 - val_loss: 1.1531 - val_accuracy: 0.7250\n",
            "Epoch 32/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1297 - accuracy: 0.9514 - val_loss: 1.1381 - val_accuracy: 0.7437\n",
            "Epoch 33/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1408 - accuracy: 0.9472 - val_loss: 1.1998 - val_accuracy: 0.7267\n",
            "Epoch 34/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1375 - accuracy: 0.9504 - val_loss: 1.1176 - val_accuracy: 0.7367\n",
            "Epoch 35/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1324 - accuracy: 0.9516 - val_loss: 1.1209 - val_accuracy: 0.7427\n",
            "Epoch 36/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1213 - accuracy: 0.9553 - val_loss: 1.1970 - val_accuracy: 0.7283\n",
            "Epoch 37/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1309 - accuracy: 0.9506 - val_loss: 1.1893 - val_accuracy: 0.7330\n",
            "Epoch 38/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.1137 - accuracy: 0.9579 - val_loss: 1.2191 - val_accuracy: 0.7417\n",
            "Epoch 39/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.1209 - accuracy: 0.9535 - val_loss: 1.1789 - val_accuracy: 0.7403\n",
            "Epoch 40/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.1159 - accuracy: 0.9565 - val_loss: 1.2282 - val_accuracy: 0.7367\n",
            "Epoch 41/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1142 - accuracy: 0.9585 - val_loss: 1.2287 - val_accuracy: 0.7350\n",
            "Epoch 42/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1193 - accuracy: 0.9561 - val_loss: 1.1929 - val_accuracy: 0.7393\n",
            "Epoch 43/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1039 - accuracy: 0.9622 - val_loss: 1.2161 - val_accuracy: 0.7323\n",
            "Epoch 44/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1012 - accuracy: 0.9636 - val_loss: 1.2831 - val_accuracy: 0.7303\n",
            "Epoch 45/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1069 - accuracy: 0.9602 - val_loss: 1.2713 - val_accuracy: 0.7400\n",
            "Epoch 46/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1045 - accuracy: 0.9628 - val_loss: 1.3027 - val_accuracy: 0.7370\n",
            "Epoch 47/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1079 - accuracy: 0.9588 - val_loss: 1.2354 - val_accuracy: 0.7410\n",
            "Epoch 48/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0955 - accuracy: 0.9642 - val_loss: 1.3204 - val_accuracy: 0.7397\n",
            "Epoch 49/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1106 - accuracy: 0.9588 - val_loss: 1.2862 - val_accuracy: 0.7370\n",
            "Epoch 50/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0916 - accuracy: 0.9672 - val_loss: 1.2795 - val_accuracy: 0.7440\n",
            "Epoch 1/50\n",
            "218/218 [==============================] - 2s 6ms/step - loss: 0.7502 - accuracy: 0.6644 - val_loss: 0.7159 - val_accuracy: 0.6753\n",
            "Epoch 2/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.6186 - accuracy: 0.7204 - val_loss: 0.6213 - val_accuracy: 0.7250\n",
            "Epoch 3/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.5570 - accuracy: 0.7599 - val_loss: 0.6442 - val_accuracy: 0.7297\n",
            "Epoch 4/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.5095 - accuracy: 0.7847 - val_loss: 0.6031 - val_accuracy: 0.7543\n",
            "Epoch 5/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.4577 - accuracy: 0.8093 - val_loss: 0.6171 - val_accuracy: 0.7573\n",
            "Epoch 6/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.4176 - accuracy: 0.8290 - val_loss: 0.5900 - val_accuracy: 0.7680\n",
            "Epoch 7/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.3706 - accuracy: 0.8509 - val_loss: 0.6135 - val_accuracy: 0.7727\n",
            "Epoch 8/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.3292 - accuracy: 0.8685 - val_loss: 0.6545 - val_accuracy: 0.7627\n",
            "Epoch 9/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.2871 - accuracy: 0.8907 - val_loss: 0.6876 - val_accuracy: 0.7600\n",
            "Epoch 10/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.2552 - accuracy: 0.9018 - val_loss: 0.6797 - val_accuracy: 0.7690\n",
            "Epoch 11/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.2252 - accuracy: 0.9167 - val_loss: 0.6815 - val_accuracy: 0.7610\n",
            "Epoch 12/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1965 - accuracy: 0.9278 - val_loss: 0.7679 - val_accuracy: 0.7623\n",
            "Epoch 13/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1752 - accuracy: 0.9348 - val_loss: 0.8183 - val_accuracy: 0.7510\n",
            "Epoch 14/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1555 - accuracy: 0.9420 - val_loss: 0.8245 - val_accuracy: 0.7600\n",
            "Epoch 15/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.1389 - accuracy: 0.9496 - val_loss: 0.8189 - val_accuracy: 0.7760\n",
            "Epoch 16/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.1307 - accuracy: 0.9536 - val_loss: 0.8746 - val_accuracy: 0.7683\n",
            "Epoch 17/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.1296 - accuracy: 0.9535 - val_loss: 0.8870 - val_accuracy: 0.7640\n",
            "Epoch 18/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1085 - accuracy: 0.9616 - val_loss: 0.9709 - val_accuracy: 0.7650\n",
            "Epoch 19/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1000 - accuracy: 0.9652 - val_loss: 0.9858 - val_accuracy: 0.7563\n",
            "Epoch 20/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0998 - accuracy: 0.9644 - val_loss: 0.9340 - val_accuracy: 0.7703\n",
            "Epoch 21/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0868 - accuracy: 0.9700 - val_loss: 1.0051 - val_accuracy: 0.7613\n",
            "Epoch 22/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0810 - accuracy: 0.9716 - val_loss: 0.9889 - val_accuracy: 0.7713\n",
            "Epoch 23/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0792 - accuracy: 0.9724 - val_loss: 1.0250 - val_accuracy: 0.7693\n",
            "Epoch 24/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0686 - accuracy: 0.9747 - val_loss: 1.0222 - val_accuracy: 0.7720\n",
            "Epoch 25/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0671 - accuracy: 0.9779 - val_loss: 1.0730 - val_accuracy: 0.7707\n",
            "Epoch 26/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0680 - accuracy: 0.9769 - val_loss: 1.0834 - val_accuracy: 0.7687\n",
            "Epoch 27/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0652 - accuracy: 0.9775 - val_loss: 1.1753 - val_accuracy: 0.7590\n",
            "Epoch 28/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0677 - accuracy: 0.9750 - val_loss: 1.1370 - val_accuracy: 0.7710\n",
            "Epoch 29/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0536 - accuracy: 0.9819 - val_loss: 1.1408 - val_accuracy: 0.7753\n",
            "Epoch 30/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0535 - accuracy: 0.9805 - val_loss: 1.1027 - val_accuracy: 0.7733\n",
            "Epoch 31/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0542 - accuracy: 0.9807 - val_loss: 1.1209 - val_accuracy: 0.7730\n",
            "Epoch 32/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0525 - accuracy: 0.9813 - val_loss: 1.1406 - val_accuracy: 0.7763\n",
            "Epoch 33/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0498 - accuracy: 0.9825 - val_loss: 1.2042 - val_accuracy: 0.7727\n",
            "Epoch 34/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0502 - accuracy: 0.9840 - val_loss: 1.1799 - val_accuracy: 0.7717\n",
            "Epoch 35/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0599 - accuracy: 0.9781 - val_loss: 1.0909 - val_accuracy: 0.7700\n",
            "Epoch 36/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0488 - accuracy: 0.9835 - val_loss: 1.2436 - val_accuracy: 0.7743\n",
            "Epoch 37/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0466 - accuracy: 0.9843 - val_loss: 1.2604 - val_accuracy: 0.7610\n",
            "Epoch 38/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0393 - accuracy: 0.9866 - val_loss: 1.2991 - val_accuracy: 0.7583\n",
            "Epoch 39/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0478 - accuracy: 0.9833 - val_loss: 1.2107 - val_accuracy: 0.7647\n",
            "Epoch 40/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0308 - accuracy: 0.9908 - val_loss: 1.2363 - val_accuracy: 0.7777\n",
            "Epoch 41/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0385 - accuracy: 0.9858 - val_loss: 1.2709 - val_accuracy: 0.7640\n",
            "Epoch 42/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0429 - accuracy: 0.9845 - val_loss: 1.1961 - val_accuracy: 0.7723\n",
            "Epoch 43/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0393 - accuracy: 0.9857 - val_loss: 1.2494 - val_accuracy: 0.7710\n",
            "Epoch 44/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0293 - accuracy: 0.9904 - val_loss: 1.2641 - val_accuracy: 0.7733\n",
            "Epoch 45/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0354 - accuracy: 0.9876 - val_loss: 1.2500 - val_accuracy: 0.7693\n",
            "Epoch 46/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0393 - accuracy: 0.9871 - val_loss: 1.2632 - val_accuracy: 0.7747\n",
            "Epoch 47/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0404 - accuracy: 0.9859 - val_loss: 1.2610 - val_accuracy: 0.7677\n",
            "Epoch 48/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0350 - accuracy: 0.9881 - val_loss: 1.3332 - val_accuracy: 0.7737\n",
            "Epoch 49/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0387 - accuracy: 0.9868 - val_loss: 1.2885 - val_accuracy: 0.7793\n",
            "Epoch 50/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0356 - accuracy: 0.9876 - val_loss: 1.2812 - val_accuracy: 0.7750\n",
            "Epoch 1/50\n",
            "218/218 [==============================] - 3s 8ms/step - loss: 0.7908 - accuracy: 0.6608 - val_loss: 0.6708 - val_accuracy: 0.6887\n",
            "Epoch 2/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.6036 - accuracy: 0.7307 - val_loss: 0.6194 - val_accuracy: 0.7223\n",
            "Epoch 3/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.5189 - accuracy: 0.7803 - val_loss: 0.5779 - val_accuracy: 0.7493\n",
            "Epoch 4/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.4503 - accuracy: 0.8178 - val_loss: 0.5862 - val_accuracy: 0.7633\n",
            "Epoch 5/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.3866 - accuracy: 0.8450 - val_loss: 0.5285 - val_accuracy: 0.7893\n",
            "Epoch 6/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.3411 - accuracy: 0.8672 - val_loss: 0.5168 - val_accuracy: 0.7993\n",
            "Epoch 7/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.2874 - accuracy: 0.8886 - val_loss: 0.5517 - val_accuracy: 0.7940\n",
            "Epoch 8/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.2449 - accuracy: 0.9054 - val_loss: 0.5631 - val_accuracy: 0.7970\n",
            "Epoch 9/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.2134 - accuracy: 0.9208 - val_loss: 0.5711 - val_accuracy: 0.8043\n",
            "Epoch 10/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.1802 - accuracy: 0.9337 - val_loss: 0.5764 - val_accuracy: 0.8030\n",
            "Epoch 11/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.1514 - accuracy: 0.9446 - val_loss: 0.5853 - val_accuracy: 0.8103\n",
            "Epoch 12/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.1330 - accuracy: 0.9513 - val_loss: 0.6780 - val_accuracy: 0.8013\n",
            "Epoch 13/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.1166 - accuracy: 0.9586 - val_loss: 0.7093 - val_accuracy: 0.7963\n",
            "Epoch 14/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0982 - accuracy: 0.9642 - val_loss: 0.6913 - val_accuracy: 0.8063\n",
            "Epoch 15/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0871 - accuracy: 0.9685 - val_loss: 0.7175 - val_accuracy: 0.8080\n",
            "Epoch 16/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0779 - accuracy: 0.9731 - val_loss: 0.7345 - val_accuracy: 0.8070\n",
            "Epoch 17/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0708 - accuracy: 0.9764 - val_loss: 0.7659 - val_accuracy: 0.8157\n",
            "Epoch 18/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0760 - accuracy: 0.9720 - val_loss: 0.8014 - val_accuracy: 0.8110\n",
            "Epoch 19/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0565 - accuracy: 0.9815 - val_loss: 0.7806 - val_accuracy: 0.8130\n",
            "Epoch 20/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0582 - accuracy: 0.9807 - val_loss: 0.7810 - val_accuracy: 0.8163\n",
            "Epoch 21/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0606 - accuracy: 0.9793 - val_loss: 0.8385 - val_accuracy: 0.8067\n",
            "Epoch 22/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0496 - accuracy: 0.9828 - val_loss: 0.8660 - val_accuracy: 0.8147\n",
            "Epoch 23/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0484 - accuracy: 0.9835 - val_loss: 0.8923 - val_accuracy: 0.8130\n",
            "Epoch 24/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0418 - accuracy: 0.9862 - val_loss: 0.8580 - val_accuracy: 0.8113\n",
            "Epoch 25/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0364 - accuracy: 0.9880 - val_loss: 0.8461 - val_accuracy: 0.8177\n",
            "Epoch 26/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0392 - accuracy: 0.9874 - val_loss: 0.9405 - val_accuracy: 0.8173\n",
            "Epoch 27/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0501 - accuracy: 0.9826 - val_loss: 0.8865 - val_accuracy: 0.8153\n",
            "Epoch 28/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0371 - accuracy: 0.9870 - val_loss: 0.8855 - val_accuracy: 0.8170\n",
            "Epoch 29/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0414 - accuracy: 0.9863 - val_loss: 0.9139 - val_accuracy: 0.8110\n",
            "Epoch 30/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0343 - accuracy: 0.9891 - val_loss: 0.9064 - val_accuracy: 0.8193\n",
            "Epoch 31/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0252 - accuracy: 0.9924 - val_loss: 1.0715 - val_accuracy: 0.8077\n",
            "Epoch 32/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0314 - accuracy: 0.9889 - val_loss: 1.0704 - val_accuracy: 0.8070\n",
            "Epoch 33/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0315 - accuracy: 0.9892 - val_loss: 1.0101 - val_accuracy: 0.8107\n",
            "Epoch 34/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0283 - accuracy: 0.9916 - val_loss: 1.0250 - val_accuracy: 0.8120\n",
            "Epoch 35/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0355 - accuracy: 0.9879 - val_loss: 1.0631 - val_accuracy: 0.8113\n",
            "Epoch 36/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0296 - accuracy: 0.9897 - val_loss: 1.0560 - val_accuracy: 0.8123\n",
            "Epoch 37/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0299 - accuracy: 0.9898 - val_loss: 1.0771 - val_accuracy: 0.8090\n",
            "Epoch 38/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0258 - accuracy: 0.9915 - val_loss: 1.1064 - val_accuracy: 0.8103\n",
            "Epoch 39/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0314 - accuracy: 0.9893 - val_loss: 1.0286 - val_accuracy: 0.8080\n",
            "Epoch 40/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0252 - accuracy: 0.9912 - val_loss: 1.0359 - val_accuracy: 0.8197\n",
            "Epoch 41/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0165 - accuracy: 0.9950 - val_loss: 1.1127 - val_accuracy: 0.8150\n",
            "Epoch 42/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0247 - accuracy: 0.9920 - val_loss: 1.0332 - val_accuracy: 0.8170\n",
            "Epoch 43/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0295 - accuracy: 0.9891 - val_loss: 1.1348 - val_accuracy: 0.8083\n",
            "Epoch 44/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0262 - accuracy: 0.9917 - val_loss: 1.1738 - val_accuracy: 0.8060\n",
            "Epoch 45/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0242 - accuracy: 0.9922 - val_loss: 1.1542 - val_accuracy: 0.8117\n",
            "Epoch 46/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0245 - accuracy: 0.9915 - val_loss: 1.0836 - val_accuracy: 0.8243\n",
            "Epoch 47/50\n",
            "218/218 [==============================] - 1s 4ms/step - loss: 0.0143 - accuracy: 0.9948 - val_loss: 1.0145 - val_accuracy: 0.8277\n",
            "Epoch 48/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0237 - accuracy: 0.9927 - val_loss: 1.0868 - val_accuracy: 0.8167\n",
            "Epoch 49/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0264 - accuracy: 0.9905 - val_loss: 1.0917 - val_accuracy: 0.8153\n",
            "Epoch 50/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0227 - accuracy: 0.9919 - val_loss: 1.1432 - val_accuracy: 0.8230\n",
            "Epoch 1/50\n",
            "218/218 [==============================] - 2s 6ms/step - loss: 0.7556 - accuracy: 0.6860 - val_loss: 0.6137 - val_accuracy: 0.7333\n",
            "Epoch 2/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.5428 - accuracy: 0.7643 - val_loss: 0.6106 - val_accuracy: 0.7480\n",
            "Epoch 3/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.4415 - accuracy: 0.8162 - val_loss: 0.5741 - val_accuracy: 0.7847\n",
            "Epoch 4/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.3684 - accuracy: 0.8487 - val_loss: 0.5772 - val_accuracy: 0.7830\n",
            "Epoch 5/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.2833 - accuracy: 0.8915 - val_loss: 0.5887 - val_accuracy: 0.7977\n",
            "Epoch 6/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.2213 - accuracy: 0.9166 - val_loss: 0.6414 - val_accuracy: 0.7817\n",
            "Epoch 7/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.1632 - accuracy: 0.9415 - val_loss: 0.6743 - val_accuracy: 0.7867\n",
            "Epoch 8/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.1251 - accuracy: 0.9569 - val_loss: 0.7407 - val_accuracy: 0.7890\n",
            "Epoch 9/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0916 - accuracy: 0.9684 - val_loss: 0.7398 - val_accuracy: 0.8043\n",
            "Epoch 10/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0727 - accuracy: 0.9773 - val_loss: 0.8385 - val_accuracy: 0.7930\n",
            "Epoch 11/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0614 - accuracy: 0.9791 - val_loss: 0.8607 - val_accuracy: 0.7873\n",
            "Epoch 12/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0516 - accuracy: 0.9818 - val_loss: 0.9662 - val_accuracy: 0.7933\n",
            "Epoch 13/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0483 - accuracy: 0.9846 - val_loss: 0.8971 - val_accuracy: 0.7893\n",
            "Epoch 14/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0410 - accuracy: 0.9861 - val_loss: 0.9400 - val_accuracy: 0.7793\n",
            "Epoch 15/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0353 - accuracy: 0.9886 - val_loss: 0.9861 - val_accuracy: 0.7933\n",
            "Epoch 16/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0338 - accuracy: 0.9892 - val_loss: 1.0266 - val_accuracy: 0.7907\n",
            "Epoch 17/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0267 - accuracy: 0.9910 - val_loss: 1.0387 - val_accuracy: 0.7923\n",
            "Epoch 18/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0315 - accuracy: 0.9889 - val_loss: 1.0938 - val_accuracy: 0.7847\n",
            "Epoch 19/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0261 - accuracy: 0.9914 - val_loss: 1.2473 - val_accuracy: 0.7740\n",
            "Epoch 20/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0254 - accuracy: 0.9912 - val_loss: 1.0670 - val_accuracy: 0.7890\n",
            "Epoch 21/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0294 - accuracy: 0.9898 - val_loss: 1.1211 - val_accuracy: 0.7907\n",
            "Epoch 22/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0265 - accuracy: 0.9918 - val_loss: 1.1109 - val_accuracy: 0.7833\n",
            "Epoch 23/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0214 - accuracy: 0.9934 - val_loss: 1.1899 - val_accuracy: 0.7943\n",
            "Epoch 24/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 1.1750 - val_accuracy: 0.7863\n",
            "Epoch 25/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0194 - accuracy: 0.9940 - val_loss: 1.1715 - val_accuracy: 0.7913\n",
            "Epoch 26/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0125 - accuracy: 0.9963 - val_loss: 1.1715 - val_accuracy: 0.7807\n",
            "Epoch 27/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0178 - accuracy: 0.9946 - val_loss: 1.1769 - val_accuracy: 0.7920\n",
            "Epoch 28/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0209 - accuracy: 0.9924 - val_loss: 1.2225 - val_accuracy: 0.7833\n",
            "Epoch 29/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0229 - accuracy: 0.9921 - val_loss: 1.3058 - val_accuracy: 0.7940\n",
            "Epoch 30/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0212 - accuracy: 0.9932 - val_loss: 1.3510 - val_accuracy: 0.7927\n",
            "Epoch 31/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0154 - accuracy: 0.9948 - val_loss: 1.2961 - val_accuracy: 0.7880\n",
            "Epoch 32/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0143 - accuracy: 0.9945 - val_loss: 1.3819 - val_accuracy: 0.7860\n",
            "Epoch 33/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0171 - accuracy: 0.9935 - val_loss: 1.2639 - val_accuracy: 0.7933\n",
            "Epoch 34/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0137 - accuracy: 0.9955 - val_loss: 1.2590 - val_accuracy: 0.7920\n",
            "Epoch 35/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0130 - accuracy: 0.9960 - val_loss: 1.2924 - val_accuracy: 0.7983\n",
            "Epoch 36/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0182 - accuracy: 0.9947 - val_loss: 1.3037 - val_accuracy: 0.7863\n",
            "Epoch 37/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0159 - accuracy: 0.9945 - val_loss: 1.3189 - val_accuracy: 0.7997\n",
            "Epoch 38/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0186 - accuracy: 0.9937 - val_loss: 1.3360 - val_accuracy: 0.7957\n",
            "Epoch 39/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0132 - accuracy: 0.9957 - val_loss: 1.2342 - val_accuracy: 0.8020\n",
            "Epoch 40/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0115 - accuracy: 0.9966 - val_loss: 1.4534 - val_accuracy: 0.7847\n",
            "Epoch 41/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0179 - accuracy: 0.9937 - val_loss: 1.4596 - val_accuracy: 0.7913\n",
            "Epoch 42/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0188 - accuracy: 0.9932 - val_loss: 1.3380 - val_accuracy: 0.7910\n",
            "Epoch 43/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0076 - accuracy: 0.9980 - val_loss: 1.3291 - val_accuracy: 0.7977\n",
            "Epoch 44/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 1.3025 - val_accuracy: 0.8003\n",
            "Epoch 45/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0130 - accuracy: 0.9950 - val_loss: 1.4311 - val_accuracy: 0.7967\n",
            "Epoch 46/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0096 - accuracy: 0.9966 - val_loss: 1.4092 - val_accuracy: 0.8067\n",
            "Epoch 47/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0076 - accuracy: 0.9977 - val_loss: 1.4239 - val_accuracy: 0.7950\n",
            "Epoch 48/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0159 - accuracy: 0.9949 - val_loss: 1.2885 - val_accuracy: 0.8017\n",
            "Epoch 49/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0129 - accuracy: 0.9960 - val_loss: 1.3831 - val_accuracy: 0.7940\n",
            "Epoch 50/50\n",
            "218/218 [==============================] - 1s 5ms/step - loss: 0.0154 - accuracy: 0.9953 - val_loss: 1.4310 - val_accuracy: 0.8003\n",
            "Epoch 1/50\n",
            "218/218 [==============================] - 2s 8ms/step - loss: 0.7843 - accuracy: 0.6680 - val_loss: 0.6902 - val_accuracy: 0.6563\n",
            "Epoch 2/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5820 - accuracy: 0.7398 - val_loss: 0.5914 - val_accuracy: 0.7407\n",
            "Epoch 3/50\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.5046 - accuracy: 0.7846 - val_loss: 0.5792 - val_accuracy: 0.7557\n",
            "Epoch 4/50\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.4372 - accuracy: 0.8221 - val_loss: 0.5656 - val_accuracy: 0.7717\n",
            "Epoch 5/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3788 - accuracy: 0.8489 - val_loss: 0.5545 - val_accuracy: 0.7877\n",
            "Epoch 6/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3154 - accuracy: 0.8774 - val_loss: 0.5924 - val_accuracy: 0.7803\n",
            "Epoch 7/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2568 - accuracy: 0.9001 - val_loss: 0.6272 - val_accuracy: 0.7843\n",
            "Epoch 8/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2141 - accuracy: 0.9177 - val_loss: 0.7076 - val_accuracy: 0.7873\n",
            "Epoch 9/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.1581 - accuracy: 0.9411 - val_loss: 0.6867 - val_accuracy: 0.7823\n",
            "Epoch 10/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.1225 - accuracy: 0.9555 - val_loss: 0.7598 - val_accuracy: 0.7947\n",
            "Epoch 11/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0983 - accuracy: 0.9649 - val_loss: 0.8047 - val_accuracy: 0.7933\n",
            "Epoch 12/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0850 - accuracy: 0.9700 - val_loss: 0.8233 - val_accuracy: 0.7830\n",
            "Epoch 13/50\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.0712 - accuracy: 0.9760 - val_loss: 0.8619 - val_accuracy: 0.8073\n",
            "Epoch 14/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0589 - accuracy: 0.9809 - val_loss: 0.8547 - val_accuracy: 0.8010\n",
            "Epoch 15/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0696 - accuracy: 0.9766 - val_loss: 0.9354 - val_accuracy: 0.7957\n",
            "Epoch 16/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0480 - accuracy: 0.9832 - val_loss: 0.9016 - val_accuracy: 0.8003\n",
            "Epoch 17/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0390 - accuracy: 0.9881 - val_loss: 1.0458 - val_accuracy: 0.7880\n",
            "Epoch 18/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0347 - accuracy: 0.9883 - val_loss: 1.1015 - val_accuracy: 0.7950\n",
            "Epoch 19/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0456 - accuracy: 0.9836 - val_loss: 1.0029 - val_accuracy: 0.8087\n",
            "Epoch 20/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0294 - accuracy: 0.9902 - val_loss: 1.0797 - val_accuracy: 0.7970\n",
            "Epoch 21/50\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.0320 - accuracy: 0.9901 - val_loss: 0.9828 - val_accuracy: 0.8023\n",
            "Epoch 22/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0309 - accuracy: 0.9897 - val_loss: 1.1217 - val_accuracy: 0.8007\n",
            "Epoch 23/50\n",
            "218/218 [==============================] - 2s 8ms/step - loss: 0.0281 - accuracy: 0.9905 - val_loss: 1.2303 - val_accuracy: 0.7950\n",
            "Epoch 24/50\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.0316 - accuracy: 0.9897 - val_loss: 1.0803 - val_accuracy: 0.7977\n",
            "Epoch 25/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0220 - accuracy: 0.9932 - val_loss: 1.1225 - val_accuracy: 0.8093\n",
            "Epoch 26/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0196 - accuracy: 0.9939 - val_loss: 1.1657 - val_accuracy: 0.8090\n",
            "Epoch 27/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0264 - accuracy: 0.9903 - val_loss: 1.0825 - val_accuracy: 0.8043\n",
            "Epoch 28/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0252 - accuracy: 0.9918 - val_loss: 1.2499 - val_accuracy: 0.7917\n",
            "Epoch 29/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0277 - accuracy: 0.9909 - val_loss: 1.2362 - val_accuracy: 0.7947\n",
            "Epoch 30/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0249 - accuracy: 0.9918 - val_loss: 1.1735 - val_accuracy: 0.7987\n",
            "Epoch 31/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0419 - accuracy: 0.9871 - val_loss: 1.2708 - val_accuracy: 0.7923\n",
            "Epoch 32/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0222 - accuracy: 0.9925 - val_loss: 1.2056 - val_accuracy: 0.8047\n",
            "Epoch 33/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0147 - accuracy: 0.9957 - val_loss: 1.1996 - val_accuracy: 0.8047\n",
            "Epoch 34/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0142 - accuracy: 0.9963 - val_loss: 1.2000 - val_accuracy: 0.8100\n",
            "Epoch 35/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0118 - accuracy: 0.9960 - val_loss: 1.3096 - val_accuracy: 0.7967\n",
            "Epoch 36/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0175 - accuracy: 0.9936 - val_loss: 1.2579 - val_accuracy: 0.7890\n",
            "Epoch 37/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0263 - accuracy: 0.9911 - val_loss: 1.3042 - val_accuracy: 0.8023\n",
            "Epoch 38/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0213 - accuracy: 0.9924 - val_loss: 1.2360 - val_accuracy: 0.8050\n",
            "Epoch 39/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0113 - accuracy: 0.9969 - val_loss: 1.3090 - val_accuracy: 0.7987\n",
            "Epoch 40/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0190 - accuracy: 0.9940 - val_loss: 1.3175 - val_accuracy: 0.7953\n",
            "Epoch 41/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0222 - accuracy: 0.9923 - val_loss: 1.2861 - val_accuracy: 0.8033\n",
            "Epoch 42/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0150 - accuracy: 0.9950 - val_loss: 1.2501 - val_accuracy: 0.7940\n",
            "Epoch 43/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0147 - accuracy: 0.9950 - val_loss: 1.3335 - val_accuracy: 0.7993\n",
            "Epoch 44/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0123 - accuracy: 0.9966 - val_loss: 1.2723 - val_accuracy: 0.8107\n",
            "Epoch 45/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0101 - accuracy: 0.9972 - val_loss: 1.3178 - val_accuracy: 0.8083\n",
            "Epoch 46/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0207 - accuracy: 0.9932 - val_loss: 1.3100 - val_accuracy: 0.8017\n",
            "Epoch 47/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0111 - accuracy: 0.9962 - val_loss: 1.3747 - val_accuracy: 0.7943\n",
            "Epoch 48/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0110 - accuracy: 0.9963 - val_loss: 1.3196 - val_accuracy: 0.8090\n",
            "Epoch 49/50\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.0207 - accuracy: 0.9919 - val_loss: 1.2077 - val_accuracy: 0.7950\n",
            "Epoch 50/50\n",
            "218/218 [==============================] - 1s 7ms/step - loss: 0.0134 - accuracy: 0.9951 - val_loss: 1.3936 - val_accuracy: 0.8067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shallow_cnn_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14pmv71C536r",
        "outputId": "7cc5ebb0-23ad-43b9-fc1a-1599a1e30fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{50: 0.7488713264465332,\n",
              " 100: 0.7573363184928894,\n",
              " 150: 0.7793453931808472,\n",
              " 200: 0.7914785742759705,\n",
              " 250: 0.7835778594017029}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN-LSTM"
      ],
      "metadata": {
        "id": "UI9eMg1u9Cv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "learning_rate = 1e-3\n",
        "epochs = 50\n",
        "rnn_optimizer = keras.optimizers.Adam(lr=learning_rate)"
      ],
      "metadata": {
        "id": "W1Iei1dU9dQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dropout, LSTM, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "def rnn_lstm_time(n):\n",
        "  rnn_lstm_model = Sequential()\n",
        "  rnn_lstm_model.add(LSTM(64, input_shape=(n,22), return_sequences=True))\n",
        "  rnn_lstm_model.add(LSTM(32, return_sequences=True))\n",
        "\n",
        "  rnn_lstm_model.add(Flatten())\n",
        "  rnn_lstm_model.add(Dropout(0.5))\n",
        "  rnn_lstm_model.add(Dense(4, activation='softmax'))\n",
        "  rnn_lstm_model.summary()\n",
        "\n",
        "  rnn_lstm_model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=rnn_optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "  return rnn_lstm_model"
      ],
      "metadata": {
        "id": "-MxoUr4W9FW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num = [50, 100, 150, 200, 250]\n",
        "\n",
        "rnn_lstm_dict = {}\n",
        "\n",
        "for n in num:\n",
        "  x_train = x_train_vae[:, :n, :, :]\n",
        "  x_train = np.reshape(x_train, (13920, n, 1, 22))\n",
        "\n",
        "  x_valid = x_valid_vae[:, :n, :, :]\n",
        "  x_valid = np.reshape(x_valid, (3000, n, 1, 22))\n",
        "\n",
        "  x_test = x_test_vae[:, :n, :, :]\n",
        "  x_test = np.reshape(x_test, (3544, n, 1, 22)) \n",
        "  \n",
        "  x_train_rnn = np.squeeze(x_train, axis=2)\n",
        "  x_valid_rnn = np.squeeze(x_valid, axis=2)\n",
        "\n",
        "  rnn_lstm_model = rnn_lstm_time(n)\n",
        "\n",
        "  rnn_lstm_model_results = rnn_lstm_model.fit(x_train_rnn,\n",
        "              y_train_vae,\n",
        "              batch_size=64,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_valid_rnn, y_valid_vae), verbose=True)\n",
        "  \n",
        "  x_test_rnn = np.squeeze(x_test, axis=2)\n",
        "\n",
        "  rnn_lstm_score = rnn_lstm_model.evaluate(x_test_rnn, y_test_vae, verbose=0)\n",
        "\n",
        "  rnn_lstm_dict[n] = rnn_lstm_score[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f43GBLX9aAO",
        "outputId": "8c5bd692-ac89-48d0-f139-9811a96ef2f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2 (LSTM)               (None, 50, 64)            22272     \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 50, 32)            12416     \n",
            "                                                                 \n",
            " flatten_32 (Flatten)        (None, 1600)              0         \n",
            "                                                                 \n",
            " dropout_95 (Dropout)        (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_42 (Dense)            (None, 4)                 6404      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 41,092\n",
            "Trainable params: 41,092\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "218/218 [==============================] - 7s 12ms/step - loss: 0.6814 - accuracy: 0.7042 - val_loss: 0.6116 - val_accuracy: 0.7180\n",
            "Epoch 2/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.5228 - accuracy: 0.7779 - val_loss: 0.6083 - val_accuracy: 0.7473\n",
            "Epoch 3/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.4297 - accuracy: 0.8236 - val_loss: 0.6510 - val_accuracy: 0.7390\n",
            "Epoch 4/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.3532 - accuracy: 0.8587 - val_loss: 0.7348 - val_accuracy: 0.7243\n",
            "Epoch 5/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.2838 - accuracy: 0.8887 - val_loss: 0.7345 - val_accuracy: 0.7423\n",
            "Epoch 6/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.2106 - accuracy: 0.9190 - val_loss: 0.8456 - val_accuracy: 0.7337\n",
            "Epoch 7/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.1741 - accuracy: 0.9335 - val_loss: 0.8558 - val_accuracy: 0.7407\n",
            "Epoch 8/50\n",
            "218/218 [==============================] - 2s 8ms/step - loss: 0.1217 - accuracy: 0.9550 - val_loss: 1.0062 - val_accuracy: 0.7303\n",
            "Epoch 9/50\n",
            "218/218 [==============================] - 2s 8ms/step - loss: 0.0965 - accuracy: 0.9647 - val_loss: 1.1727 - val_accuracy: 0.7243\n",
            "Epoch 10/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0814 - accuracy: 0.9694 - val_loss: 1.1565 - val_accuracy: 0.7250\n",
            "Epoch 11/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0707 - accuracy: 0.9742 - val_loss: 1.1807 - val_accuracy: 0.7380\n",
            "Epoch 12/50\n",
            "218/218 [==============================] - 2s 9ms/step - loss: 0.0665 - accuracy: 0.9767 - val_loss: 1.1964 - val_accuracy: 0.7413\n",
            "Epoch 13/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0641 - accuracy: 0.9784 - val_loss: 1.3785 - val_accuracy: 0.7327\n",
            "Epoch 14/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0433 - accuracy: 0.9848 - val_loss: 1.3287 - val_accuracy: 0.7447\n",
            "Epoch 15/50\n",
            "218/218 [==============================] - 2s 8ms/step - loss: 0.0413 - accuracy: 0.9855 - val_loss: 1.3787 - val_accuracy: 0.7343\n",
            "Epoch 16/50\n",
            "218/218 [==============================] - 2s 8ms/step - loss: 0.0474 - accuracy: 0.9832 - val_loss: 1.3971 - val_accuracy: 0.7270\n",
            "Epoch 17/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0406 - accuracy: 0.9851 - val_loss: 1.5065 - val_accuracy: 0.7227\n",
            "Epoch 18/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0411 - accuracy: 0.9849 - val_loss: 1.5776 - val_accuracy: 0.7273\n",
            "Epoch 19/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0239 - accuracy: 0.9918 - val_loss: 1.6003 - val_accuracy: 0.7320\n",
            "Epoch 20/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0234 - accuracy: 0.9911 - val_loss: 1.6204 - val_accuracy: 0.7350\n",
            "Epoch 21/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0468 - accuracy: 0.9830 - val_loss: 1.4285 - val_accuracy: 0.7347\n",
            "Epoch 22/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0324 - accuracy: 0.9880 - val_loss: 1.5701 - val_accuracy: 0.7380\n",
            "Epoch 23/50\n",
            "218/218 [==============================] - 2s 8ms/step - loss: 0.0290 - accuracy: 0.9901 - val_loss: 1.6261 - val_accuracy: 0.7373\n",
            "Epoch 24/50\n",
            "218/218 [==============================] - 2s 8ms/step - loss: 0.0426 - accuracy: 0.9846 - val_loss: 1.5310 - val_accuracy: 0.7250\n",
            "Epoch 25/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0396 - accuracy: 0.9857 - val_loss: 1.5882 - val_accuracy: 0.7343\n",
            "Epoch 26/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0124 - accuracy: 0.9958 - val_loss: 1.6671 - val_accuracy: 0.7340\n",
            "Epoch 27/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0145 - accuracy: 0.9955 - val_loss: 1.8882 - val_accuracy: 0.7280\n",
            "Epoch 28/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0428 - accuracy: 0.9844 - val_loss: 1.7085 - val_accuracy: 0.7177\n",
            "Epoch 29/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0363 - accuracy: 0.9872 - val_loss: 1.6975 - val_accuracy: 0.7227\n",
            "Epoch 30/50\n",
            "218/218 [==============================] - 2s 8ms/step - loss: 0.0324 - accuracy: 0.9887 - val_loss: 1.8060 - val_accuracy: 0.7233\n",
            "Epoch 31/50\n",
            "218/218 [==============================] - 2s 8ms/step - loss: 0.0262 - accuracy: 0.9901 - val_loss: 1.7027 - val_accuracy: 0.7367\n",
            "Epoch 32/50\n",
            "218/218 [==============================] - 2s 8ms/step - loss: 0.0157 - accuracy: 0.9948 - val_loss: 1.7591 - val_accuracy: 0.7367\n",
            "Epoch 33/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0172 - accuracy: 0.9945 - val_loss: 1.7125 - val_accuracy: 0.7283\n",
            "Epoch 34/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0212 - accuracy: 0.9926 - val_loss: 1.8441 - val_accuracy: 0.7253\n",
            "Epoch 35/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0310 - accuracy: 0.9902 - val_loss: 1.7750 - val_accuracy: 0.7193\n",
            "Epoch 36/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0265 - accuracy: 0.9911 - val_loss: 1.7845 - val_accuracy: 0.7257\n",
            "Epoch 37/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0297 - accuracy: 0.9901 - val_loss: 1.7701 - val_accuracy: 0.7333\n",
            "Epoch 38/50\n",
            "218/218 [==============================] - 2s 8ms/step - loss: 0.0162 - accuracy: 0.9944 - val_loss: 1.9302 - val_accuracy: 0.7260\n",
            "Epoch 39/50\n",
            "218/218 [==============================] - 2s 8ms/step - loss: 0.0252 - accuracy: 0.9912 - val_loss: 1.7274 - val_accuracy: 0.7370\n",
            "Epoch 40/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0179 - accuracy: 0.9935 - val_loss: 1.8948 - val_accuracy: 0.7313\n",
            "Epoch 41/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0119 - accuracy: 0.9960 - val_loss: 1.9328 - val_accuracy: 0.7283\n",
            "Epoch 42/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0086 - accuracy: 0.9975 - val_loss: 2.0075 - val_accuracy: 0.7257\n",
            "Epoch 43/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0310 - accuracy: 0.9890 - val_loss: 1.9592 - val_accuracy: 0.7173\n",
            "Epoch 44/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0470 - accuracy: 0.9835 - val_loss: 1.7593 - val_accuracy: 0.7357\n",
            "Epoch 45/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0272 - accuracy: 0.9896 - val_loss: 1.9098 - val_accuracy: 0.7333\n",
            "Epoch 46/50\n",
            "218/218 [==============================] - 2s 9ms/step - loss: 0.0208 - accuracy: 0.9927 - val_loss: 1.8795 - val_accuracy: 0.7320\n",
            "Epoch 47/50\n",
            "218/218 [==============================] - 2s 8ms/step - loss: 0.0112 - accuracy: 0.9963 - val_loss: 1.9464 - val_accuracy: 0.7273\n",
            "Epoch 48/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0111 - accuracy: 0.9962 - val_loss: 1.9688 - val_accuracy: 0.7203\n",
            "Epoch 49/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0283 - accuracy: 0.9907 - val_loss: 1.6847 - val_accuracy: 0.7420\n",
            "Epoch 50/50\n",
            "218/218 [==============================] - 2s 7ms/step - loss: 0.0237 - accuracy: 0.9919 - val_loss: 1.8443 - val_accuracy: 0.7310\n",
            "Model: \"sequential_33\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_4 (LSTM)               (None, 100, 64)           22272     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 100, 32)           12416     \n",
            "                                                                 \n",
            " flatten_33 (Flatten)        (None, 3200)              0         \n",
            "                                                                 \n",
            " dropout_96 (Dropout)        (None, 3200)              0         \n",
            "                                                                 \n",
            " dense_43 (Dense)            (None, 4)                 12804     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 47,492\n",
            "Trainable params: 47,492\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "218/218 [==============================] - 6s 15ms/step - loss: 0.6099 - accuracy: 0.7358 - val_loss: 0.5965 - val_accuracy: 0.7350\n",
            "Epoch 2/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.4267 - accuracy: 0.8252 - val_loss: 0.6258 - val_accuracy: 0.7520\n",
            "Epoch 3/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.3180 - accuracy: 0.8739 - val_loss: 0.6662 - val_accuracy: 0.7557\n",
            "Epoch 4/50\n",
            "218/218 [==============================] - 2s 9ms/step - loss: 0.2275 - accuracy: 0.9124 - val_loss: 0.7495 - val_accuracy: 0.7527\n",
            "Epoch 5/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.1709 - accuracy: 0.9389 - val_loss: 0.8164 - val_accuracy: 0.7560\n",
            "Epoch 6/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.1263 - accuracy: 0.9545 - val_loss: 0.8636 - val_accuracy: 0.7617\n",
            "Epoch 7/50\n",
            "218/218 [==============================] - 2s 11ms/step - loss: 0.0994 - accuracy: 0.9647 - val_loss: 0.9407 - val_accuracy: 0.7617\n",
            "Epoch 8/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0836 - accuracy: 0.9694 - val_loss: 1.0196 - val_accuracy: 0.7583\n",
            "Epoch 9/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0571 - accuracy: 0.9801 - val_loss: 1.0422 - val_accuracy: 0.7657\n",
            "Epoch 10/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0530 - accuracy: 0.9812 - val_loss: 1.1137 - val_accuracy: 0.7520\n",
            "Epoch 11/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0483 - accuracy: 0.9826 - val_loss: 1.1216 - val_accuracy: 0.7520\n",
            "Epoch 12/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0461 - accuracy: 0.9848 - val_loss: 1.2256 - val_accuracy: 0.7567\n",
            "Epoch 13/50\n",
            "218/218 [==============================] - 2s 11ms/step - loss: 0.0352 - accuracy: 0.9879 - val_loss: 1.2410 - val_accuracy: 0.7603\n",
            "Epoch 14/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0345 - accuracy: 0.9886 - val_loss: 1.2748 - val_accuracy: 0.7577\n",
            "Epoch 15/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0335 - accuracy: 0.9882 - val_loss: 1.3927 - val_accuracy: 0.7460\n",
            "Epoch 16/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0318 - accuracy: 0.9892 - val_loss: 1.3266 - val_accuracy: 0.7470\n",
            "Epoch 17/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0257 - accuracy: 0.9912 - val_loss: 1.3202 - val_accuracy: 0.7667\n",
            "Epoch 18/50\n",
            "218/218 [==============================] - 2s 11ms/step - loss: 0.0217 - accuracy: 0.9926 - val_loss: 1.4040 - val_accuracy: 0.7613\n",
            "Epoch 19/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0209 - accuracy: 0.9936 - val_loss: 1.4549 - val_accuracy: 0.7557\n",
            "Epoch 20/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0298 - accuracy: 0.9889 - val_loss: 1.5269 - val_accuracy: 0.7433\n",
            "Epoch 21/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0228 - accuracy: 0.9923 - val_loss: 1.4801 - val_accuracy: 0.7503\n",
            "Epoch 22/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0310 - accuracy: 0.9894 - val_loss: 1.4648 - val_accuracy: 0.7573\n",
            "Epoch 23/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0228 - accuracy: 0.9916 - val_loss: 1.5456 - val_accuracy: 0.7567\n",
            "Epoch 24/50\n",
            "218/218 [==============================] - 2s 11ms/step - loss: 0.0200 - accuracy: 0.9928 - val_loss: 1.5538 - val_accuracy: 0.7537\n",
            "Epoch 25/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0184 - accuracy: 0.9935 - val_loss: 1.6151 - val_accuracy: 0.7560\n",
            "Epoch 26/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0255 - accuracy: 0.9924 - val_loss: 1.4952 - val_accuracy: 0.7550\n",
            "Epoch 27/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0227 - accuracy: 0.9916 - val_loss: 1.5963 - val_accuracy: 0.7553\n",
            "Epoch 28/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0206 - accuracy: 0.9927 - val_loss: 1.5307 - val_accuracy: 0.7533\n",
            "Epoch 29/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0173 - accuracy: 0.9942 - val_loss: 1.6238 - val_accuracy: 0.7617\n",
            "Epoch 30/50\n",
            "218/218 [==============================] - 2s 11ms/step - loss: 0.0193 - accuracy: 0.9932 - val_loss: 1.5364 - val_accuracy: 0.7520\n",
            "Epoch 31/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0192 - accuracy: 0.9933 - val_loss: 1.6218 - val_accuracy: 0.7493\n",
            "Epoch 32/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0170 - accuracy: 0.9940 - val_loss: 1.7238 - val_accuracy: 0.7553\n",
            "Epoch 33/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0189 - accuracy: 0.9932 - val_loss: 1.7545 - val_accuracy: 0.7457\n",
            "Epoch 34/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0200 - accuracy: 0.9935 - val_loss: 1.6476 - val_accuracy: 0.7560\n",
            "Epoch 35/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0189 - accuracy: 0.9931 - val_loss: 1.6793 - val_accuracy: 0.7487\n",
            "Epoch 36/50\n",
            "218/218 [==============================] - 2s 11ms/step - loss: 0.0128 - accuracy: 0.9956 - val_loss: 1.7202 - val_accuracy: 0.7497\n",
            "Epoch 37/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0148 - accuracy: 0.9948 - val_loss: 1.8028 - val_accuracy: 0.7483\n",
            "Epoch 38/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0235 - accuracy: 0.9916 - val_loss: 1.7734 - val_accuracy: 0.7477\n",
            "Epoch 39/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0160 - accuracy: 0.9941 - val_loss: 1.6521 - val_accuracy: 0.7603\n",
            "Epoch 40/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0129 - accuracy: 0.9954 - val_loss: 1.7221 - val_accuracy: 0.7603\n",
            "Epoch 41/50\n",
            "218/218 [==============================] - 2s 11ms/step - loss: 0.0168 - accuracy: 0.9940 - val_loss: 1.7512 - val_accuracy: 0.7573\n",
            "Epoch 42/50\n",
            "218/218 [==============================] - 2s 11ms/step - loss: 0.0124 - accuracy: 0.9958 - val_loss: 1.7883 - val_accuracy: 0.7533\n",
            "Epoch 43/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0199 - accuracy: 0.9930 - val_loss: 1.8443 - val_accuracy: 0.7450\n",
            "Epoch 44/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0210 - accuracy: 0.9933 - val_loss: 1.6257 - val_accuracy: 0.7660\n",
            "Epoch 45/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0202 - accuracy: 0.9927 - val_loss: 1.7952 - val_accuracy: 0.7557\n",
            "Epoch 46/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0133 - accuracy: 0.9951 - val_loss: 1.7798 - val_accuracy: 0.7537\n",
            "Epoch 47/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0078 - accuracy: 0.9973 - val_loss: 1.7760 - val_accuracy: 0.7520\n",
            "Epoch 48/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0158 - accuracy: 0.9942 - val_loss: 1.7857 - val_accuracy: 0.7560\n",
            "Epoch 49/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0148 - accuracy: 0.9952 - val_loss: 1.7833 - val_accuracy: 0.7570\n",
            "Epoch 50/50\n",
            "218/218 [==============================] - 2s 10ms/step - loss: 0.0194 - accuracy: 0.9930 - val_loss: 1.7604 - val_accuracy: 0.7560\n",
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_6 (LSTM)               (None, 150, 64)           22272     \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 150, 32)           12416     \n",
            "                                                                 \n",
            " flatten_34 (Flatten)        (None, 4800)              0         \n",
            "                                                                 \n",
            " dropout_97 (Dropout)        (None, 4800)              0         \n",
            "                                                                 \n",
            " dense_44 (Dense)            (None, 4)                 19204     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 53,892\n",
            "Trainable params: 53,892\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "218/218 [==============================] - 7s 20ms/step - loss: 0.5900 - accuracy: 0.7462 - val_loss: 0.5654 - val_accuracy: 0.7537\n",
            "Epoch 2/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.3648 - accuracy: 0.8564 - val_loss: 0.6011 - val_accuracy: 0.7560\n",
            "Epoch 3/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.2344 - accuracy: 0.9125 - val_loss: 0.6902 - val_accuracy: 0.7670\n",
            "Epoch 4/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.1559 - accuracy: 0.9448 - val_loss: 0.7147 - val_accuracy: 0.7693\n",
            "Epoch 5/50\n",
            "218/218 [==============================] - 3s 13ms/step - loss: 0.1041 - accuracy: 0.9642 - val_loss: 0.7639 - val_accuracy: 0.7820\n",
            "Epoch 6/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0777 - accuracy: 0.9721 - val_loss: 0.8668 - val_accuracy: 0.7710\n",
            "Epoch 7/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0550 - accuracy: 0.9802 - val_loss: 0.9712 - val_accuracy: 0.7763\n",
            "Epoch 8/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0476 - accuracy: 0.9838 - val_loss: 0.9848 - val_accuracy: 0.7763\n",
            "Epoch 9/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0414 - accuracy: 0.9857 - val_loss: 1.0682 - val_accuracy: 0.7747\n",
            "Epoch 10/50\n",
            "218/218 [==============================] - 3s 13ms/step - loss: 0.0345 - accuracy: 0.9878 - val_loss: 1.0068 - val_accuracy: 0.7767\n",
            "Epoch 11/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0302 - accuracy: 0.9897 - val_loss: 1.0414 - val_accuracy: 0.7847\n",
            "Epoch 12/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0308 - accuracy: 0.9899 - val_loss: 1.0891 - val_accuracy: 0.7767\n",
            "Epoch 13/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0281 - accuracy: 0.9892 - val_loss: 1.1478 - val_accuracy: 0.7797\n",
            "Epoch 14/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0270 - accuracy: 0.9909 - val_loss: 1.2159 - val_accuracy: 0.7623\n",
            "Epoch 15/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0239 - accuracy: 0.9913 - val_loss: 1.1802 - val_accuracy: 0.7807\n",
            "Epoch 16/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0191 - accuracy: 0.9937 - val_loss: 1.2898 - val_accuracy: 0.7660\n",
            "Epoch 17/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0186 - accuracy: 0.9932 - val_loss: 1.2659 - val_accuracy: 0.7767\n",
            "Epoch 18/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0188 - accuracy: 0.9937 - val_loss: 1.2007 - val_accuracy: 0.7733\n",
            "Epoch 19/50\n",
            "218/218 [==============================] - 3s 13ms/step - loss: 0.0121 - accuracy: 0.9959 - val_loss: 1.2976 - val_accuracy: 0.7790\n",
            "Epoch 20/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0189 - accuracy: 0.9935 - val_loss: 1.2398 - val_accuracy: 0.7753\n",
            "Epoch 21/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0203 - accuracy: 0.9938 - val_loss: 1.4154 - val_accuracy: 0.7567\n",
            "Epoch 22/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0218 - accuracy: 0.9919 - val_loss: 1.2632 - val_accuracy: 0.7693\n",
            "Epoch 23/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0196 - accuracy: 0.9927 - val_loss: 1.3830 - val_accuracy: 0.7643\n",
            "Epoch 24/50\n",
            "218/218 [==============================] - 3s 13ms/step - loss: 0.0180 - accuracy: 0.9935 - val_loss: 1.3477 - val_accuracy: 0.7847\n",
            "Epoch 25/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0142 - accuracy: 0.9945 - val_loss: 1.3721 - val_accuracy: 0.7803\n",
            "Epoch 26/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0177 - accuracy: 0.9934 - val_loss: 1.2777 - val_accuracy: 0.7810\n",
            "Epoch 27/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0208 - accuracy: 0.9928 - val_loss: 1.5250 - val_accuracy: 0.7640\n",
            "Epoch 28/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0178 - accuracy: 0.9934 - val_loss: 1.4064 - val_accuracy: 0.7800\n",
            "Epoch 29/50\n",
            "218/218 [==============================] - 3s 13ms/step - loss: 0.0158 - accuracy: 0.9942 - val_loss: 1.3999 - val_accuracy: 0.7737\n",
            "Epoch 30/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0111 - accuracy: 0.9965 - val_loss: 1.4141 - val_accuracy: 0.7783\n",
            "Epoch 31/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0152 - accuracy: 0.9948 - val_loss: 1.3963 - val_accuracy: 0.7677\n",
            "Epoch 32/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0136 - accuracy: 0.9949 - val_loss: 1.4326 - val_accuracy: 0.7800\n",
            "Epoch 33/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0151 - accuracy: 0.9943 - val_loss: 1.4713 - val_accuracy: 0.7800\n",
            "Epoch 34/50\n",
            "218/218 [==============================] - 3s 13ms/step - loss: 0.0189 - accuracy: 0.9936 - val_loss: 1.5941 - val_accuracy: 0.7687\n",
            "Epoch 35/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0123 - accuracy: 0.9950 - val_loss: 1.5867 - val_accuracy: 0.7767\n",
            "Epoch 36/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0217 - accuracy: 0.9926 - val_loss: 1.4885 - val_accuracy: 0.7737\n",
            "Epoch 37/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0099 - accuracy: 0.9969 - val_loss: 1.5015 - val_accuracy: 0.7817\n",
            "Epoch 38/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0123 - accuracy: 0.9957 - val_loss: 1.4312 - val_accuracy: 0.7757\n",
            "Epoch 39/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0136 - accuracy: 0.9948 - val_loss: 1.4518 - val_accuracy: 0.7800\n",
            "Epoch 40/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0155 - accuracy: 0.9937 - val_loss: 1.6030 - val_accuracy: 0.7737\n",
            "Epoch 41/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0165 - accuracy: 0.9947 - val_loss: 1.4749 - val_accuracy: 0.7707\n",
            "Epoch 42/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0107 - accuracy: 0.9959 - val_loss: 1.5560 - val_accuracy: 0.7690\n",
            "Epoch 43/50\n",
            "218/218 [==============================] - 3s 13ms/step - loss: 0.0148 - accuracy: 0.9957 - val_loss: 1.6225 - val_accuracy: 0.7600\n",
            "Epoch 44/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 1.6251 - val_accuracy: 0.7773\n",
            "Epoch 45/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0166 - accuracy: 0.9938 - val_loss: 1.6196 - val_accuracy: 0.7710\n",
            "Epoch 46/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0180 - accuracy: 0.9936 - val_loss: 1.7065 - val_accuracy: 0.7677\n",
            "Epoch 47/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0131 - accuracy: 0.9960 - val_loss: 1.5227 - val_accuracy: 0.7787\n",
            "Epoch 48/50\n",
            "218/218 [==============================] - 3s 13ms/step - loss: 0.0106 - accuracy: 0.9967 - val_loss: 1.6897 - val_accuracy: 0.7627\n",
            "Epoch 49/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0105 - accuracy: 0.9971 - val_loss: 1.7049 - val_accuracy: 0.7713\n",
            "Epoch 50/50\n",
            "218/218 [==============================] - 3s 12ms/step - loss: 0.0169 - accuracy: 0.9944 - val_loss: 1.7644 - val_accuracy: 0.7610\n",
            "Model: \"sequential_35\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_8 (LSTM)               (None, 200, 64)           22272     \n",
            "                                                                 \n",
            " lstm_9 (LSTM)               (None, 200, 32)           12416     \n",
            "                                                                 \n",
            " flatten_35 (Flatten)        (None, 6400)              0         \n",
            "                                                                 \n",
            " dropout_98 (Dropout)        (None, 6400)              0         \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 4)                 25604     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 60,292\n",
            "Trainable params: 60,292\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "218/218 [==============================] - 7s 21ms/step - loss: 0.5471 - accuracy: 0.7740 - val_loss: 0.5384 - val_accuracy: 0.7773\n",
            "Epoch 2/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.2979 - accuracy: 0.8844 - val_loss: 0.5642 - val_accuracy: 0.7877\n",
            "Epoch 3/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.1738 - accuracy: 0.9341 - val_loss: 0.6173 - val_accuracy: 0.7903\n",
            "Epoch 4/50\n",
            "218/218 [==============================] - 3s 15ms/step - loss: 0.1067 - accuracy: 0.9614 - val_loss: 0.6873 - val_accuracy: 0.7960\n",
            "Epoch 5/50\n",
            "218/218 [==============================] - 3s 15ms/step - loss: 0.0665 - accuracy: 0.9770 - val_loss: 0.7789 - val_accuracy: 0.7940\n",
            "Epoch 6/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0576 - accuracy: 0.9798 - val_loss: 0.8420 - val_accuracy: 0.7803\n",
            "Epoch 7/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0459 - accuracy: 0.9844 - val_loss: 0.8960 - val_accuracy: 0.7860\n",
            "Epoch 8/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0357 - accuracy: 0.9877 - val_loss: 0.8888 - val_accuracy: 0.7897\n",
            "Epoch 9/50\n",
            "218/218 [==============================] - 3s 15ms/step - loss: 0.0311 - accuracy: 0.9892 - val_loss: 0.9334 - val_accuracy: 0.7880\n",
            "Epoch 10/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0264 - accuracy: 0.9907 - val_loss: 0.9976 - val_accuracy: 0.7860\n",
            "Epoch 11/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0237 - accuracy: 0.9916 - val_loss: 0.9725 - val_accuracy: 0.7857\n",
            "Epoch 12/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0239 - accuracy: 0.9913 - val_loss: 1.0875 - val_accuracy: 0.7983\n",
            "Epoch 13/50\n",
            "218/218 [==============================] - 3s 15ms/step - loss: 0.0291 - accuracy: 0.9901 - val_loss: 1.1087 - val_accuracy: 0.7773\n",
            "Epoch 14/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0174 - accuracy: 0.9946 - val_loss: 1.0999 - val_accuracy: 0.7840\n",
            "Epoch 15/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0182 - accuracy: 0.9934 - val_loss: 1.0502 - val_accuracy: 0.7953\n",
            "Epoch 16/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0194 - accuracy: 0.9937 - val_loss: 1.1173 - val_accuracy: 0.7910\n",
            "Epoch 17/50\n",
            "218/218 [==============================] - 3s 15ms/step - loss: 0.0210 - accuracy: 0.9924 - val_loss: 1.1466 - val_accuracy: 0.7893\n",
            "Epoch 18/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0136 - accuracy: 0.9959 - val_loss: 1.1088 - val_accuracy: 0.7930\n",
            "Epoch 19/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0118 - accuracy: 0.9962 - val_loss: 1.2201 - val_accuracy: 0.7843\n",
            "Epoch 20/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0233 - accuracy: 0.9915 - val_loss: 1.2363 - val_accuracy: 0.7860\n",
            "Epoch 21/50\n",
            "218/218 [==============================] - 3s 15ms/step - loss: 0.0239 - accuracy: 0.9917 - val_loss: 1.1861 - val_accuracy: 0.7900\n",
            "Epoch 22/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0138 - accuracy: 0.9955 - val_loss: 1.2784 - val_accuracy: 0.7850\n",
            "Epoch 23/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0169 - accuracy: 0.9944 - val_loss: 1.1911 - val_accuracy: 0.7903\n",
            "Epoch 24/50\n",
            "218/218 [==============================] - 3s 15ms/step - loss: 0.0180 - accuracy: 0.9934 - val_loss: 1.3237 - val_accuracy: 0.7853\n",
            "Epoch 25/50\n",
            "218/218 [==============================] - 3s 15ms/step - loss: 0.0215 - accuracy: 0.9926 - val_loss: 1.2125 - val_accuracy: 0.7820\n",
            "Epoch 26/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0167 - accuracy: 0.9939 - val_loss: 1.3071 - val_accuracy: 0.7837\n",
            "Epoch 27/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0158 - accuracy: 0.9947 - val_loss: 1.1271 - val_accuracy: 0.7927\n",
            "Epoch 28/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0113 - accuracy: 0.9952 - val_loss: 1.2686 - val_accuracy: 0.7890\n",
            "Epoch 29/50\n",
            "218/218 [==============================] - 3s 15ms/step - loss: 0.0114 - accuracy: 0.9959 - val_loss: 1.2908 - val_accuracy: 0.7900\n",
            "Epoch 30/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0149 - accuracy: 0.9948 - val_loss: 1.3471 - val_accuracy: 0.7883\n",
            "Epoch 31/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0150 - accuracy: 0.9944 - val_loss: 1.2875 - val_accuracy: 0.7887\n",
            "Epoch 32/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0176 - accuracy: 0.9940 - val_loss: 1.3492 - val_accuracy: 0.7870\n",
            "Epoch 33/50\n",
            "218/218 [==============================] - 3s 15ms/step - loss: 0.0163 - accuracy: 0.9943 - val_loss: 1.2753 - val_accuracy: 0.7913\n",
            "Epoch 34/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0135 - accuracy: 0.9953 - val_loss: 1.3622 - val_accuracy: 0.7983\n",
            "Epoch 35/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0144 - accuracy: 0.9950 - val_loss: 1.3265 - val_accuracy: 0.7880\n",
            "Epoch 36/50\n",
            "218/218 [==============================] - 3s 15ms/step - loss: 0.0117 - accuracy: 0.9966 - val_loss: 1.2453 - val_accuracy: 0.8060\n",
            "Epoch 37/50\n",
            "218/218 [==============================] - 3s 15ms/step - loss: 0.0178 - accuracy: 0.9941 - val_loss: 1.4952 - val_accuracy: 0.7793\n",
            "Epoch 38/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0184 - accuracy: 0.9930 - val_loss: 1.4263 - val_accuracy: 0.7813\n",
            "Epoch 39/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0323 - accuracy: 0.9891 - val_loss: 1.3420 - val_accuracy: 0.8007\n",
            "Epoch 40/50\n",
            "218/218 [==============================] - 3s 15ms/step - loss: 0.0093 - accuracy: 0.9968 - val_loss: 1.4081 - val_accuracy: 0.7907\n",
            "Epoch 41/50\n",
            "218/218 [==============================] - 3s 15ms/step - loss: 0.0122 - accuracy: 0.9957 - val_loss: 1.4097 - val_accuracy: 0.7923\n",
            "Epoch 42/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0077 - accuracy: 0.9970 - val_loss: 1.4594 - val_accuracy: 0.7893\n",
            "Epoch 43/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0104 - accuracy: 0.9968 - val_loss: 1.3543 - val_accuracy: 0.7887\n",
            "Epoch 44/50\n",
            "218/218 [==============================] - 3s 15ms/step - loss: 0.0110 - accuracy: 0.9966 - val_loss: 1.3928 - val_accuracy: 0.7980\n",
            "Epoch 45/50\n",
            "218/218 [==============================] - 3s 15ms/step - loss: 0.0096 - accuracy: 0.9966 - val_loss: 1.3406 - val_accuracy: 0.7963\n",
            "Epoch 46/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0104 - accuracy: 0.9963 - val_loss: 1.4192 - val_accuracy: 0.7840\n",
            "Epoch 47/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0159 - accuracy: 0.9943 - val_loss: 1.4937 - val_accuracy: 0.7780\n",
            "Epoch 48/50\n",
            "218/218 [==============================] - 3s 15ms/step - loss: 0.0149 - accuracy: 0.9948 - val_loss: 1.4385 - val_accuracy: 0.7910\n",
            "Epoch 49/50\n",
            "218/218 [==============================] - 3s 15ms/step - loss: 0.0118 - accuracy: 0.9956 - val_loss: 1.3835 - val_accuracy: 0.7913\n",
            "Epoch 50/50\n",
            "218/218 [==============================] - 3s 14ms/step - loss: 0.0081 - accuracy: 0.9971 - val_loss: 1.4075 - val_accuracy: 0.7940\n",
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_10 (LSTM)              (None, 250, 64)           22272     \n",
            "                                                                 \n",
            " lstm_11 (LSTM)              (None, 250, 32)           12416     \n",
            "                                                                 \n",
            " flatten_36 (Flatten)        (None, 8000)              0         \n",
            "                                                                 \n",
            " dropout_99 (Dropout)        (None, 8000)              0         \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 4)                 32004     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 66,692\n",
            "Trainable params: 66,692\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "218/218 [==============================] - 7s 21ms/step - loss: 0.5523 - accuracy: 0.7775 - val_loss: 0.5465 - val_accuracy: 0.7777\n",
            "Epoch 2/50\n",
            "218/218 [==============================] - 4s 18ms/step - loss: 0.2804 - accuracy: 0.8958 - val_loss: 0.6489 - val_accuracy: 0.7880\n",
            "Epoch 3/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.1502 - accuracy: 0.9447 - val_loss: 0.6959 - val_accuracy: 0.7927\n",
            "Epoch 4/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0903 - accuracy: 0.9689 - val_loss: 0.8102 - val_accuracy: 0.7853\n",
            "Epoch 5/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0609 - accuracy: 0.9779 - val_loss: 0.8786 - val_accuracy: 0.7853\n",
            "Epoch 6/50\n",
            "218/218 [==============================] - 4s 18ms/step - loss: 0.0482 - accuracy: 0.9838 - val_loss: 0.9066 - val_accuracy: 0.7957\n",
            "Epoch 7/50\n",
            "218/218 [==============================] - 4s 16ms/step - loss: 0.0386 - accuracy: 0.9870 - val_loss: 0.9437 - val_accuracy: 0.7937\n",
            "Epoch 8/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0381 - accuracy: 0.9858 - val_loss: 0.9247 - val_accuracy: 0.7923\n",
            "Epoch 9/50\n",
            "218/218 [==============================] - 4s 18ms/step - loss: 0.0271 - accuracy: 0.9914 - val_loss: 1.0105 - val_accuracy: 0.7980\n",
            "Epoch 10/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0196 - accuracy: 0.9937 - val_loss: 1.0637 - val_accuracy: 0.7953\n",
            "Epoch 11/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0224 - accuracy: 0.9926 - val_loss: 1.0678 - val_accuracy: 0.7973\n",
            "Epoch 12/50\n",
            "218/218 [==============================] - 4s 18ms/step - loss: 0.0206 - accuracy: 0.9931 - val_loss: 1.0885 - val_accuracy: 0.7953\n",
            "Epoch 13/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0199 - accuracy: 0.9935 - val_loss: 1.1708 - val_accuracy: 0.7987\n",
            "Epoch 14/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0178 - accuracy: 0.9937 - val_loss: 1.1471 - val_accuracy: 0.7917\n",
            "Epoch 15/50\n",
            "218/218 [==============================] - 4s 19ms/step - loss: 0.0185 - accuracy: 0.9938 - val_loss: 1.1071 - val_accuracy: 0.7990\n",
            "Epoch 16/50\n",
            "218/218 [==============================] - 4s 18ms/step - loss: 0.0179 - accuracy: 0.9943 - val_loss: 1.2117 - val_accuracy: 0.7940\n",
            "Epoch 17/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0139 - accuracy: 0.9950 - val_loss: 1.2774 - val_accuracy: 0.7800\n",
            "Epoch 18/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0236 - accuracy: 0.9922 - val_loss: 1.2372 - val_accuracy: 0.7947\n",
            "Epoch 19/50\n",
            "218/218 [==============================] - 4s 19ms/step - loss: 0.0215 - accuracy: 0.9929 - val_loss: 1.1706 - val_accuracy: 0.7897\n",
            "Epoch 20/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0158 - accuracy: 0.9950 - val_loss: 1.1539 - val_accuracy: 0.7953\n",
            "Epoch 21/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0151 - accuracy: 0.9945 - val_loss: 1.2462 - val_accuracy: 0.7917\n",
            "Epoch 22/50\n",
            "218/218 [==============================] - 4s 19ms/step - loss: 0.0144 - accuracy: 0.9948 - val_loss: 1.2729 - val_accuracy: 0.7850\n",
            "Epoch 23/50\n",
            "218/218 [==============================] - 4s 18ms/step - loss: 0.0180 - accuracy: 0.9930 - val_loss: 1.3617 - val_accuracy: 0.7843\n",
            "Epoch 24/50\n",
            "218/218 [==============================] - 4s 18ms/step - loss: 0.0165 - accuracy: 0.9945 - val_loss: 1.3440 - val_accuracy: 0.7930\n",
            "Epoch 25/50\n",
            "218/218 [==============================] - 4s 19ms/step - loss: 0.0134 - accuracy: 0.9955 - val_loss: 1.3784 - val_accuracy: 0.7887\n",
            "Epoch 26/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0091 - accuracy: 0.9966 - val_loss: 1.2881 - val_accuracy: 0.7917\n",
            "Epoch 27/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0206 - accuracy: 0.9928 - val_loss: 1.2842 - val_accuracy: 0.7967\n",
            "Epoch 28/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0155 - accuracy: 0.9948 - val_loss: 1.4089 - val_accuracy: 0.7880\n",
            "Epoch 29/50\n",
            "218/218 [==============================] - 4s 18ms/step - loss: 0.0102 - accuracy: 0.9964 - val_loss: 1.4182 - val_accuracy: 0.7950\n",
            "Epoch 30/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0100 - accuracy: 0.9963 - val_loss: 1.3471 - val_accuracy: 0.8057\n",
            "Epoch 31/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0219 - accuracy: 0.9928 - val_loss: 1.4469 - val_accuracy: 0.7813\n",
            "Epoch 32/50\n",
            "218/218 [==============================] - 4s 18ms/step - loss: 0.0219 - accuracy: 0.9927 - val_loss: 1.4494 - val_accuracy: 0.7773\n",
            "Epoch 33/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0133 - accuracy: 0.9955 - val_loss: 1.3925 - val_accuracy: 0.7857\n",
            "Epoch 34/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0090 - accuracy: 0.9963 - val_loss: 1.4715 - val_accuracy: 0.7853\n",
            "Epoch 35/50\n",
            "218/218 [==============================] - 4s 18ms/step - loss: 0.0088 - accuracy: 0.9971 - val_loss: 1.4668 - val_accuracy: 0.7840\n",
            "Epoch 36/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0135 - accuracy: 0.9955 - val_loss: 1.4543 - val_accuracy: 0.7877\n",
            "Epoch 37/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0237 - accuracy: 0.9917 - val_loss: 1.4641 - val_accuracy: 0.7820\n",
            "Epoch 38/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0173 - accuracy: 0.9938 - val_loss: 1.5016 - val_accuracy: 0.7907\n",
            "Epoch 39/50\n",
            "218/218 [==============================] - 4s 18ms/step - loss: 0.0077 - accuracy: 0.9971 - val_loss: 1.4703 - val_accuracy: 0.7903\n",
            "Epoch 40/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0145 - accuracy: 0.9949 - val_loss: 1.5232 - val_accuracy: 0.7887\n",
            "Epoch 41/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0135 - accuracy: 0.9955 - val_loss: 1.5019 - val_accuracy: 0.7890\n",
            "Epoch 42/50\n",
            "218/218 [==============================] - 4s 18ms/step - loss: 0.0121 - accuracy: 0.9957 - val_loss: 1.5144 - val_accuracy: 0.7843\n",
            "Epoch 43/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0115 - accuracy: 0.9962 - val_loss: 1.6509 - val_accuracy: 0.7843\n",
            "Epoch 44/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0136 - accuracy: 0.9950 - val_loss: 1.5341 - val_accuracy: 0.7910\n",
            "Epoch 45/50\n",
            "218/218 [==============================] - 4s 18ms/step - loss: 0.0084 - accuracy: 0.9975 - val_loss: 1.6101 - val_accuracy: 0.7897\n",
            "Epoch 46/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0100 - accuracy: 0.9966 - val_loss: 1.5814 - val_accuracy: 0.7940\n",
            "Epoch 47/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0098 - accuracy: 0.9967 - val_loss: 1.4914 - val_accuracy: 0.7920\n",
            "Epoch 48/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0205 - accuracy: 0.9927 - val_loss: 1.5942 - val_accuracy: 0.7897\n",
            "Epoch 49/50\n",
            "218/218 [==============================] - 4s 18ms/step - loss: 0.0120 - accuracy: 0.9957 - val_loss: 1.5808 - val_accuracy: 0.7860\n",
            "Epoch 50/50\n",
            "218/218 [==============================] - 4s 17ms/step - loss: 0.0200 - accuracy: 0.9926 - val_loss: 1.5881 - val_accuracy: 0.7827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_lstm_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLuPqQVM-EKX",
        "outputId": "03f8f0f7-b1b6-4aa2-d045-79781bfd6390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{50: 0.7291196584701538,\n",
              " 100: 0.7542325258255005,\n",
              " 150: 0.765237033367157,\n",
              " 200: 0.7765237092971802,\n",
              " 250: 0.7703160047531128}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: Right and Left Hand Accuracy\n",
        "\n",
        "\n",
        "\n",
        "*   Does the Right and Left Hand Accuracy improve when only training on the Class 0 and Class 1 Data compared to all classes?\n",
        "\n"
      ],
      "metadata": {
        "id": "dXVL6HpJ7-C4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subset Class 0 and Class 1 Data"
      ],
      "metadata": {
        "id": "lGvDRAoqx1Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_vae = np.load(\"drive/MyDrive/ecengr 247/Preprocessed_Data/x_train_vae.npy\")\n",
        "y_train_vae = np.load(\"drive/MyDrive/ecengr 247/Preprocessed_Data/y_train_vae.npy\")\n",
        "x_valid_vae = np.load(\"drive/MyDrive/ecengr 247/Preprocessed_Data/x_valid_vae.npy\")\n",
        "y_valid_vae = np.load(\"drive/MyDrive/ecengr 247/Preprocessed_Data/y_valid_vae.npy\")\n",
        "x_test_vae = np.load(\"drive/MyDrive/ecengr 247/Preprocessed_Data/x_test_vae.npy\")\n",
        "y_test_vae = np.load(\"drive/MyDrive/ecengr 247/Preprocessed_Data/y_test_vae.npy\")"
      ],
      "metadata": {
        "id": "jmigYUVR9FdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract Class 0 and Class 1 Data\n",
        "y_train_vae_b = np.argmax(y_train_vae, axis = 1)\n",
        "y_valid_vae_b = np.argmax(y_valid_vae, axis = 1)\n",
        "y_test_vae_b = np.argmax(y_test_vae, axis = 1)\n"
      ],
      "metadata": {
        "id": "Vo4kTScesZuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract indices \n",
        "class_0_idx_train = np.where(y_train_vae_b == 0)[0]\n",
        "class_0_idx_valid = np.where(y_valid_vae_b == 0)[0]\n",
        "class_0_idx_test = np.where(y_test_vae_b == 0)[0]\n",
        "\n",
        "class_1_idx_train = np.where(y_train_vae_b == 1)[0]\n",
        "class_1_idx_valid = np.where(y_valid_vae_b == 1)[0]\n",
        "class_1_idx_test = np.where(y_test_vae_b == 1)[0]\n",
        "\n",
        "# subset class 0\n",
        "x_train_vae_class_0 = x_train_vae[class_0_idx_train, :, :, ]\n",
        "y_train_vae_class_0= y_train_vae[class_0_idx_train]\n",
        "\n",
        "x_valid_vae_class_0 = x_valid_vae[class_0_idx_valid, :, :, ]\n",
        "y_valid_vae_class_0= y_valid_vae[class_0_idx_valid]\n",
        "\n",
        "x_test_vae_class_0 = x_test_vae[class_0_idx_test, :, :, ]\n",
        "y_test_vae_class_0= y_test_vae[class_0_idx_test]\n",
        "\n",
        "# subset class 1\n",
        "x_train_vae_class_1 = x_train_vae[class_1_idx_train, :, :, ]\n",
        "y_train_vae_class_1= y_train_vae[class_1_idx_train]\n",
        "\n",
        "x_valid_vae_class_1 = x_valid_vae[class_1_idx_valid, :, :, ]\n",
        "y_valid_vae_class_1= y_valid_vae[class_1_idx_valid]\n",
        "\n",
        "x_test_vae_class_1 = x_test_vae[class_1_idx_test, :, :, ]\n",
        "y_test_vae_class_1= y_test_vae[class_1_idx_test]"
      ],
      "metadata": {
        "id": "z04Yu2dzvXF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(class_0_idx_train.shape)\n",
        "print(class_1_idx_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TIfntww2WIB",
        "outputId": "88054976-59b5-4c95-b842-ede67e5414c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8732,)\n",
            "(1636,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Class labels for hand: 0 and 1\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "x_train_vae_hand = np.vstack((x_train_vae_class_0, x_train_vae_class_1))\n",
        "x_valid_vae_hand = np.vstack((x_valid_vae_class_0, x_valid_vae_class_1))\n",
        "x_test_vae_hand = np.vstack((x_test_vae_class_0, x_test_vae_class_1))\n",
        "\n",
        "y_train_vae_hand = np.vstack((y_train_vae_class_0, y_train_vae_class_1))\n",
        "y_valid_vae_hand = np.vstack((y_valid_vae_class_0, y_valid_vae_class_1))\n",
        "y_test_vae_hand = np.vstack((y_test_vae_class_0, y_test_vae_class_1))\n",
        "\n",
        "# shuffle hand data\n",
        "x_train_vae_hand, y_train_vae_hand = shuffle(x_train_vae_hand, y_train_vae_hand)\n",
        "x_valid_vae_hand, y_valid_vae_hand = shuffle(x_valid_vae_hand, y_valid_vae_hand)\n",
        "x_test_vae_hand, y_test_vae_hand = shuffle(x_test_vae_hand, y_test_vae_hand)"
      ],
      "metadata": {
        "id": "_6Lkeexlzt5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_vae_class_0.shape)\n",
        "print(x_train_vae_class_1.shape)\n",
        "print(x_train_vae_hand.shape)\n",
        "\n",
        "print(y_train_vae_class_0.shape)\n",
        "print(y_train_vae_class_1.shape)\n",
        "print(y_train_vae_hand.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ae76jn_2Ij9",
        "outputId": "f99c7023-5b8a-48cf-f2d5-5eadb4388f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8732, 250, 1, 22)\n",
            "(1636, 250, 1, 22)\n",
            "(10368, 250, 1, 22)\n",
            "(8732, 4)\n",
            "(1636, 4)\n",
            "(10368, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training on Class 0 and Class 1 Data"
      ],
      "metadata": {
        "id": "iB3-B76_4SCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the CNN model using sequential class\n",
        "def cnn_func(dropout):\n",
        "  dp = dropout # set dropout value for tuning \n",
        "  basic_cnn_model = Sequential()\n",
        "\n",
        "  # Conv. block 1\n",
        "  basic_cnn_model.add(Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(250,1,22)))\n",
        "  basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) # Read the keras documentation\n",
        "  basic_cnn_model.add(BatchNormalization())\n",
        "  basic_cnn_model.add(Dropout(dp))\n",
        "\n",
        "  # Conv. block 2\n",
        "  basic_cnn_model.add(Conv2D(filters=50, kernel_size=(10,1), padding='same', activation='elu'))\n",
        "  basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
        "  basic_cnn_model.add(BatchNormalization())\n",
        "  basic_cnn_model.add(Dropout(dp))\n",
        "\n",
        "  # Conv. block 3\n",
        "  basic_cnn_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
        "  basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
        "  basic_cnn_model.add(BatchNormalization())\n",
        "  basic_cnn_model.add(Dropout(dp))\n",
        "\n",
        "  # Conv. block 4\n",
        "  basic_cnn_model.add(Conv2D(filters=200, kernel_size=(10,1), padding='same', activation='elu'))\n",
        "  basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
        "  basic_cnn_model.add(BatchNormalization())\n",
        "  basic_cnn_model.add(Dropout(dp))\n",
        "\n",
        "  # Output layer with Softmax activation\n",
        "  basic_cnn_model.add(Flatten()) # Flattens the input\n",
        "  basic_cnn_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
        "\n",
        "  return basic_cnn_model\n"
      ],
      "metadata": {
        "id": "BflbFqTg4RUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimal hyperparameters\n",
        "lr = 1e-3\n",
        "epoch = 75\n",
        "bs = 64\n",
        "drop = 0.6"
      ],
      "metadata": {
        "id": "EbInbeyM5AdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train over only classes 0 and 1\n",
        "# Initiate CNN model \n",
        "basic_cnn_model_hand = cnn_func(drop)\n",
        "# Setting model parameters \n",
        "cnn_optimizer = keras.optimizers.Adam(learning_rate = lr)\n",
        "basic_cnn_model_hand.compile(loss='categorical_crossentropy',\n",
        "                optimizer=cnn_optimizer,\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Training and validating the model\n",
        "basic_cnn_model_results_hand = basic_cnn_model_hand.fit(x_train_vae_hand,\n",
        "            y_train_vae_hand,\n",
        "            batch_size=bs,\n",
        "            epochs=epoch,\n",
        "            validation_data=(x_valid_vae_hand, y_valid_vae_hand), verbose=True)\n",
        "# Testing model\n",
        "cnn_score_left_hand = basic_cnn_model_hand.evaluate(x_test_vae_class_0, y_test_vae_class_0, verbose=0)\n",
        "cnn_score_right_hand = basic_cnn_model_hand.evaluate(x_test_vae_class_1, y_test_vae_class_1, verbose=0)\n",
        "print('Test accuracy for left hand of the CNN model over hand data:',cnn_score_left_hand[1])\n",
        "print('Test accuracy for right hand of the CNN model over hand data:',cnn_score_right_hand[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FgCp2CJ5E0S",
        "outputId": "f34ece16-105f-4cb3-d782-052886f73301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/75\n",
            "162/162 [==============================] - 12s 8ms/step - loss: 0.6026 - accuracy: 0.8331 - val_loss: 0.5932 - val_accuracy: 0.8487\n",
            "Epoch 2/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.3882 - accuracy: 0.8524 - val_loss: 0.2394 - val_accuracy: 0.8615\n",
            "Epoch 3/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.3338 - accuracy: 0.8512 - val_loss: 0.2327 - val_accuracy: 0.8602\n",
            "Epoch 4/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.2737 - accuracy: 0.8639 - val_loss: 0.2348 - val_accuracy: 0.8761\n",
            "Epoch 5/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.2456 - accuracy: 0.8724 - val_loss: 0.2188 - val_accuracy: 0.8863\n",
            "Epoch 6/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.2389 - accuracy: 0.8768 - val_loss: 0.2220 - val_accuracy: 0.8960\n",
            "Epoch 7/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.2276 - accuracy: 0.8879 - val_loss: 0.2285 - val_accuracy: 0.8885\n",
            "Epoch 8/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.2133 - accuracy: 0.8920 - val_loss: 0.1987 - val_accuracy: 0.9080\n",
            "Epoch 9/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.1982 - accuracy: 0.9052 - val_loss: 0.1958 - val_accuracy: 0.9066\n",
            "Epoch 10/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.1886 - accuracy: 0.9111 - val_loss: 0.1833 - val_accuracy: 0.9168\n",
            "Epoch 11/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.1777 - accuracy: 0.9155 - val_loss: 0.1801 - val_accuracy: 0.9204\n",
            "Epoch 12/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.1765 - accuracy: 0.9177 - val_loss: 0.1927 - val_accuracy: 0.9155\n",
            "Epoch 13/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.1684 - accuracy: 0.9212 - val_loss: 0.1647 - val_accuracy: 0.9319\n",
            "Epoch 14/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.1653 - accuracy: 0.9237 - val_loss: 0.1590 - val_accuracy: 0.9274\n",
            "Epoch 15/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.1593 - accuracy: 0.9255 - val_loss: 0.1810 - val_accuracy: 0.9261\n",
            "Epoch 16/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.1465 - accuracy: 0.9373 - val_loss: 0.1664 - val_accuracy: 0.9270\n",
            "Epoch 17/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.1404 - accuracy: 0.9370 - val_loss: 0.1563 - val_accuracy: 0.9301\n",
            "Epoch 18/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.1388 - accuracy: 0.9394 - val_loss: 0.1617 - val_accuracy: 0.9336\n",
            "Epoch 19/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.1374 - accuracy: 0.9388 - val_loss: 0.1692 - val_accuracy: 0.9323\n",
            "Epoch 20/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.1317 - accuracy: 0.9416 - val_loss: 0.1604 - val_accuracy: 0.9350\n",
            "Epoch 21/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.1243 - accuracy: 0.9455 - val_loss: 0.1789 - val_accuracy: 0.9274\n",
            "Epoch 22/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.1217 - accuracy: 0.9473 - val_loss: 0.1539 - val_accuracy: 0.9398\n",
            "Epoch 23/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.1204 - accuracy: 0.9476 - val_loss: 0.1811 - val_accuracy: 0.9239\n",
            "Epoch 24/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.1146 - accuracy: 0.9504 - val_loss: 0.1421 - val_accuracy: 0.9354\n",
            "Epoch 25/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.1078 - accuracy: 0.9547 - val_loss: 0.1724 - val_accuracy: 0.9301\n",
            "Epoch 26/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.1168 - accuracy: 0.9526 - val_loss: 0.1590 - val_accuracy: 0.9274\n",
            "Epoch 27/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.1064 - accuracy: 0.9548 - val_loss: 0.2049 - val_accuracy: 0.9235\n",
            "Epoch 28/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.1056 - accuracy: 0.9528 - val_loss: 0.1451 - val_accuracy: 0.9332\n",
            "Epoch 29/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.1041 - accuracy: 0.9577 - val_loss: 0.1594 - val_accuracy: 0.9336\n",
            "Epoch 30/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.0945 - accuracy: 0.9617 - val_loss: 0.1676 - val_accuracy: 0.9283\n",
            "Epoch 31/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0958 - accuracy: 0.9599 - val_loss: 0.1850 - val_accuracy: 0.9279\n",
            "Epoch 32/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0940 - accuracy: 0.9600 - val_loss: 0.1772 - val_accuracy: 0.9252\n",
            "Epoch 33/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0888 - accuracy: 0.9628 - val_loss: 0.1842 - val_accuracy: 0.9230\n",
            "Epoch 34/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.0847 - accuracy: 0.9651 - val_loss: 0.1595 - val_accuracy: 0.9314\n",
            "Epoch 35/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0893 - accuracy: 0.9635 - val_loss: 0.1921 - val_accuracy: 0.9173\n",
            "Epoch 36/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.0845 - accuracy: 0.9649 - val_loss: 0.1415 - val_accuracy: 0.9372\n",
            "Epoch 37/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.0838 - accuracy: 0.9653 - val_loss: 0.1915 - val_accuracy: 0.9296\n",
            "Epoch 38/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.0846 - accuracy: 0.9655 - val_loss: 0.1404 - val_accuracy: 0.9412\n",
            "Epoch 39/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.0739 - accuracy: 0.9714 - val_loss: 0.1614 - val_accuracy: 0.9323\n",
            "Epoch 40/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.0781 - accuracy: 0.9689 - val_loss: 0.1476 - val_accuracy: 0.9416\n",
            "Epoch 41/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.0732 - accuracy: 0.9724 - val_loss: 0.1564 - val_accuracy: 0.9363\n",
            "Epoch 42/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0806 - accuracy: 0.9669 - val_loss: 0.1723 - val_accuracy: 0.9292\n",
            "Epoch 43/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.0707 - accuracy: 0.9726 - val_loss: 0.1579 - val_accuracy: 0.9350\n",
            "Epoch 44/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0722 - accuracy: 0.9715 - val_loss: 0.1391 - val_accuracy: 0.9398\n",
            "Epoch 45/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0686 - accuracy: 0.9731 - val_loss: 0.1493 - val_accuracy: 0.9425\n",
            "Epoch 46/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0709 - accuracy: 0.9718 - val_loss: 0.1517 - val_accuracy: 0.9363\n",
            "Epoch 47/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.0627 - accuracy: 0.9745 - val_loss: 0.1768 - val_accuracy: 0.9363\n",
            "Epoch 48/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0668 - accuracy: 0.9726 - val_loss: 0.1539 - val_accuracy: 0.9412\n",
            "Epoch 49/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0712 - accuracy: 0.9705 - val_loss: 0.1596 - val_accuracy: 0.9354\n",
            "Epoch 50/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0674 - accuracy: 0.9734 - val_loss: 0.2044 - val_accuracy: 0.9257\n",
            "Epoch 51/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.0594 - accuracy: 0.9758 - val_loss: 0.1311 - val_accuracy: 0.9429\n",
            "Epoch 52/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.0544 - accuracy: 0.9787 - val_loss: 0.1568 - val_accuracy: 0.9420\n",
            "Epoch 53/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0636 - accuracy: 0.9744 - val_loss: 0.1788 - val_accuracy: 0.9301\n",
            "Epoch 54/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0599 - accuracy: 0.9763 - val_loss: 0.1558 - val_accuracy: 0.9491\n",
            "Epoch 55/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0625 - accuracy: 0.9759 - val_loss: 0.1634 - val_accuracy: 0.9420\n",
            "Epoch 56/75\n",
            "162/162 [==============================] - 1s 5ms/step - loss: 0.0560 - accuracy: 0.9781 - val_loss: 0.1909 - val_accuracy: 0.9327\n",
            "Epoch 57/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0582 - accuracy: 0.9760 - val_loss: 0.1423 - val_accuracy: 0.9460\n",
            "Epoch 58/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0532 - accuracy: 0.9793 - val_loss: 0.1673 - val_accuracy: 0.9385\n",
            "Epoch 59/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0575 - accuracy: 0.9769 - val_loss: 0.1565 - val_accuracy: 0.9469\n",
            "Epoch 60/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0531 - accuracy: 0.9786 - val_loss: 0.1674 - val_accuracy: 0.9420\n",
            "Epoch 61/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0472 - accuracy: 0.9811 - val_loss: 0.1637 - val_accuracy: 0.9416\n",
            "Epoch 62/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0450 - accuracy: 0.9826 - val_loss: 0.1507 - val_accuracy: 0.9478\n",
            "Epoch 63/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0496 - accuracy: 0.9804 - val_loss: 0.1626 - val_accuracy: 0.9456\n",
            "Epoch 64/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0520 - accuracy: 0.9788 - val_loss: 0.1608 - val_accuracy: 0.9425\n",
            "Epoch 65/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0483 - accuracy: 0.9805 - val_loss: 0.1460 - val_accuracy: 0.9540\n",
            "Epoch 66/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0499 - accuracy: 0.9813 - val_loss: 0.1429 - val_accuracy: 0.9509\n",
            "Epoch 67/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0506 - accuracy: 0.9823 - val_loss: 0.1643 - val_accuracy: 0.9416\n",
            "Epoch 68/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0464 - accuracy: 0.9826 - val_loss: 0.1523 - val_accuracy: 0.9407\n",
            "Epoch 69/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0517 - accuracy: 0.9807 - val_loss: 0.1552 - val_accuracy: 0.9500\n",
            "Epoch 70/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0420 - accuracy: 0.9827 - val_loss: 0.1431 - val_accuracy: 0.9549\n",
            "Epoch 71/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0507 - accuracy: 0.9812 - val_loss: 0.1402 - val_accuracy: 0.9518\n",
            "Epoch 72/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0459 - accuracy: 0.9817 - val_loss: 0.1662 - val_accuracy: 0.9456\n",
            "Epoch 73/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0416 - accuracy: 0.9840 - val_loss: 0.1655 - val_accuracy: 0.9465\n",
            "Epoch 74/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0424 - accuracy: 0.9828 - val_loss: 0.1756 - val_accuracy: 0.9442\n",
            "Epoch 75/75\n",
            "162/162 [==============================] - 1s 6ms/step - loss: 0.0409 - accuracy: 0.9840 - val_loss: 0.1848 - val_accuracy: 0.9447\n",
            "Test accuracy for left hand of the CNN model over hand data: 0.9562274217605591\n",
            "Test accuracy for right hand of the CNN model over hand data: 0.9685039520263672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training on All Class Data"
      ],
      "metadata": {
        "id": "S5ziLfme6k33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train over only classes 0 and 1\n",
        "# Initiate CNN model \n",
        "basic_cnn_model = cnn_func(drop)\n",
        "# Setting model parameters \n",
        "cnn_optimizer = keras.optimizers.Adam(learning_rate = lr)\n",
        "basic_cnn_model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=cnn_optimizer,\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Training and validating the model\n",
        "basic_cnn_model_results = basic_cnn_model.fit(x_train_vae,\n",
        "            y_train_vae,\n",
        "            batch_size=bs,\n",
        "            epochs=epoch,\n",
        "            validation_data=(x_valid_vae, y_valid_vae), verbose=True)\n",
        "# Testing model\n",
        "cnn_score_left_hand = basic_cnn_model.evaluate(x_test_vae_class_0, y_test_vae_class_0, verbose=0)\n",
        "cnn_score_right_hand = basic_cnn_model.evaluate(x_test_vae_class_1, y_test_vae_class_1, verbose=0)\n",
        "print('Test accuracy for left hand of the CNN model over all data:',cnn_score_left_hand[1])\n",
        "print('Test accuracy for right hand of the CNN model over all data:',cnn_score_right_hand[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr6OZQKU6m_s",
        "outputId": "9a7982db-99ed-4fe8-d05f-7c4a456a9cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/75\n",
            "218/218 [==============================] - 2s 8ms/step - loss: 1.1958 - accuracy: 0.6410 - val_loss: 0.7678 - val_accuracy: 0.6627\n",
            "Epoch 2/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.8583 - accuracy: 0.6595 - val_loss: 0.6798 - val_accuracy: 0.6670\n",
            "Epoch 3/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.7337 - accuracy: 0.6868 - val_loss: 0.6004 - val_accuracy: 0.7300\n",
            "Epoch 4/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6760 - accuracy: 0.7122 - val_loss: 0.5996 - val_accuracy: 0.7373\n",
            "Epoch 5/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6431 - accuracy: 0.7225 - val_loss: 0.5676 - val_accuracy: 0.7510\n",
            "Epoch 6/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.6092 - accuracy: 0.7422 - val_loss: 0.5481 - val_accuracy: 0.7583\n",
            "Epoch 7/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5980 - accuracy: 0.7456 - val_loss: 0.5377 - val_accuracy: 0.7837\n",
            "Epoch 8/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5822 - accuracy: 0.7555 - val_loss: 0.5253 - val_accuracy: 0.7737\n",
            "Epoch 9/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5591 - accuracy: 0.7681 - val_loss: 0.5410 - val_accuracy: 0.7670\n",
            "Epoch 10/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5443 - accuracy: 0.7752 - val_loss: 0.5097 - val_accuracy: 0.7883\n",
            "Epoch 11/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5365 - accuracy: 0.7784 - val_loss: 0.5167 - val_accuracy: 0.7920\n",
            "Epoch 12/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5158 - accuracy: 0.7888 - val_loss: 0.4820 - val_accuracy: 0.8013\n",
            "Epoch 13/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.5006 - accuracy: 0.7962 - val_loss: 0.4608 - val_accuracy: 0.8147\n",
            "Epoch 14/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4766 - accuracy: 0.8098 - val_loss: 0.4421 - val_accuracy: 0.8280\n",
            "Epoch 15/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4765 - accuracy: 0.8076 - val_loss: 0.4497 - val_accuracy: 0.8207\n",
            "Epoch 16/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4649 - accuracy: 0.8142 - val_loss: 0.4425 - val_accuracy: 0.8203\n",
            "Epoch 17/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4478 - accuracy: 0.8180 - val_loss: 0.4357 - val_accuracy: 0.8300\n",
            "Epoch 18/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4344 - accuracy: 0.8269 - val_loss: 0.4471 - val_accuracy: 0.8230\n",
            "Epoch 19/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4324 - accuracy: 0.8309 - val_loss: 0.4265 - val_accuracy: 0.8253\n",
            "Epoch 20/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4220 - accuracy: 0.8343 - val_loss: 0.4417 - val_accuracy: 0.8263\n",
            "Epoch 21/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4118 - accuracy: 0.8333 - val_loss: 0.4439 - val_accuracy: 0.8250\n",
            "Epoch 22/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4061 - accuracy: 0.8362 - val_loss: 0.4106 - val_accuracy: 0.8327\n",
            "Epoch 23/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.4053 - accuracy: 0.8399 - val_loss: 0.3956 - val_accuracy: 0.8310\n",
            "Epoch 24/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3872 - accuracy: 0.8483 - val_loss: 0.4176 - val_accuracy: 0.8333\n",
            "Epoch 25/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3814 - accuracy: 0.8491 - val_loss: 0.3875 - val_accuracy: 0.8460\n",
            "Epoch 26/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3894 - accuracy: 0.8466 - val_loss: 0.3852 - val_accuracy: 0.8433\n",
            "Epoch 27/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3738 - accuracy: 0.8528 - val_loss: 0.3909 - val_accuracy: 0.8457\n",
            "Epoch 28/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3609 - accuracy: 0.8573 - val_loss: 0.3930 - val_accuracy: 0.8393\n",
            "Epoch 29/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3678 - accuracy: 0.8555 - val_loss: 0.3956 - val_accuracy: 0.8450\n",
            "Epoch 30/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3615 - accuracy: 0.8588 - val_loss: 0.3657 - val_accuracy: 0.8530\n",
            "Epoch 31/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3531 - accuracy: 0.8613 - val_loss: 0.3743 - val_accuracy: 0.8570\n",
            "Epoch 32/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3479 - accuracy: 0.8624 - val_loss: 0.3917 - val_accuracy: 0.8457\n",
            "Epoch 33/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3499 - accuracy: 0.8616 - val_loss: 0.3723 - val_accuracy: 0.8543\n",
            "Epoch 34/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3435 - accuracy: 0.8635 - val_loss: 0.3672 - val_accuracy: 0.8620\n",
            "Epoch 35/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3415 - accuracy: 0.8654 - val_loss: 0.3872 - val_accuracy: 0.8507\n",
            "Epoch 36/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3386 - accuracy: 0.8684 - val_loss: 0.3713 - val_accuracy: 0.8550\n",
            "Epoch 37/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3280 - accuracy: 0.8692 - val_loss: 0.4108 - val_accuracy: 0.8453\n",
            "Epoch 38/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3363 - accuracy: 0.8684 - val_loss: 0.3799 - val_accuracy: 0.8523\n",
            "Epoch 39/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3200 - accuracy: 0.8759 - val_loss: 0.4057 - val_accuracy: 0.8367\n",
            "Epoch 40/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3204 - accuracy: 0.8744 - val_loss: 0.3861 - val_accuracy: 0.8507\n",
            "Epoch 41/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3165 - accuracy: 0.8787 - val_loss: 0.3695 - val_accuracy: 0.8583\n",
            "Epoch 42/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3129 - accuracy: 0.8790 - val_loss: 0.3856 - val_accuracy: 0.8483\n",
            "Epoch 43/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3089 - accuracy: 0.8797 - val_loss: 0.3658 - val_accuracy: 0.8630\n",
            "Epoch 44/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3079 - accuracy: 0.8817 - val_loss: 0.3891 - val_accuracy: 0.8517\n",
            "Epoch 45/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3075 - accuracy: 0.8823 - val_loss: 0.3776 - val_accuracy: 0.8490\n",
            "Epoch 46/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3013 - accuracy: 0.8823 - val_loss: 0.3886 - val_accuracy: 0.8557\n",
            "Epoch 47/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2997 - accuracy: 0.8836 - val_loss: 0.3761 - val_accuracy: 0.8530\n",
            "Epoch 48/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3038 - accuracy: 0.8810 - val_loss: 0.3673 - val_accuracy: 0.8603\n",
            "Epoch 49/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.3021 - accuracy: 0.8812 - val_loss: 0.3645 - val_accuracy: 0.8633\n",
            "Epoch 50/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2969 - accuracy: 0.8860 - val_loss: 0.4040 - val_accuracy: 0.8487\n",
            "Epoch 51/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2906 - accuracy: 0.8853 - val_loss: 0.3566 - val_accuracy: 0.8670\n",
            "Epoch 52/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2911 - accuracy: 0.8870 - val_loss: 0.3638 - val_accuracy: 0.8553\n",
            "Epoch 53/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2802 - accuracy: 0.8915 - val_loss: 0.3624 - val_accuracy: 0.8693\n",
            "Epoch 54/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2803 - accuracy: 0.8904 - val_loss: 0.3525 - val_accuracy: 0.8600\n",
            "Epoch 55/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2782 - accuracy: 0.8969 - val_loss: 0.3565 - val_accuracy: 0.8693\n",
            "Epoch 56/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2822 - accuracy: 0.8904 - val_loss: 0.3703 - val_accuracy: 0.8573\n",
            "Epoch 57/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2768 - accuracy: 0.8909 - val_loss: 0.3555 - val_accuracy: 0.8607\n",
            "Epoch 58/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2759 - accuracy: 0.8927 - val_loss: 0.3755 - val_accuracy: 0.8490\n",
            "Epoch 59/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2734 - accuracy: 0.8938 - val_loss: 0.3742 - val_accuracy: 0.8570\n",
            "Epoch 60/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2713 - accuracy: 0.8927 - val_loss: 0.3504 - val_accuracy: 0.8670\n",
            "Epoch 61/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2647 - accuracy: 0.8981 - val_loss: 0.3754 - val_accuracy: 0.8580\n",
            "Epoch 62/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2672 - accuracy: 0.8968 - val_loss: 0.3686 - val_accuracy: 0.8523\n",
            "Epoch 63/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2614 - accuracy: 0.9001 - val_loss: 0.3678 - val_accuracy: 0.8617\n",
            "Epoch 64/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2657 - accuracy: 0.8968 - val_loss: 0.3593 - val_accuracy: 0.8643\n",
            "Epoch 65/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2623 - accuracy: 0.8971 - val_loss: 0.3634 - val_accuracy: 0.8593\n",
            "Epoch 66/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2585 - accuracy: 0.9012 - val_loss: 0.4002 - val_accuracy: 0.8510\n",
            "Epoch 67/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2665 - accuracy: 0.8991 - val_loss: 0.3740 - val_accuracy: 0.8530\n",
            "Epoch 68/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2632 - accuracy: 0.8975 - val_loss: 0.3618 - val_accuracy: 0.8653\n",
            "Epoch 69/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2541 - accuracy: 0.9014 - val_loss: 0.3804 - val_accuracy: 0.8573\n",
            "Epoch 70/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2526 - accuracy: 0.9011 - val_loss: 0.3788 - val_accuracy: 0.8593\n",
            "Epoch 71/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2544 - accuracy: 0.9018 - val_loss: 0.3711 - val_accuracy: 0.8597\n",
            "Epoch 72/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2492 - accuracy: 0.9041 - val_loss: 0.3498 - val_accuracy: 0.8630\n",
            "Epoch 73/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2511 - accuracy: 0.9038 - val_loss: 0.3593 - val_accuracy: 0.8670\n",
            "Epoch 74/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2474 - accuracy: 0.9057 - val_loss: 0.3501 - val_accuracy: 0.8660\n",
            "Epoch 75/75\n",
            "218/218 [==============================] - 1s 6ms/step - loss: 0.2478 - accuracy: 0.9038 - val_loss: 0.3858 - val_accuracy: 0.8560\n",
            "Test accuracy for left hand of the CNN model over all data: 0.9282491207122803\n",
            "Test accuracy for right hand of the CNN model over all data: 0.6732283234596252\n"
          ]
        }
      ]
    }
  ]
}